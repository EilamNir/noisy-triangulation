{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from simulation.generate_path import generate_path\n",
    "import matplotlib.pyplot as plt\n",
    "from estimation.distance_sensor import distance_sensors\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 1)\n",
      "(4, 3, 10, 569)\n"
     ]
    }
   ],
   "source": [
    "# create test data\n",
    "target_initial_pos = np.array([0, 0, 5000])\n",
    "target_speed_xy = 50\n",
    "target_speed_z = 10\n",
    "target_rot_speed = 3\n",
    "time_res = 0.5\n",
    "\n",
    "path1 = generate_path(0, target_speed_xy, target_speed_z, target_initial_pos, time_res)\n",
    "path1.add_straight_interval(100)\n",
    "path1.add_xy_turn_interval(90, -np.deg2rad(target_rot_speed))\n",
    "path1.add_straight_interval(100)\n",
    "\n",
    "# create noisy sensors\n",
    "sensors = distance_sensors([[-5000,0,0],[ 400, -7400, 0],[ 800, 800, 0]], 15)\n",
    "sensors.calculate_measurements(path1.path)\n",
    "\n",
    "\n",
    "sample = 10\n",
    "XTest = []\n",
    "for i in np.arange(len(path1.path) - sample + 1):\n",
    "    tmp = np.concatenate((sensors.sensor_locations, np.reshape(path1.path[i,:], (1,3))), 0)\n",
    "    tmp = tmp.reshape(4,3,1)\n",
    "    for j in np.arange(1,sample):\n",
    "        matrix = np.concatenate((sensors.sensor_locations, np.reshape(path1.path[i+j,:], (1,3))), 0)\n",
    "        matrix = matrix.reshape(4,3,1)\n",
    "        tmp = np.concatenate((tmp, matrix),axis=2)\n",
    "    if i > 0:\n",
    "        tmp = tmp.reshape(4,3,sample,1)\n",
    "        XTest = np.concatenate((XTest, tmp), 3)\n",
    "    else:\n",
    "        tmp = tmp.reshape(4,3,sample,1)\n",
    "        XTest = tmp\n",
    "\n",
    "YTest = path1.state_key[sample-1:]\n",
    "\n",
    "print(np.shape(YTest))\n",
    "print(np.shape(XTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125312, 1)\n",
      "(4, 3, 10, 125312)\n"
     ]
    }
   ],
   "source": [
    "# create train data\n",
    "run_number = 600\n",
    "# run_number = 1000\n",
    "XTrain = []\n",
    "YTrain = []\n",
    "for k in np.arange(run_number):\n",
    "    target_initial_pos = np.random.randint(-7000, 7000, size=(1, 3))[0]\n",
    "    sensors_pos = np.random.randint(-7000, 7000, size=(3, 3))[:,:]\n",
    "    target_speed_xy = 50\n",
    "    target_speed_z = 10\n",
    "    target_rot_speed = 3\n",
    "    time_res = 0.5\n",
    "\n",
    "    path1 = generate_path(np.deg2rad(np.random.randint(0,360,size=1)[0]), target_speed_xy, target_speed_z, target_initial_pos, time_res)\n",
    "    path1.add_straight_interval(np.random.randint(10,100,size=1)[0])\n",
    "    path1.add_xy_turn_interval(np.random.randint(10,100,size=1)[0], -random.choice([-1, 1])*np.deg2rad(target_rot_speed))\n",
    "\n",
    "    # create noisy sensors\n",
    "    sensors = distance_sensors(sensors_pos, 20)\n",
    "    sensors.calculate_measurements(path1.path)\n",
    "\n",
    "    sample = 10\n",
    "    for i in np.arange(len(path1.path) - sample + 1):\n",
    "        tmp = np.concatenate((sensors.sensor_locations, np.reshape(path1.path[i,:], (1,3))), 0)\n",
    "        tmp = tmp.reshape(4,3,1)\n",
    "        for j in np.arange(1,sample):\n",
    "            matrix = np.concatenate((sensors.sensor_locations, np.reshape(path1.path[i+j,:], (1,3))), 0)\n",
    "            matrix = matrix.reshape(4,3,1)\n",
    "            tmp = np.concatenate((tmp, matrix),axis=2)\n",
    "        if len(XTrain):\n",
    "            tmp = tmp.reshape(4,3,sample,1)\n",
    "            XTrain = np.concatenate((XTrain, tmp), 3)\n",
    "        else:\n",
    "            tmp = tmp.reshape(4,3,sample,1)\n",
    "            XTrain = tmp\n",
    "    if len(YTrain):\n",
    "        YTrain = np.concatenate((YTrain, path1.state_key[sample-1:]), 0)\n",
    "    else:\n",
    "        YTrain = path1.state_key[sample-1:]\n",
    "\n",
    "print(np.shape(YTrain))\n",
    "print(np.shape(XTrain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "ind = np.arange(len(YTrain))\n",
    "random.shuffle(ind)\n",
    "\n",
    "XVal = torch.from_numpy(np.transpose(XTest, (3, 2, 0, 1)))\n",
    "YVal = torch.from_numpy(YTest)\n",
    "\n",
    "XTrain = torch.from_numpy(np.transpose(XTrain[:,:,:,ind], (3, 2, 0, 1)))\n",
    "YTrain = torch.from_numpy(YTrain[ind,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create network\n",
    "class state_estimat(nn.Module):\n",
    "    def __init__(self, d_in, num_classes):\n",
    "        # initialzing the parent object (important!)\n",
    "        super(state_estimat, self).__init__()\n",
    "        # Create a pipeline - a sequence of layers\n",
    "        \n",
    "        self.pipe = torch.nn.Sequential(\n",
    "            nn.Conv2d(d_in, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*3*4, 1))\n",
    "            # nn.Sigmoid())\n",
    "\n",
    "        # self.d_in = d_in\n",
    "        # self.device = device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = nn.Conv2d(self.d_in, 64, kernel_size=3, padding=1, device=self.device)(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x = nn.Conv2d(64, 64, kernel_size=3, padding=1, device=self.device)(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x = nn.Conv2d(64, 64, kernel_size=3, padding=1, device=self.device)(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x = nn.Flatten()(x)\n",
    "        # x = nn.Linear(64*3*4, 1, device=self.device)(x)\n",
    "        # return x\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "state_estimat(\n",
      "  (pipe): Sequential(\n",
      "    (0): Conv2d(10, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameters:\n",
    "# num_epochs = 100\n",
    "num_epochs = 30\n",
    "batch_size = 256\n",
    "# batch_size = 256\n",
    "learning_rate = 0.001\n",
    "learning_rate_drop_period = 10\n",
    "input_shape = [3,4,3]\n",
    "\n",
    "# Device configuration, as before\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "# create model\n",
    "model = state_estimat(d_in=10, num_classes=1).to(device)\n",
    "print(model)\n",
    "# Loss and optimizer\n",
    "# criterion = torch.nn.BCELoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_rate_drop_period, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [5/489], Loss: 18247.6465, Time: 0.1373 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [10/489], Loss: 607.3508, Time: 0.1425 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [15/489], Loss: 513.5128, Time: 0.1466 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [20/489], Loss: 192.3581, Time: 0.1507 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [25/489], Loss: 71.7856, Time: 0.1547 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [30/489], Loss: 136.5184, Time: 0.1589 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [35/489], Loss: 43.9509, Time: 0.1629 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [40/489], Loss: 44.5438, Time: 0.1670 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [45/489], Loss: 32.1719, Time: 0.1710 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [50/489], Loss: 26.8774, Time: 0.1752 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [55/489], Loss: 22.2054, Time: 0.1792 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [60/489], Loss: 16.1585, Time: 0.1832 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [65/489], Loss: 14.6218, Time: 0.1872 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [70/489], Loss: 12.6544, Time: 0.1914 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [75/489], Loss: 11.0769, Time: 0.1954 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [80/489], Loss: 8.9666, Time: 0.1995 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [85/489], Loss: 10.3055, Time: 0.2035 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [90/489], Loss: 9.5774, Time: 0.2077 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [95/489], Loss: 8.1211, Time: 0.2117 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [100/489], Loss: 7.8396, Time: 0.2158 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [105/489], Loss: 7.1862, Time: 0.2198 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [110/489], Loss: 7.2715, Time: 0.2239 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [115/489], Loss: 6.7570, Time: 0.2278 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [120/489], Loss: 5.2748, Time: 0.2316 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [125/489], Loss: 6.0146, Time: 0.2354 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [130/489], Loss: 5.8790, Time: 0.2394 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [135/489], Loss: 5.5864, Time: 0.2432 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [140/489], Loss: 5.6574, Time: 0.2471 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [145/489], Loss: 5.4715, Time: 0.2509 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [150/489], Loss: 5.2333, Time: 0.2548 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [155/489], Loss: 4.3399, Time: 0.2588 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [160/489], Loss: 4.3423, Time: 0.2627 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [165/489], Loss: 4.6494, Time: 0.2665 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [170/489], Loss: 4.4019, Time: 0.2704 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [175/489], Loss: 4.7456, Time: 0.2744 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [180/489], Loss: 4.7615, Time: 0.2782 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [185/489], Loss: 4.0860, Time: 0.2821 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [190/489], Loss: 4.7216, Time: 0.2860 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [195/489], Loss: 4.3860, Time: 0.2899 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [200/489], Loss: 3.3605, Time: 0.2937 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [205/489], Loss: 3.4544, Time: 0.2975 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [210/489], Loss: 3.7420, Time: 0.3013 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [215/489], Loss: 3.0750, Time: 0.3051 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [220/489], Loss: 3.1908, Time: 0.3091 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [225/489], Loss: 2.9802, Time: 0.3129 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [230/489], Loss: 3.4360, Time: 0.3168 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [235/489], Loss: 2.9691, Time: 0.3206 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [240/489], Loss: 2.8812, Time: 0.3246 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [245/489], Loss: 2.9612, Time: 0.3284 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [250/489], Loss: 3.1551, Time: 0.3322 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [255/489], Loss: 2.7005, Time: 0.3360 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [260/489], Loss: 2.6947, Time: 0.3402 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [265/489], Loss: 2.7403, Time: 0.3442 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [270/489], Loss: 2.8848, Time: 0.3481 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [275/489], Loss: 3.0550, Time: 0.3520 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [280/489], Loss: 2.5517, Time: 0.3559 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [285/489], Loss: 2.5675, Time: 0.3597 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [290/489], Loss: 2.5375, Time: 0.3636 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [295/489], Loss: 2.6578, Time: 0.3674 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [300/489], Loss: 2.4939, Time: 0.3713 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [305/489], Loss: 2.3464, Time: 0.3753 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [310/489], Loss: 2.4111, Time: 0.3791 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [315/489], Loss: 2.4956, Time: 0.3830 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [320/489], Loss: 2.4139, Time: 0.3868 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [325/489], Loss: 2.4535, Time: 0.3907 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [330/489], Loss: 2.1771, Time: 0.3949 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [335/489], Loss: 2.2442, Time: 0.3993 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [340/489], Loss: 2.4972, Time: 0.4034 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [345/489], Loss: 1.8944, Time: 0.4082 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [350/489], Loss: 2.3177, Time: 0.4130 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [355/489], Loss: 2.1832, Time: 0.4170 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [360/489], Loss: 2.2208, Time: 0.4208 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [365/489], Loss: 2.0916, Time: 0.4246 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [370/489], Loss: 2.0855, Time: 0.4285 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [375/489], Loss: 1.9662, Time: 0.4323 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [380/489], Loss: 1.8729, Time: 0.4361 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [385/489], Loss: 2.0021, Time: 0.4400 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [390/489], Loss: 1.9349, Time: 0.4438 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [395/489], Loss: 1.8682, Time: 0.4476 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [400/489], Loss: 2.0083, Time: 0.4514 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [405/489], Loss: 1.9407, Time: 0.4552 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [410/489], Loss: 2.0158, Time: 0.4592 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [415/489], Loss: 1.7748, Time: 0.4630 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [420/489], Loss: 1.7446, Time: 0.4668 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [425/489], Loss: 1.7984, Time: 0.4706 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [430/489], Loss: 1.8455, Time: 0.4746 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [435/489], Loss: 1.5129, Time: 0.4784 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [440/489], Loss: 1.7167, Time: 0.4822 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [445/489], Loss: 1.4271, Time: 0.4860 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [450/489], Loss: 1.6911, Time: 0.4899 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [455/489], Loss: 1.5473, Time: 0.4937 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [460/489], Loss: 1.5073, Time: 0.4976 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [465/489], Loss: 1.7071, Time: 0.5014 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [470/489], Loss: 1.2938, Time: 0.5052 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [475/489], Loss: 1.7876, Time: 0.5091 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [480/489], Loss: 1.3968, Time: 0.5129 secs, learning rate: 0.0010\n",
      "Epoch [1/30], Step [485/489], Loss: 1.5598, Time: 0.5167 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [5/489], Loss: 1.5045, Time: 0.6462 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [10/489], Loss: 1.3540, Time: 0.6516 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [15/489], Loss: 1.6069, Time: 0.6566 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [20/489], Loss: 1.4407, Time: 0.6608 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [25/489], Loss: 1.4571, Time: 0.6647 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [30/489], Loss: 1.4819, Time: 0.6685 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [35/489], Loss: 1.4216, Time: 0.6724 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [40/489], Loss: 1.1927, Time: 0.6762 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [45/489], Loss: 1.3440, Time: 0.6800 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [50/489], Loss: 1.2449, Time: 0.6838 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [55/489], Loss: 1.2782, Time: 0.6877 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [60/489], Loss: 1.2658, Time: 0.6915 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [65/489], Loss: 1.0773, Time: 0.6953 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [70/489], Loss: 1.2122, Time: 0.6991 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [75/489], Loss: 1.0560, Time: 0.7029 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [80/489], Loss: 1.1531, Time: 0.7067 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [85/489], Loss: 1.2606, Time: 0.7105 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [90/489], Loss: 1.1450, Time: 0.7143 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [95/489], Loss: 1.1343, Time: 0.7181 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [100/489], Loss: 1.2678, Time: 0.7219 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [105/489], Loss: 1.1873, Time: 0.7259 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [110/489], Loss: 1.0622, Time: 0.7297 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [115/489], Loss: 1.0771, Time: 0.7335 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [120/489], Loss: 1.2287, Time: 0.7373 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [125/489], Loss: 1.2256, Time: 0.7413 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [130/489], Loss: 1.1850, Time: 0.7451 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [135/489], Loss: 1.1654, Time: 0.7491 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [140/489], Loss: 1.1630, Time: 0.7537 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [145/489], Loss: 1.0255, Time: 0.7584 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [150/489], Loss: 0.9044, Time: 0.7631 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [155/489], Loss: 0.9609, Time: 0.7674 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [160/489], Loss: 1.0172, Time: 0.7715 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [165/489], Loss: 0.9790, Time: 0.7756 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [170/489], Loss: 1.1243, Time: 0.7796 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [175/489], Loss: 1.1170, Time: 0.7835 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [180/489], Loss: 1.0755, Time: 0.7914 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [185/489], Loss: 1.0665, Time: 0.8006 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [190/489], Loss: 0.9027, Time: 0.8048 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [195/489], Loss: 0.9566, Time: 0.8089 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [200/489], Loss: 0.9360, Time: 0.8129 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [205/489], Loss: 1.0098, Time: 0.8174 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [210/489], Loss: 0.9891, Time: 0.8214 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [215/489], Loss: 0.9707, Time: 0.8290 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [220/489], Loss: 0.9165, Time: 0.8356 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [225/489], Loss: 0.9275, Time: 0.8428 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [230/489], Loss: 0.8292, Time: 0.8497 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [235/489], Loss: 0.8331, Time: 0.8570 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [240/489], Loss: 0.9666, Time: 0.8641 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [245/489], Loss: 0.9199, Time: 0.8710 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [250/489], Loss: 0.9499, Time: 0.8770 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [255/489], Loss: 0.9404, Time: 0.8813 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [260/489], Loss: 0.9374, Time: 0.8869 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [265/489], Loss: 1.0297, Time: 0.8927 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [270/489], Loss: 0.9641, Time: 0.8973 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [275/489], Loss: 0.9296, Time: 0.9011 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [280/489], Loss: 0.9037, Time: 0.9051 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [285/489], Loss: 0.8995, Time: 0.9091 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [290/489], Loss: 0.8985, Time: 0.9130 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [295/489], Loss: 0.8044, Time: 0.9169 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [300/489], Loss: 0.8891, Time: 0.9208 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [305/489], Loss: 0.8902, Time: 0.9246 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [310/489], Loss: 0.8125, Time: 0.9285 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [315/489], Loss: 1.0073, Time: 0.9323 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [320/489], Loss: 0.7994, Time: 0.9361 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [325/489], Loss: 0.8676, Time: 0.9400 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [330/489], Loss: 0.8435, Time: 0.9438 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [335/489], Loss: 0.8182, Time: 0.9477 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [340/489], Loss: 0.7934, Time: 0.9516 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [345/489], Loss: 0.9036, Time: 0.9554 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [350/489], Loss: 0.7700, Time: 0.9594 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [355/489], Loss: 0.7907, Time: 0.9632 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [360/489], Loss: 0.8171, Time: 0.9671 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [365/489], Loss: 0.7837, Time: 0.9709 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [370/489], Loss: 0.7430, Time: 0.9748 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [375/489], Loss: 0.7607, Time: 0.9787 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [380/489], Loss: 0.7817, Time: 0.9825 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [385/489], Loss: 0.7897, Time: 0.9864 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [390/489], Loss: 0.8073, Time: 0.9904 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [395/489], Loss: 0.7197, Time: 0.9943 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [400/489], Loss: 0.7489, Time: 0.9981 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [405/489], Loss: 0.8091, Time: 1.0020 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [410/489], Loss: 0.7695, Time: 1.0059 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [415/489], Loss: 0.7120, Time: 1.0097 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [420/489], Loss: 0.6962, Time: 1.0136 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [425/489], Loss: 0.7466, Time: 1.0175 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [430/489], Loss: 0.8138, Time: 1.0213 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [435/489], Loss: 0.7493, Time: 1.0253 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [440/489], Loss: 0.7515, Time: 1.0292 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [445/489], Loss: 0.7350, Time: 1.0331 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [450/489], Loss: 0.7390, Time: 1.0374 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [455/489], Loss: 0.6879, Time: 1.0417 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [460/489], Loss: 0.7156, Time: 1.0459 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [465/489], Loss: 0.7958, Time: 1.0512 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [470/489], Loss: 0.6758, Time: 1.0565 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [475/489], Loss: 0.7505, Time: 1.0606 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [480/489], Loss: 0.7993, Time: 1.0645 secs, learning rate: 0.0010\n",
      "Epoch [2/30], Step [485/489], Loss: 0.6481, Time: 1.0684 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [5/489], Loss: 0.5996, Time: 1.1981 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [10/489], Loss: 0.7797, Time: 1.2028 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [15/489], Loss: 0.6385, Time: 1.2076 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [20/489], Loss: 0.6810, Time: 1.2123 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [25/489], Loss: 0.7255, Time: 1.2167 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [30/489], Loss: 0.6604, Time: 1.2206 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [35/489], Loss: 0.6921, Time: 1.2246 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [40/489], Loss: 0.6563, Time: 1.2285 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [45/489], Loss: 0.6453, Time: 1.2323 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [50/489], Loss: 0.6256, Time: 1.2362 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [55/489], Loss: 0.6714, Time: 1.2402 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [60/489], Loss: 0.6815, Time: 1.2440 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [65/489], Loss: 0.6734, Time: 1.2479 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [70/489], Loss: 0.6833, Time: 1.2519 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [75/489], Loss: 0.5976, Time: 1.2559 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [80/489], Loss: 0.6295, Time: 1.2598 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [85/489], Loss: 0.6200, Time: 1.2636 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [90/489], Loss: 0.6323, Time: 1.2675 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [95/489], Loss: 0.7319, Time: 1.2714 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [100/489], Loss: 0.6106, Time: 1.2754 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [105/489], Loss: 0.5894, Time: 1.2792 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [110/489], Loss: 0.6071, Time: 1.2831 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [115/489], Loss: 0.6547, Time: 1.2869 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [120/489], Loss: 0.5742, Time: 1.2907 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [125/489], Loss: 0.5579, Time: 1.2946 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [130/489], Loss: 0.6209, Time: 1.2985 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [135/489], Loss: 0.5801, Time: 1.3023 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [140/489], Loss: 0.5738, Time: 1.3063 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [145/489], Loss: 0.5904, Time: 1.3101 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [150/489], Loss: 0.5962, Time: 1.3140 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [155/489], Loss: 0.6087, Time: 1.3178 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [160/489], Loss: 0.6297, Time: 1.3216 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [165/489], Loss: 0.5748, Time: 1.3256 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [170/489], Loss: 0.5733, Time: 1.3294 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [175/489], Loss: 0.5516, Time: 1.3333 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [180/489], Loss: 0.5928, Time: 1.3371 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [185/489], Loss: 0.5538, Time: 1.3411 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [190/489], Loss: 0.5717, Time: 1.3449 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [195/489], Loss: 0.6235, Time: 1.3488 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [200/489], Loss: 0.6268, Time: 1.3526 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [205/489], Loss: 0.5759, Time: 1.3565 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [210/489], Loss: 0.5950, Time: 1.3604 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [215/489], Loss: 0.5605, Time: 1.3642 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [220/489], Loss: 0.6289, Time: 1.3681 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [225/489], Loss: 0.6216, Time: 1.3720 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [230/489], Loss: 0.5647, Time: 1.3759 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [235/489], Loss: 0.5340, Time: 1.3797 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [240/489], Loss: 0.5525, Time: 1.3836 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [245/489], Loss: 0.5211, Time: 1.3874 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [250/489], Loss: 0.5279, Time: 1.3914 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [255/489], Loss: 0.5978, Time: 1.3953 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [260/489], Loss: 0.5343, Time: 1.3991 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [265/489], Loss: 0.5049, Time: 1.4030 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [270/489], Loss: 0.5809, Time: 1.4070 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [275/489], Loss: 0.5902, Time: 1.4109 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [280/489], Loss: 0.5690, Time: 1.4147 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [285/489], Loss: 0.5624, Time: 1.4185 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [290/489], Loss: 0.5179, Time: 1.4224 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [295/489], Loss: 0.5448, Time: 1.4262 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [300/489], Loss: 0.5502, Time: 1.4301 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [305/489], Loss: 0.4983, Time: 1.4339 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [310/489], Loss: 0.5493, Time: 1.4377 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [315/489], Loss: 0.5169, Time: 1.4417 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [320/489], Loss: 0.4827, Time: 1.4456 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [325/489], Loss: 0.5037, Time: 1.4494 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [330/489], Loss: 0.4731, Time: 1.4535 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [335/489], Loss: 0.4926, Time: 1.4574 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [340/489], Loss: 0.5396, Time: 1.4613 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [345/489], Loss: 0.4879, Time: 1.4651 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [350/489], Loss: 0.5430, Time: 1.4690 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [355/489], Loss: 0.5217, Time: 1.4730 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [360/489], Loss: 0.5272, Time: 1.4768 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [365/489], Loss: 0.5575, Time: 1.4807 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [370/489], Loss: 0.5090, Time: 1.4845 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [375/489], Loss: 0.5312, Time: 1.4884 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [380/489], Loss: 0.4420, Time: 1.4923 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [385/489], Loss: 0.4950, Time: 1.4962 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [390/489], Loss: 0.4879, Time: 1.5000 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [395/489], Loss: 0.5579, Time: 1.5038 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [400/489], Loss: 0.5287, Time: 1.5078 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [405/489], Loss: 0.4813, Time: 1.5117 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [410/489], Loss: 0.4878, Time: 1.5155 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [415/489], Loss: 0.4943, Time: 1.5194 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [420/489], Loss: 0.4972, Time: 1.5233 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [425/489], Loss: 0.5087, Time: 1.5272 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [430/489], Loss: 0.5393, Time: 1.5310 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [435/489], Loss: 0.5123, Time: 1.5348 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [440/489], Loss: 0.5018, Time: 1.5387 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [445/489], Loss: 0.4761, Time: 1.5426 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [450/489], Loss: 0.5078, Time: 1.5465 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [455/489], Loss: 0.4773, Time: 1.5503 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [460/489], Loss: 0.4648, Time: 1.5541 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [465/489], Loss: 0.5037, Time: 1.5581 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [470/489], Loss: 0.4495, Time: 1.5623 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [475/489], Loss: 0.4780, Time: 1.5668 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [480/489], Loss: 0.4705, Time: 1.5710 secs, learning rate: 0.0010\n",
      "Epoch [3/30], Step [485/489], Loss: 0.4852, Time: 1.5756 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [5/489], Loss: 0.4954, Time: 1.7055 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [10/489], Loss: 0.4910, Time: 1.7100 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [15/489], Loss: 0.4823, Time: 1.7148 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [20/489], Loss: 0.4714, Time: 1.7188 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [25/489], Loss: 0.5064, Time: 1.7228 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [30/489], Loss: 0.4841, Time: 1.7271 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [35/489], Loss: 0.4425, Time: 1.7318 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [40/489], Loss: 0.4731, Time: 1.7367 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [45/489], Loss: 0.4368, Time: 1.7418 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [50/489], Loss: 0.4593, Time: 1.7463 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [55/489], Loss: 0.4852, Time: 1.7505 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [60/489], Loss: 0.4411, Time: 1.7545 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [65/489], Loss: 0.4893, Time: 1.7583 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [70/489], Loss: 0.4702, Time: 1.7622 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [75/489], Loss: 0.4832, Time: 1.7660 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [80/489], Loss: 0.4592, Time: 1.7699 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [85/489], Loss: 0.4350, Time: 1.7737 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [90/489], Loss: 0.4507, Time: 1.7777 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [95/489], Loss: 0.4421, Time: 1.7816 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [100/489], Loss: 0.4492, Time: 1.7855 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [105/489], Loss: 0.4255, Time: 1.7894 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [110/489], Loss: 0.4412, Time: 1.7933 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [115/489], Loss: 0.4523, Time: 1.7972 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [120/489], Loss: 0.4234, Time: 1.8010 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [125/489], Loss: 0.4480, Time: 1.8048 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [130/489], Loss: 0.4548, Time: 1.8088 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [135/489], Loss: 0.4170, Time: 1.8127 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [140/489], Loss: 0.4239, Time: 1.8165 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [145/489], Loss: 0.5107, Time: 1.8204 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [150/489], Loss: 0.4963, Time: 1.8243 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [155/489], Loss: 0.4675, Time: 1.8282 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [160/489], Loss: 0.4661, Time: 1.8321 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [165/489], Loss: 0.4686, Time: 1.8359 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [170/489], Loss: 0.4191, Time: 1.8399 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [175/489], Loss: 0.4130, Time: 1.8438 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [180/489], Loss: 0.4228, Time: 1.8476 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [185/489], Loss: 0.4283, Time: 1.8515 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [190/489], Loss: 0.4277, Time: 1.8553 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [195/489], Loss: 0.4017, Time: 1.8593 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [200/489], Loss: 0.4118, Time: 1.8637 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [205/489], Loss: 0.4700, Time: 1.8680 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [210/489], Loss: 0.4285, Time: 1.8725 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [215/489], Loss: 0.4199, Time: 1.8771 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [220/489], Loss: 0.4497, Time: 1.8811 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [225/489], Loss: 0.4131, Time: 1.8849 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [230/489], Loss: 0.4049, Time: 1.8888 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [235/489], Loss: 0.4201, Time: 1.8928 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [240/489], Loss: 0.4324, Time: 1.8966 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [245/489], Loss: 0.4223, Time: 1.9005 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [250/489], Loss: 0.4011, Time: 1.9043 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [255/489], Loss: 0.4249, Time: 1.9086 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [260/489], Loss: 0.4728, Time: 1.9125 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [265/489], Loss: 0.4919, Time: 1.9163 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [270/489], Loss: 0.3928, Time: 1.9202 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [275/489], Loss: 0.4241, Time: 1.9241 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [280/489], Loss: 0.5091, Time: 1.9280 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [285/489], Loss: 0.4731, Time: 1.9319 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [290/489], Loss: 0.4131, Time: 1.9357 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [295/489], Loss: 0.5116, Time: 1.9397 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [300/489], Loss: 0.4749, Time: 1.9436 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [305/489], Loss: 0.4233, Time: 1.9474 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [310/489], Loss: 0.5151, Time: 1.9513 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [315/489], Loss: 0.4486, Time: 1.9551 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [320/489], Loss: 0.4197, Time: 1.9590 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [325/489], Loss: 0.4116, Time: 1.9628 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [330/489], Loss: 0.4004, Time: 1.9667 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [335/489], Loss: 0.3968, Time: 1.9706 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [340/489], Loss: 0.3940, Time: 1.9746 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [345/489], Loss: 0.4129, Time: 1.9784 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [350/489], Loss: 0.4439, Time: 1.9822 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [355/489], Loss: 0.4178, Time: 1.9861 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [360/489], Loss: 0.4132, Time: 1.9900 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [365/489], Loss: 0.3827, Time: 1.9938 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [370/489], Loss: 0.3963, Time: 1.9977 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [375/489], Loss: 0.4065, Time: 2.0015 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [380/489], Loss: 0.4193, Time: 2.0054 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [385/489], Loss: 0.3998, Time: 2.0093 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [390/489], Loss: 0.3854, Time: 2.0132 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [395/489], Loss: 0.4060, Time: 2.0170 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [400/489], Loss: 0.3995, Time: 2.0209 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [405/489], Loss: 0.3994, Time: 2.0248 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [410/489], Loss: 0.3897, Time: 2.0287 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [415/489], Loss: 0.4248, Time: 2.0325 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [420/489], Loss: 0.4147, Time: 2.0363 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [425/489], Loss: 0.3790, Time: 2.0403 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [430/489], Loss: 0.3859, Time: 2.0442 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [435/489], Loss: 0.3705, Time: 2.0480 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [440/489], Loss: 0.3965, Time: 2.0519 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [445/489], Loss: 0.3745, Time: 2.0558 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [450/489], Loss: 0.3529, Time: 2.0596 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [455/489], Loss: 0.3788, Time: 2.0635 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [460/489], Loss: 0.3940, Time: 2.0674 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [465/489], Loss: 0.4024, Time: 2.0712 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [470/489], Loss: 0.3790, Time: 2.0752 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [475/489], Loss: 0.3898, Time: 2.0791 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [480/489], Loss: 0.3828, Time: 2.0829 secs, learning rate: 0.0010\n",
      "Epoch [4/30], Step [485/489], Loss: 0.3965, Time: 2.0868 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [5/489], Loss: 0.3892, Time: 2.2164 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [10/489], Loss: 0.3735, Time: 2.2208 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [15/489], Loss: 0.3562, Time: 2.2253 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [20/489], Loss: 0.3897, Time: 2.2294 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [25/489], Loss: 0.4175, Time: 2.2333 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [30/489], Loss: 0.4006, Time: 2.2371 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [35/489], Loss: 0.3755, Time: 2.2410 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [40/489], Loss: 0.3951, Time: 2.2448 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [45/489], Loss: 0.4013, Time: 2.2487 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [50/489], Loss: 0.3812, Time: 2.2525 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [55/489], Loss: 0.3812, Time: 2.2565 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [60/489], Loss: 0.3635, Time: 2.2604 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [65/489], Loss: 0.3566, Time: 2.2642 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [70/489], Loss: 0.4652, Time: 2.2681 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [75/489], Loss: 0.4200, Time: 2.2719 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [80/489], Loss: 0.3647, Time: 2.2759 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [85/489], Loss: 0.3697, Time: 2.2797 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [90/489], Loss: 0.3511, Time: 2.2836 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [95/489], Loss: 0.3809, Time: 2.2874 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [100/489], Loss: 0.3728, Time: 2.2914 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [105/489], Loss: 0.3651, Time: 2.2952 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [110/489], Loss: 0.3630, Time: 2.2991 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [115/489], Loss: 0.3762, Time: 2.3030 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [120/489], Loss: 0.3661, Time: 2.3069 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [125/489], Loss: 0.3616, Time: 2.3108 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [130/489], Loss: 0.3833, Time: 2.3146 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [135/489], Loss: 0.3729, Time: 2.3184 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [140/489], Loss: 0.4382, Time: 2.3224 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [145/489], Loss: 0.4251, Time: 2.3263 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [150/489], Loss: 0.4146, Time: 2.3301 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [155/489], Loss: 0.3936, Time: 2.3340 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [160/489], Loss: 0.3739, Time: 2.3378 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [165/489], Loss: 0.3951, Time: 2.3418 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [170/489], Loss: 0.3684, Time: 2.3457 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [175/489], Loss: 0.3669, Time: 2.3495 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [180/489], Loss: 0.3739, Time: 2.3534 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [185/489], Loss: 0.3659, Time: 2.3573 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [190/489], Loss: 0.3952, Time: 2.3612 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [195/489], Loss: 0.3819, Time: 2.3650 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [200/489], Loss: 0.3604, Time: 2.3688 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [205/489], Loss: 0.3478, Time: 2.3728 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [210/489], Loss: 0.3811, Time: 2.3766 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [215/489], Loss: 0.3703, Time: 2.3805 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [220/489], Loss: 0.3871, Time: 2.3843 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [225/489], Loss: 0.3659, Time: 2.3881 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [230/489], Loss: 0.3482, Time: 2.3922 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [235/489], Loss: 0.3828, Time: 2.3960 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [240/489], Loss: 0.3786, Time: 2.3998 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [245/489], Loss: 0.3680, Time: 2.4037 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [250/489], Loss: 0.3582, Time: 2.4076 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [255/489], Loss: 0.3868, Time: 2.4115 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [260/489], Loss: 0.3814, Time: 2.4154 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [265/489], Loss: 0.3856, Time: 2.4194 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [270/489], Loss: 0.3590, Time: 2.4235 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [275/489], Loss: 0.3615, Time: 2.4273 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [280/489], Loss: 0.3836, Time: 2.4313 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [285/489], Loss: 0.3640, Time: 2.4352 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [290/489], Loss: 0.3717, Time: 2.4391 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [295/489], Loss: 0.3730, Time: 2.4429 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [300/489], Loss: 0.3632, Time: 2.4468 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [305/489], Loss: 0.3528, Time: 2.4506 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [310/489], Loss: 0.3656, Time: 2.4545 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [315/489], Loss: 0.3681, Time: 2.4585 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [320/489], Loss: 0.3638, Time: 2.4623 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [325/489], Loss: 0.3691, Time: 2.4662 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [330/489], Loss: 0.3920, Time: 2.4700 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [335/489], Loss: 0.3433, Time: 2.4740 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [340/489], Loss: 0.3579, Time: 2.4778 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [345/489], Loss: 0.3768, Time: 2.4817 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [350/489], Loss: 0.3759, Time: 2.4855 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [355/489], Loss: 0.3554, Time: 2.4895 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [360/489], Loss: 0.3669, Time: 2.4933 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [365/489], Loss: 0.3417, Time: 2.4972 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [370/489], Loss: 0.3804, Time: 2.5011 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [375/489], Loss: 0.3587, Time: 2.5049 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [380/489], Loss: 0.3508, Time: 2.5089 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [385/489], Loss: 0.3779, Time: 2.5127 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [390/489], Loss: 0.3812, Time: 2.5165 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [395/489], Loss: 0.3532, Time: 2.5204 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [400/489], Loss: 0.3466, Time: 2.5244 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [405/489], Loss: 0.3536, Time: 2.5282 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [410/489], Loss: 0.3672, Time: 2.5321 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [415/489], Loss: 0.3775, Time: 2.5359 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [420/489], Loss: 0.3454, Time: 2.5399 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [425/489], Loss: 0.3399, Time: 2.5437 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [430/489], Loss: 0.3370, Time: 2.5476 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [435/489], Loss: 0.3435, Time: 2.5514 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [440/489], Loss: 0.3286, Time: 2.5553 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [445/489], Loss: 0.3569, Time: 2.5592 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [450/489], Loss: 0.3520, Time: 2.5631 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [455/489], Loss: 0.3572, Time: 2.5670 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [460/489], Loss: 0.3303, Time: 2.5708 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [465/489], Loss: 0.3509, Time: 2.5748 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [470/489], Loss: 0.3345, Time: 2.5786 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [475/489], Loss: 0.3619, Time: 2.5824 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [480/489], Loss: 0.3502, Time: 2.5863 secs, learning rate: 0.0010\n",
      "Epoch [5/30], Step [485/489], Loss: 0.3423, Time: 2.5903 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [5/489], Loss: 0.3347, Time: 2.7186 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [10/489], Loss: 0.3673, Time: 2.7236 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [15/489], Loss: 0.3540, Time: 2.7288 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [20/489], Loss: 0.3283, Time: 2.7336 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [25/489], Loss: 0.3369, Time: 2.7380 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [30/489], Loss: 0.3399, Time: 2.7420 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [35/489], Loss: 0.3699, Time: 2.7458 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [40/489], Loss: 0.3623, Time: 2.7497 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [45/489], Loss: 0.3403, Time: 2.7535 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [50/489], Loss: 0.3367, Time: 2.7574 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [55/489], Loss: 0.3436, Time: 2.7613 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [60/489], Loss: 0.3302, Time: 2.7651 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [65/489], Loss: 0.3764, Time: 2.7690 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [70/489], Loss: 0.3470, Time: 2.7729 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [75/489], Loss: 0.3391, Time: 2.7768 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [80/489], Loss: 0.3433, Time: 2.7812 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [85/489], Loss: 0.3314, Time: 2.7858 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [90/489], Loss: 0.3542, Time: 2.7903 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [95/489], Loss: 0.3303, Time: 2.7947 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [100/489], Loss: 0.3472, Time: 2.7986 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [105/489], Loss: 0.3305, Time: 2.8025 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [110/489], Loss: 0.3620, Time: 2.8063 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [115/489], Loss: 0.3414, Time: 2.8101 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [120/489], Loss: 0.3283, Time: 2.8140 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [125/489], Loss: 0.3379, Time: 2.8178 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [130/489], Loss: 0.3793, Time: 2.8217 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [135/489], Loss: 0.3385, Time: 2.8255 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [140/489], Loss: 0.3322, Time: 2.8295 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [145/489], Loss: 0.3271, Time: 2.8334 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [150/489], Loss: 0.3386, Time: 2.8372 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [155/489], Loss: 0.3334, Time: 2.8412 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [160/489], Loss: 0.3233, Time: 2.8451 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [165/489], Loss: 0.3420, Time: 2.8489 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [170/489], Loss: 0.3544, Time: 2.8528 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [175/489], Loss: 0.3364, Time: 2.8567 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [180/489], Loss: 0.3387, Time: 2.8607 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [185/489], Loss: 0.3310, Time: 2.8652 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [190/489], Loss: 0.3277, Time: 2.8694 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [195/489], Loss: 0.3274, Time: 2.8740 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [200/489], Loss: 0.3483, Time: 2.8781 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [205/489], Loss: 0.4088, Time: 2.8820 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [210/489], Loss: 0.3581, Time: 2.8859 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [215/489], Loss: 0.3350, Time: 2.8899 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [220/489], Loss: 0.3402, Time: 2.8937 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [225/489], Loss: 0.3468, Time: 2.8975 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [230/489], Loss: 0.3169, Time: 2.9014 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [235/489], Loss: 0.3312, Time: 2.9052 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [240/489], Loss: 0.3343, Time: 2.9092 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [245/489], Loss: 0.3825, Time: 2.9130 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [250/489], Loss: 0.3742, Time: 2.9168 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [255/489], Loss: 0.4056, Time: 2.9209 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [260/489], Loss: 0.3855, Time: 2.9248 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [265/489], Loss: 0.3351, Time: 2.9287 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [270/489], Loss: 0.3298, Time: 2.9326 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [275/489], Loss: 0.3284, Time: 2.9365 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [280/489], Loss: 0.3406, Time: 2.9404 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [285/489], Loss: 0.3513, Time: 2.9442 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [290/489], Loss: 0.3395, Time: 2.9481 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [295/489], Loss: 0.3264, Time: 2.9519 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [300/489], Loss: 0.3338, Time: 2.9558 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [305/489], Loss: 0.3234, Time: 2.9597 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [310/489], Loss: 0.3276, Time: 2.9635 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [315/489], Loss: 0.3147, Time: 2.9674 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [320/489], Loss: 0.3220, Time: 2.9712 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [325/489], Loss: 0.3193, Time: 2.9752 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [330/489], Loss: 0.3196, Time: 2.9791 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [335/489], Loss: 0.3364, Time: 2.9829 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [340/489], Loss: 0.3414, Time: 2.9868 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [345/489], Loss: 0.3160, Time: 2.9907 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [350/489], Loss: 0.3756, Time: 2.9945 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [355/489], Loss: 0.3385, Time: 2.9984 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [360/489], Loss: 0.3335, Time: 3.0022 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [365/489], Loss: 0.3133, Time: 3.0062 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [370/489], Loss: 0.3182, Time: 3.0101 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [375/489], Loss: 0.3342, Time: 3.0139 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [380/489], Loss: 0.3201, Time: 3.0178 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [385/489], Loss: 0.3326, Time: 3.0216 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [390/489], Loss: 0.3383, Time: 3.0255 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [395/489], Loss: 0.3129, Time: 3.0294 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [400/489], Loss: 0.3217, Time: 3.0332 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [405/489], Loss: 0.3392, Time: 3.0371 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [410/489], Loss: 0.3282, Time: 3.0411 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [415/489], Loss: 0.3256, Time: 3.0449 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [420/489], Loss: 0.3123, Time: 3.0487 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [425/489], Loss: 0.3139, Time: 3.0526 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [430/489], Loss: 0.3166, Time: 3.0566 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [435/489], Loss: 0.3815, Time: 3.0604 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [440/489], Loss: 0.3087, Time: 3.0643 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [445/489], Loss: 0.3305, Time: 3.0681 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [450/489], Loss: 0.3268, Time: 3.0720 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [455/489], Loss: 0.3162, Time: 3.0759 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [460/489], Loss: 0.3189, Time: 3.0797 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [465/489], Loss: 0.3331, Time: 3.0836 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [470/489], Loss: 0.3209, Time: 3.0874 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [475/489], Loss: 0.3329, Time: 3.0914 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [480/489], Loss: 0.3346, Time: 3.0953 secs, learning rate: 0.0010\n",
      "Epoch [6/30], Step [485/489], Loss: 0.3295, Time: 3.0991 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [5/489], Loss: 0.3264, Time: 3.2286 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [10/489], Loss: 0.3442, Time: 3.2335 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [15/489], Loss: 0.3354, Time: 3.2388 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [20/489], Loss: 0.3165, Time: 3.2436 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [25/489], Loss: 0.3359, Time: 3.2479 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [30/489], Loss: 0.3451, Time: 3.2518 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [35/489], Loss: 0.2956, Time: 3.2558 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [40/489], Loss: 0.3421, Time: 3.2596 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [45/489], Loss: 0.3347, Time: 3.2635 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [50/489], Loss: 0.3145, Time: 3.2673 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [55/489], Loss: 0.3187, Time: 3.2712 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [60/489], Loss: 0.3171, Time: 3.2751 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [65/489], Loss: 0.3097, Time: 3.2789 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [70/489], Loss: 0.3068, Time: 3.2828 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [75/489], Loss: 0.3130, Time: 3.2867 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [80/489], Loss: 0.3112, Time: 3.2906 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [85/489], Loss: 0.3527, Time: 3.2945 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [90/489], Loss: 0.3146, Time: 3.2983 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [95/489], Loss: 0.3134, Time: 3.3022 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [100/489], Loss: 0.3187, Time: 3.3062 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [105/489], Loss: 0.3385, Time: 3.3100 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [110/489], Loss: 0.3367, Time: 3.3138 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [115/489], Loss: 0.3307, Time: 3.3177 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [120/489], Loss: 0.3002, Time: 3.3215 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [125/489], Loss: 0.3179, Time: 3.3255 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [130/489], Loss: 0.3153, Time: 3.3293 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [135/489], Loss: 0.3266, Time: 3.3332 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [140/489], Loss: 0.3211, Time: 3.3370 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [145/489], Loss: 0.3017, Time: 3.3410 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [150/489], Loss: 0.3339, Time: 3.3448 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [155/489], Loss: 0.3273, Time: 3.3487 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [160/489], Loss: 0.3123, Time: 3.3525 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [165/489], Loss: 0.3241, Time: 3.3565 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [170/489], Loss: 0.3648, Time: 3.3604 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [175/489], Loss: 0.3062, Time: 3.3642 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [180/489], Loss: 0.3199, Time: 3.3680 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [185/489], Loss: 0.3793, Time: 3.3718 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [190/489], Loss: 0.3312, Time: 3.3759 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [195/489], Loss: 0.3189, Time: 3.3797 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [200/489], Loss: 0.3198, Time: 3.3836 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [205/489], Loss: 0.3820, Time: 3.3874 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [210/489], Loss: 0.3748, Time: 3.3914 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [215/489], Loss: 0.3113, Time: 3.3953 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [220/489], Loss: 0.4738, Time: 3.3991 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [225/489], Loss: 0.3556, Time: 3.4030 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [230/489], Loss: 0.3562, Time: 3.4070 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [235/489], Loss: 0.4837, Time: 3.4109 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [240/489], Loss: 0.3263, Time: 3.4147 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [245/489], Loss: 0.3406, Time: 3.4186 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [250/489], Loss: 0.3172, Time: 3.4225 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [255/489], Loss: 0.3275, Time: 3.4263 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [260/489], Loss: 0.3254, Time: 3.4304 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [265/489], Loss: 0.3162, Time: 3.4343 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [270/489], Loss: 0.3573, Time: 3.4382 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [275/489], Loss: 0.3372, Time: 3.4420 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [280/489], Loss: 0.3201, Time: 3.4459 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [285/489], Loss: 0.3113, Time: 3.4498 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [290/489], Loss: 0.3143, Time: 3.4536 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [295/489], Loss: 0.3035, Time: 3.4576 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [300/489], Loss: 0.3233, Time: 3.4615 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [305/489], Loss: 0.3126, Time: 3.4653 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [310/489], Loss: 0.3501, Time: 3.4692 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [315/489], Loss: 0.3089, Time: 3.4732 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [320/489], Loss: 0.3253, Time: 3.4771 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [325/489], Loss: 0.3114, Time: 3.4809 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [330/489], Loss: 0.3286, Time: 3.4848 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [335/489], Loss: 0.3282, Time: 3.4886 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [340/489], Loss: 0.3079, Time: 3.4926 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [345/489], Loss: 0.3259, Time: 3.4965 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [350/489], Loss: 0.3771, Time: 3.5003 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [355/489], Loss: 0.3026, Time: 3.5042 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [360/489], Loss: 0.3648, Time: 3.5081 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [365/489], Loss: 0.3118, Time: 3.5120 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [370/489], Loss: 0.3186, Time: 3.5158 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [375/489], Loss: 0.2990, Time: 3.5197 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [380/489], Loss: 0.3059, Time: 3.5236 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [385/489], Loss: 0.3175, Time: 3.5275 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [390/489], Loss: 0.3317, Time: 3.5313 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [395/489], Loss: 0.3063, Time: 3.5352 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [400/489], Loss: 0.3564, Time: 3.5391 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [405/489], Loss: 0.3356, Time: 3.5430 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [410/489], Loss: 0.3079, Time: 3.5468 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [415/489], Loss: 0.4000, Time: 3.5507 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [420/489], Loss: 0.3092, Time: 3.5545 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [425/489], Loss: 0.3925, Time: 3.5585 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [430/489], Loss: 0.2971, Time: 3.5624 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [435/489], Loss: 0.3008, Time: 3.5662 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [440/489], Loss: 0.3198, Time: 3.5700 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [445/489], Loss: 0.3189, Time: 3.5740 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [450/489], Loss: 0.3137, Time: 3.5779 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [455/489], Loss: 0.3094, Time: 3.5817 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [460/489], Loss: 0.3032, Time: 3.5856 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [465/489], Loss: 0.3051, Time: 3.5896 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [470/489], Loss: 0.3256, Time: 3.5934 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [475/489], Loss: 0.3005, Time: 3.5973 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [480/489], Loss: 0.3161, Time: 3.6012 secs, learning rate: 0.0010\n",
      "Epoch [7/30], Step [485/489], Loss: 0.3140, Time: 3.6051 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [5/489], Loss: 0.4356, Time: 3.7340 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [10/489], Loss: 0.3180, Time: 3.7383 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [15/489], Loss: 0.3810, Time: 3.7435 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [20/489], Loss: 0.3042, Time: 3.7479 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [25/489], Loss: 0.3137, Time: 3.7522 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [30/489], Loss: 0.3642, Time: 3.7562 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [35/489], Loss: 0.3085, Time: 3.7600 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [40/489], Loss: 0.3224, Time: 3.7639 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [45/489], Loss: 0.3016, Time: 3.7677 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [50/489], Loss: 0.3179, Time: 3.7716 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [55/489], Loss: 0.3008, Time: 3.7755 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [60/489], Loss: 0.3041, Time: 3.7794 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [65/489], Loss: 0.3150, Time: 3.7832 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [70/489], Loss: 0.3182, Time: 3.7871 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [75/489], Loss: 0.3121, Time: 3.7910 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [80/489], Loss: 0.3296, Time: 3.7948 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [85/489], Loss: 0.3117, Time: 3.7987 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [90/489], Loss: 0.3451, Time: 3.8025 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [95/489], Loss: 0.3099, Time: 3.8065 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [100/489], Loss: 0.3528, Time: 3.8103 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [105/489], Loss: 0.3545, Time: 3.8142 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [110/489], Loss: 0.3116, Time: 3.8180 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [115/489], Loss: 0.3311, Time: 3.8219 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [120/489], Loss: 0.3227, Time: 3.8259 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [125/489], Loss: 0.2994, Time: 3.8298 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [130/489], Loss: 0.3064, Time: 3.8337 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [135/489], Loss: 0.3382, Time: 3.8376 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [140/489], Loss: 0.3399, Time: 3.8415 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [145/489], Loss: 0.3107, Time: 3.8453 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [150/489], Loss: 0.3801, Time: 3.8491 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [155/489], Loss: 0.3037, Time: 3.8530 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [160/489], Loss: 0.3408, Time: 3.8568 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [165/489], Loss: 0.3065, Time: 3.8607 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [170/489], Loss: 0.3804, Time: 3.8645 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [175/489], Loss: 0.2883, Time: 3.8683 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [180/489], Loss: 0.3482, Time: 3.8721 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [185/489], Loss: 0.3306, Time: 3.8760 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [190/489], Loss: 0.3357, Time: 3.8798 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [195/489], Loss: 0.3663, Time: 3.8838 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [200/489], Loss: 0.3235, Time: 3.8876 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [205/489], Loss: 0.3113, Time: 3.8915 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [210/489], Loss: 0.3674, Time: 3.8954 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [215/489], Loss: 0.3426, Time: 3.8993 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [220/489], Loss: 0.3027, Time: 3.9031 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [225/489], Loss: 0.3130, Time: 3.9071 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [230/489], Loss: 0.2984, Time: 3.9109 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [235/489], Loss: 0.3143, Time: 3.9148 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [240/489], Loss: 0.3169, Time: 3.9186 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [245/489], Loss: 0.3091, Time: 3.9225 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [250/489], Loss: 0.3073, Time: 3.9263 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [255/489], Loss: 0.4189, Time: 3.9302 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [260/489], Loss: 0.3191, Time: 3.9341 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [265/489], Loss: 0.3438, Time: 3.9381 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [270/489], Loss: 0.2986, Time: 3.9420 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [275/489], Loss: 0.3263, Time: 3.9459 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [280/489], Loss: 0.2973, Time: 3.9498 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [285/489], Loss: 0.3247, Time: 3.9537 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [290/489], Loss: 0.3732, Time: 3.9577 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [295/489], Loss: 0.2943, Time: 3.9616 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [300/489], Loss: 0.2943, Time: 3.9654 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [305/489], Loss: 0.3081, Time: 3.9693 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [310/489], Loss: 0.4212, Time: 3.9732 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [315/489], Loss: 0.3031, Time: 3.9771 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [320/489], Loss: 0.3017, Time: 3.9809 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [325/489], Loss: 0.3516, Time: 3.9847 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [330/489], Loss: 0.3101, Time: 3.9886 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [335/489], Loss: 0.4247, Time: 3.9925 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [340/489], Loss: 0.3636, Time: 3.9964 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [345/489], Loss: 0.3003, Time: 4.0002 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [350/489], Loss: 0.3177, Time: 4.0041 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [355/489], Loss: 0.3323, Time: 4.0081 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [360/489], Loss: 0.3043, Time: 4.0119 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [365/489], Loss: 0.3360, Time: 4.0158 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [370/489], Loss: 0.3555, Time: 4.0196 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [375/489], Loss: 0.3128, Time: 4.0236 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [380/489], Loss: 0.2980, Time: 4.0275 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [385/489], Loss: 0.3195, Time: 4.0313 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [390/489], Loss: 0.3034, Time: 4.0351 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [395/489], Loss: 0.3143, Time: 4.0391 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [400/489], Loss: 0.2943, Time: 4.0430 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [405/489], Loss: 0.3367, Time: 4.0468 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [410/489], Loss: 0.2989, Time: 4.0507 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [415/489], Loss: 0.3121, Time: 4.0545 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [420/489], Loss: 0.3039, Time: 4.0585 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [425/489], Loss: 0.3095, Time: 4.0624 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [430/489], Loss: 0.3100, Time: 4.0662 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [435/489], Loss: 0.3066, Time: 4.0701 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [440/489], Loss: 0.2900, Time: 4.0740 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [445/489], Loss: 0.3188, Time: 4.0779 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [450/489], Loss: 0.2998, Time: 4.0817 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [455/489], Loss: 0.3342, Time: 4.0855 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [460/489], Loss: 0.3825, Time: 4.0896 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [465/489], Loss: 0.4122, Time: 4.0934 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [470/489], Loss: 0.3044, Time: 4.0972 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [475/489], Loss: 0.2929, Time: 4.1011 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [480/489], Loss: 0.3924, Time: 4.1049 secs, learning rate: 0.0010\n",
      "Epoch [8/30], Step [485/489], Loss: 0.4097, Time: 4.1089 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [5/489], Loss: 0.4622, Time: 4.2380 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [10/489], Loss: 0.4657, Time: 4.2428 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [15/489], Loss: 0.3098, Time: 4.2477 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [20/489], Loss: 0.3858, Time: 4.2523 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [25/489], Loss: 0.3484, Time: 4.2566 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [30/489], Loss: 0.2929, Time: 4.2604 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [35/489], Loss: 0.3190, Time: 4.2643 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [40/489], Loss: 0.3048, Time: 4.2682 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [45/489], Loss: 0.3001, Time: 4.2720 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [50/489], Loss: 0.3240, Time: 4.2760 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [55/489], Loss: 0.2855, Time: 4.2798 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [60/489], Loss: 0.2884, Time: 4.2837 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [65/489], Loss: 0.3075, Time: 4.2875 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [70/489], Loss: 0.3423, Time: 4.2915 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [75/489], Loss: 0.3025, Time: 4.2953 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [80/489], Loss: 0.3427, Time: 4.2991 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [85/489], Loss: 0.3407, Time: 4.3030 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [90/489], Loss: 0.2953, Time: 4.3070 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [95/489], Loss: 0.3199, Time: 4.3108 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [100/489], Loss: 0.2935, Time: 4.3147 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [105/489], Loss: 0.2881, Time: 4.3186 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [110/489], Loss: 0.3551, Time: 4.3225 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [115/489], Loss: 0.3086, Time: 4.3263 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [120/489], Loss: 0.3334, Time: 4.3302 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [125/489], Loss: 0.3048, Time: 4.3340 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [130/489], Loss: 0.3056, Time: 4.3378 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [135/489], Loss: 0.3065, Time: 4.3418 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [140/489], Loss: 0.3003, Time: 4.3457 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [145/489], Loss: 0.3487, Time: 4.3495 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [150/489], Loss: 0.3112, Time: 4.3534 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [155/489], Loss: 0.3431, Time: 4.3574 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [160/489], Loss: 0.3132, Time: 4.3612 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [165/489], Loss: 0.2837, Time: 4.3651 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [170/489], Loss: 0.3009, Time: 4.3689 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [175/489], Loss: 0.2941, Time: 4.3729 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [180/489], Loss: 0.2951, Time: 4.3767 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [185/489], Loss: 0.3560, Time: 4.3806 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [190/489], Loss: 0.2910, Time: 4.3844 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [195/489], Loss: 0.2916, Time: 4.3882 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [200/489], Loss: 0.2952, Time: 4.3922 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [205/489], Loss: 0.2919, Time: 4.3960 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [210/489], Loss: 0.3060, Time: 4.3998 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [215/489], Loss: 0.2882, Time: 4.4037 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [220/489], Loss: 0.3034, Time: 4.4077 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [225/489], Loss: 0.3450, Time: 4.4115 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [230/489], Loss: 0.3101, Time: 4.4154 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [235/489], Loss: 0.2944, Time: 4.4192 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [240/489], Loss: 0.4444, Time: 4.4232 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [245/489], Loss: 0.3321, Time: 4.4270 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [250/489], Loss: 0.3685, Time: 4.4309 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [255/489], Loss: 0.2898, Time: 4.4347 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [260/489], Loss: 0.3159, Time: 4.4388 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [265/489], Loss: 0.3361, Time: 4.4427 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [270/489], Loss: 0.3133, Time: 4.4465 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [275/489], Loss: 0.3662, Time: 4.4504 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [280/489], Loss: 0.4580, Time: 4.4543 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [285/489], Loss: 0.2925, Time: 4.4582 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [290/489], Loss: 0.2943, Time: 4.4621 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [295/489], Loss: 0.2939, Time: 4.4659 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [300/489], Loss: 0.3074, Time: 4.4698 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [305/489], Loss: 0.2850, Time: 4.4738 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [310/489], Loss: 0.2842, Time: 4.4776 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [315/489], Loss: 0.2923, Time: 4.4815 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [320/489], Loss: 0.2951, Time: 4.4853 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [325/489], Loss: 0.3423, Time: 4.4892 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [330/489], Loss: 0.3273, Time: 4.4931 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [335/489], Loss: 0.3330, Time: 4.4969 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [340/489], Loss: 0.3160, Time: 4.5007 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [345/489], Loss: 0.2881, Time: 4.5046 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [350/489], Loss: 0.2880, Time: 4.5086 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [355/489], Loss: 0.3087, Time: 4.5124 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [360/489], Loss: 0.3191, Time: 4.5163 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [365/489], Loss: 0.2817, Time: 4.5201 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [370/489], Loss: 0.2923, Time: 4.5241 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [375/489], Loss: 0.2949, Time: 4.5280 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [380/489], Loss: 0.2878, Time: 4.5319 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [385/489], Loss: 0.2801, Time: 4.5360 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [390/489], Loss: 0.2807, Time: 4.5399 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [395/489], Loss: 0.2930, Time: 4.5438 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [400/489], Loss: 0.2830, Time: 4.5476 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [405/489], Loss: 0.2849, Time: 4.5515 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [410/489], Loss: 0.2832, Time: 4.5553 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [415/489], Loss: 0.2861, Time: 4.5593 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [420/489], Loss: 0.2915, Time: 4.5631 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [425/489], Loss: 0.2891, Time: 4.5670 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [430/489], Loss: 0.3578, Time: 4.5708 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [435/489], Loss: 0.4228, Time: 4.5748 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [440/489], Loss: 0.3813, Time: 4.5786 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [445/489], Loss: 0.3566, Time: 4.5825 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [450/489], Loss: 0.3001, Time: 4.5863 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [455/489], Loss: 0.3448, Time: 4.5903 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [460/489], Loss: 0.3128, Time: 4.5942 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [465/489], Loss: 0.2924, Time: 4.5981 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [470/489], Loss: 0.2916, Time: 4.6019 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [475/489], Loss: 0.2915, Time: 4.6058 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [480/489], Loss: 0.2858, Time: 4.6097 secs, learning rate: 0.0010\n",
      "Epoch [9/30], Step [485/489], Loss: 0.2972, Time: 4.6136 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [5/489], Loss: 0.3106, Time: 4.7425 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [10/489], Loss: 0.2895, Time: 4.7473 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [15/489], Loss: 0.2982, Time: 4.7525 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [20/489], Loss: 0.2838, Time: 4.7569 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [25/489], Loss: 0.2896, Time: 4.7612 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [30/489], Loss: 0.2862, Time: 4.7651 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [35/489], Loss: 0.3051, Time: 4.7689 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [40/489], Loss: 0.2973, Time: 4.7729 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [45/489], Loss: 0.3238, Time: 4.7767 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [50/489], Loss: 0.2884, Time: 4.7806 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [55/489], Loss: 0.2869, Time: 4.7844 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [60/489], Loss: 0.2780, Time: 4.7883 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [65/489], Loss: 0.2898, Time: 4.7922 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [70/489], Loss: 0.3171, Time: 4.7961 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [75/489], Loss: 0.3385, Time: 4.7999 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [80/489], Loss: 0.3111, Time: 4.8037 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [85/489], Loss: 0.3184, Time: 4.8077 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [90/489], Loss: 0.3216, Time: 4.8115 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [95/489], Loss: 0.2929, Time: 4.8154 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [100/489], Loss: 0.2808, Time: 4.8192 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [105/489], Loss: 0.2910, Time: 4.8232 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [110/489], Loss: 0.2898, Time: 4.8271 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [115/489], Loss: 0.2851, Time: 4.8309 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [120/489], Loss: 0.2899, Time: 4.8348 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [125/489], Loss: 0.3406, Time: 4.8386 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [130/489], Loss: 0.3230, Time: 4.8426 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [135/489], Loss: 0.3298, Time: 4.8464 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [140/489], Loss: 0.3520, Time: 4.8503 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [145/489], Loss: 0.2934, Time: 4.8541 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [150/489], Loss: 0.2882, Time: 4.8581 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [155/489], Loss: 0.2835, Time: 4.8620 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [160/489], Loss: 0.2907, Time: 4.8659 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [165/489], Loss: 0.2826, Time: 4.8697 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [170/489], Loss: 0.2743, Time: 4.8737 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [175/489], Loss: 0.3334, Time: 4.8776 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [180/489], Loss: 0.3047, Time: 4.8815 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [185/489], Loss: 0.2998, Time: 4.8853 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [190/489], Loss: 0.3313, Time: 4.8892 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [195/489], Loss: 0.2887, Time: 4.8931 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [200/489], Loss: 0.2868, Time: 4.8970 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [205/489], Loss: 0.2796, Time: 4.9008 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [210/489], Loss: 0.2813, Time: 4.9047 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [215/489], Loss: 0.2922, Time: 4.9085 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [220/489], Loss: 0.2863, Time: 4.9124 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [225/489], Loss: 0.2841, Time: 4.9162 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [230/489], Loss: 0.2870, Time: 4.9201 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [235/489], Loss: 0.2937, Time: 4.9239 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [240/489], Loss: 0.2954, Time: 4.9277 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [245/489], Loss: 0.3847, Time: 4.9316 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [250/489], Loss: 0.3437, Time: 4.9355 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [255/489], Loss: 0.2792, Time: 4.9393 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [260/489], Loss: 0.3744, Time: 4.9433 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [265/489], Loss: 0.3546, Time: 4.9473 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [270/489], Loss: 0.3145, Time: 4.9511 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [275/489], Loss: 0.2947, Time: 4.9550 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [280/489], Loss: 0.3397, Time: 4.9590 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [285/489], Loss: 0.3279, Time: 4.9629 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [290/489], Loss: 0.2867, Time: 4.9667 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [295/489], Loss: 0.3096, Time: 4.9706 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [300/489], Loss: 0.2962, Time: 4.9746 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [305/489], Loss: 0.3372, Time: 4.9784 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [310/489], Loss: 0.2966, Time: 4.9823 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [315/489], Loss: 0.2812, Time: 4.9861 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [320/489], Loss: 0.3079, Time: 4.9900 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [325/489], Loss: 0.2829, Time: 4.9938 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [330/489], Loss: 0.3867, Time: 4.9977 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [335/489], Loss: 0.3018, Time: 5.0015 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [340/489], Loss: 0.2882, Time: 5.0054 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [345/489], Loss: 0.2940, Time: 5.0094 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [350/489], Loss: 0.2870, Time: 5.0132 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [355/489], Loss: 0.2790, Time: 5.0171 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [360/489], Loss: 0.2834, Time: 5.0209 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [365/489], Loss: 0.2866, Time: 5.0248 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [370/489], Loss: 0.3017, Time: 5.0287 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [375/489], Loss: 0.3874, Time: 5.0326 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [380/489], Loss: 0.5403, Time: 5.0364 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [385/489], Loss: 0.3999, Time: 5.0404 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [390/489], Loss: 0.2973, Time: 5.0443 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [395/489], Loss: 0.2870, Time: 5.0481 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [400/489], Loss: 0.2855, Time: 5.0519 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [405/489], Loss: 0.2760, Time: 5.0558 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [410/489], Loss: 0.3526, Time: 5.0597 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [415/489], Loss: 0.2891, Time: 5.0635 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [420/489], Loss: 0.2828, Time: 5.0674 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [425/489], Loss: 0.3146, Time: 5.0712 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [430/489], Loss: 0.3514, Time: 5.0753 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [435/489], Loss: 0.2905, Time: 5.0791 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [440/489], Loss: 0.2901, Time: 5.0830 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [445/489], Loss: 0.2956, Time: 5.0868 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [450/489], Loss: 0.3550, Time: 5.0907 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [455/489], Loss: 0.3364, Time: 5.0945 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [460/489], Loss: 0.2828, Time: 5.0984 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [465/489], Loss: 0.2818, Time: 5.1022 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [470/489], Loss: 0.2946, Time: 5.1063 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [475/489], Loss: 0.3054, Time: 5.1101 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [480/489], Loss: 0.2853, Time: 5.1140 secs, learning rate: 0.0010\n",
      "Epoch [10/30], Step [485/489], Loss: 0.3336, Time: 5.1178 secs, learning rate: 0.0010\n",
      "Epoch [11/30], Step [5/489], Loss: 0.2921, Time: 5.2468 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [10/489], Loss: 0.2753, Time: 5.2513 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [15/489], Loss: 0.2791, Time: 5.2564 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [20/489], Loss: 0.2762, Time: 5.2610 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [25/489], Loss: 0.2812, Time: 5.2653 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [30/489], Loss: 0.2704, Time: 5.2691 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [35/489], Loss: 0.2782, Time: 5.2731 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [40/489], Loss: 0.2773, Time: 5.2770 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [45/489], Loss: 0.2680, Time: 5.2808 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [50/489], Loss: 0.2721, Time: 5.2847 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [55/489], Loss: 0.2773, Time: 5.2885 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [60/489], Loss: 0.2759, Time: 5.2925 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [65/489], Loss: 0.2834, Time: 5.2964 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [70/489], Loss: 0.2760, Time: 5.3002 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [75/489], Loss: 0.2753, Time: 5.3041 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [80/489], Loss: 0.2723, Time: 5.3081 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [85/489], Loss: 0.2723, Time: 5.3119 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [90/489], Loss: 0.2739, Time: 5.3158 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [95/489], Loss: 0.2775, Time: 5.3196 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [100/489], Loss: 0.2756, Time: 5.3236 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [105/489], Loss: 0.2743, Time: 5.3274 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [110/489], Loss: 0.2694, Time: 5.3313 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [115/489], Loss: 0.2845, Time: 5.3351 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [120/489], Loss: 0.2846, Time: 5.3391 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [125/489], Loss: 0.2775, Time: 5.3430 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [130/489], Loss: 0.2807, Time: 5.3468 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [135/489], Loss: 0.2726, Time: 5.3507 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [140/489], Loss: 0.2756, Time: 5.3545 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [145/489], Loss: 0.2811, Time: 5.3585 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [150/489], Loss: 0.2762, Time: 5.3623 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [155/489], Loss: 0.2752, Time: 5.3661 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [160/489], Loss: 0.2718, Time: 5.3700 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [165/489], Loss: 0.2693, Time: 5.3740 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [170/489], Loss: 0.2744, Time: 5.3779 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [175/489], Loss: 0.2742, Time: 5.3817 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [180/489], Loss: 0.2757, Time: 5.3855 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [185/489], Loss: 0.2756, Time: 5.3895 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [190/489], Loss: 0.2745, Time: 5.3933 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [195/489], Loss: 0.2758, Time: 5.3972 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [200/489], Loss: 0.2726, Time: 5.4010 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [205/489], Loss: 0.2750, Time: 5.4049 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [210/489], Loss: 0.2764, Time: 5.4089 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [215/489], Loss: 0.2742, Time: 5.4128 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [220/489], Loss: 0.2752, Time: 5.4166 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [225/489], Loss: 0.2672, Time: 5.4204 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [230/489], Loss: 0.2770, Time: 5.4244 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [235/489], Loss: 0.2779, Time: 5.4283 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [240/489], Loss: 0.2764, Time: 5.4321 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [245/489], Loss: 0.2741, Time: 5.4359 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [250/489], Loss: 0.2759, Time: 5.4398 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [255/489], Loss: 0.2719, Time: 5.4436 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [260/489], Loss: 0.2724, Time: 5.4477 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [265/489], Loss: 0.2751, Time: 5.4516 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [270/489], Loss: 0.2744, Time: 5.4555 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [275/489], Loss: 0.2761, Time: 5.4595 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [280/489], Loss: 0.2741, Time: 5.4633 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [285/489], Loss: 0.2710, Time: 5.4673 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [290/489], Loss: 0.2684, Time: 5.4711 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [295/489], Loss: 0.2738, Time: 5.4751 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [300/489], Loss: 0.2681, Time: 5.4789 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [305/489], Loss: 0.2741, Time: 5.4828 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [310/489], Loss: 0.2687, Time: 5.4866 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [315/489], Loss: 0.2730, Time: 5.4906 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [320/489], Loss: 0.2679, Time: 5.4945 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [325/489], Loss: 0.2753, Time: 5.4983 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [330/489], Loss: 0.2746, Time: 5.5022 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [335/489], Loss: 0.2762, Time: 5.5061 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [340/489], Loss: 0.2699, Time: 5.5100 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [345/489], Loss: 0.2777, Time: 5.5139 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [350/489], Loss: 0.2751, Time: 5.5177 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [355/489], Loss: 0.2714, Time: 5.5215 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [360/489], Loss: 0.2726, Time: 5.5255 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [365/489], Loss: 0.2762, Time: 5.5294 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [370/489], Loss: 0.2760, Time: 5.5332 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [375/489], Loss: 0.2729, Time: 5.5373 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [380/489], Loss: 0.2737, Time: 5.5417 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [385/489], Loss: 0.2740, Time: 5.5459 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [390/489], Loss: 0.2722, Time: 5.5504 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [395/489], Loss: 0.2825, Time: 5.5549 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [400/489], Loss: 0.2735, Time: 5.5589 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [405/489], Loss: 0.2752, Time: 5.5628 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [410/489], Loss: 0.2812, Time: 5.5666 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [415/489], Loss: 0.2768, Time: 5.5705 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [420/489], Loss: 0.2771, Time: 5.5745 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [425/489], Loss: 0.2722, Time: 5.5784 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [430/489], Loss: 0.2721, Time: 5.5827 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [435/489], Loss: 0.2742, Time: 5.5866 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [440/489], Loss: 0.2742, Time: 5.5912 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [445/489], Loss: 0.2758, Time: 5.5965 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [450/489], Loss: 0.2738, Time: 5.6006 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [455/489], Loss: 0.2701, Time: 5.6045 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [460/489], Loss: 0.2677, Time: 5.6085 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [465/489], Loss: 0.2765, Time: 5.6123 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [470/489], Loss: 0.2744, Time: 5.6162 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [475/489], Loss: 0.2729, Time: 5.6201 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [480/489], Loss: 0.2757, Time: 5.6239 secs, learning rate: 0.0001\n",
      "Epoch [11/30], Step [485/489], Loss: 0.2749, Time: 5.6278 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [5/489], Loss: 0.2714, Time: 5.7572 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [10/489], Loss: 0.2722, Time: 5.7621 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [15/489], Loss: 0.2763, Time: 5.7672 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [20/489], Loss: 0.2770, Time: 5.7721 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [25/489], Loss: 0.2774, Time: 5.7767 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [30/489], Loss: 0.2749, Time: 5.7806 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [35/489], Loss: 0.2760, Time: 5.7845 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [40/489], Loss: 0.2762, Time: 5.7883 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [45/489], Loss: 0.2747, Time: 5.7922 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [50/489], Loss: 0.2695, Time: 5.7961 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [55/489], Loss: 0.2750, Time: 5.7999 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [60/489], Loss: 0.2691, Time: 5.8038 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [65/489], Loss: 0.2735, Time: 5.8078 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [70/489], Loss: 0.2744, Time: 5.8116 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [75/489], Loss: 0.2738, Time: 5.8155 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [80/489], Loss: 0.2673, Time: 5.8193 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [85/489], Loss: 0.2704, Time: 5.8233 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [90/489], Loss: 0.2779, Time: 5.8271 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [95/489], Loss: 0.2719, Time: 5.8310 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [100/489], Loss: 0.2714, Time: 5.8348 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [105/489], Loss: 0.2775, Time: 5.8387 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [110/489], Loss: 0.2745, Time: 5.8427 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [115/489], Loss: 0.2802, Time: 5.8465 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [120/489], Loss: 0.2778, Time: 5.8504 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [125/489], Loss: 0.2765, Time: 5.8542 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [130/489], Loss: 0.2787, Time: 5.8581 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [135/489], Loss: 0.2736, Time: 5.8620 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [140/489], Loss: 0.2655, Time: 5.8658 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [145/489], Loss: 0.2725, Time: 5.8697 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [150/489], Loss: 0.2734, Time: 5.8736 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [155/489], Loss: 0.2769, Time: 5.8775 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [160/489], Loss: 0.2728, Time: 5.8813 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [165/489], Loss: 0.2854, Time: 5.8852 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [170/489], Loss: 0.2751, Time: 5.8892 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [175/489], Loss: 0.2717, Time: 5.8930 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [180/489], Loss: 0.2768, Time: 5.8968 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [185/489], Loss: 0.2737, Time: 5.9007 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [190/489], Loss: 0.2825, Time: 5.9045 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [195/489], Loss: 0.2742, Time: 5.9085 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [200/489], Loss: 0.2789, Time: 5.9123 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [205/489], Loss: 0.2848, Time: 5.9162 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [210/489], Loss: 0.2818, Time: 5.9200 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [215/489], Loss: 0.2688, Time: 5.9240 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [220/489], Loss: 0.2726, Time: 5.9279 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [225/489], Loss: 0.2730, Time: 5.9317 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [230/489], Loss: 0.2741, Time: 5.9356 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [235/489], Loss: 0.2693, Time: 5.9394 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [240/489], Loss: 0.2766, Time: 5.9433 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [245/489], Loss: 0.2756, Time: 5.9472 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [250/489], Loss: 0.2743, Time: 5.9510 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [255/489], Loss: 0.2727, Time: 5.9548 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [260/489], Loss: 0.2693, Time: 5.9588 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [265/489], Loss: 0.2690, Time: 5.9627 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [270/489], Loss: 0.2704, Time: 5.9665 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [275/489], Loss: 0.2743, Time: 5.9704 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [280/489], Loss: 0.2721, Time: 5.9743 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [285/489], Loss: 0.2771, Time: 5.9782 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [290/489], Loss: 0.2681, Time: 5.9820 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [295/489], Loss: 0.2762, Time: 5.9860 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [300/489], Loss: 0.2740, Time: 5.9900 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [305/489], Loss: 0.2710, Time: 5.9938 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [310/489], Loss: 0.2761, Time: 5.9977 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [315/489], Loss: 0.2709, Time: 6.0015 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [320/489], Loss: 0.2673, Time: 6.0054 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [325/489], Loss: 0.2708, Time: 6.0093 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [330/489], Loss: 0.2736, Time: 6.0132 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [335/489], Loss: 0.2836, Time: 6.0170 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [340/489], Loss: 0.2724, Time: 6.0209 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [345/489], Loss: 0.2737, Time: 6.0248 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [350/489], Loss: 0.2751, Time: 6.0287 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [355/489], Loss: 0.2749, Time: 6.0325 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [360/489], Loss: 0.2738, Time: 6.0364 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [365/489], Loss: 0.2696, Time: 6.0403 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [370/489], Loss: 0.2731, Time: 6.0442 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [375/489], Loss: 0.2729, Time: 6.0480 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [380/489], Loss: 0.2733, Time: 6.0519 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [385/489], Loss: 0.2721, Time: 6.0558 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [390/489], Loss: 0.2804, Time: 6.0597 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [395/489], Loss: 0.2905, Time: 6.0636 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [400/489], Loss: 0.2783, Time: 6.0674 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [405/489], Loss: 0.2700, Time: 6.0713 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [410/489], Loss: 0.2730, Time: 6.0753 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [415/489], Loss: 0.2724, Time: 6.0791 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [420/489], Loss: 0.2721, Time: 6.0830 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [425/489], Loss: 0.2734, Time: 6.0868 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [430/489], Loss: 0.2716, Time: 6.0908 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [435/489], Loss: 0.2718, Time: 6.0946 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [440/489], Loss: 0.2752, Time: 6.0985 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [445/489], Loss: 0.2773, Time: 6.1023 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [450/489], Loss: 0.2767, Time: 6.1063 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [455/489], Loss: 0.2713, Time: 6.1102 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [460/489], Loss: 0.2815, Time: 6.1140 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [465/489], Loss: 0.2708, Time: 6.1178 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [470/489], Loss: 0.2783, Time: 6.1217 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [475/489], Loss: 0.2719, Time: 6.1256 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [480/489], Loss: 0.2707, Time: 6.1295 secs, learning rate: 0.0001\n",
      "Epoch [12/30], Step [485/489], Loss: 0.2767, Time: 6.1333 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [5/489], Loss: 0.2729, Time: 6.2632 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [10/489], Loss: 0.2729, Time: 6.2682 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [15/489], Loss: 0.2742, Time: 6.2734 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [20/489], Loss: 0.2728, Time: 6.2781 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [25/489], Loss: 0.2764, Time: 6.2822 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [30/489], Loss: 0.2730, Time: 6.2861 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [35/489], Loss: 0.2721, Time: 6.2900 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [40/489], Loss: 0.2751, Time: 6.2938 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [45/489], Loss: 0.2760, Time: 6.2977 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [50/489], Loss: 0.2795, Time: 6.3015 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [55/489], Loss: 0.2688, Time: 6.3054 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [60/489], Loss: 0.2683, Time: 6.3094 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [65/489], Loss: 0.2729, Time: 6.3132 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [70/489], Loss: 0.2787, Time: 6.3170 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [75/489], Loss: 0.2708, Time: 6.3209 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [80/489], Loss: 0.2686, Time: 6.3248 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [85/489], Loss: 0.2671, Time: 6.3287 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [90/489], Loss: 0.2737, Time: 6.3325 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [95/489], Loss: 0.2729, Time: 6.3363 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [100/489], Loss: 0.2770, Time: 6.3403 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [105/489], Loss: 0.2754, Time: 6.3442 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [110/489], Loss: 0.2916, Time: 6.3480 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [115/489], Loss: 0.2728, Time: 6.3519 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [120/489], Loss: 0.2766, Time: 6.3559 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [125/489], Loss: 0.2715, Time: 6.3597 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [130/489], Loss: 0.2744, Time: 6.3636 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [135/489], Loss: 0.2708, Time: 6.3674 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [140/489], Loss: 0.2725, Time: 6.3712 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [145/489], Loss: 0.2788, Time: 6.3752 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [150/489], Loss: 0.2699, Time: 6.3791 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [155/489], Loss: 0.2737, Time: 6.3829 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [160/489], Loss: 0.2722, Time: 6.3868 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [165/489], Loss: 0.2748, Time: 6.3907 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [170/489], Loss: 0.2687, Time: 6.3946 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [175/489], Loss: 0.2794, Time: 6.3985 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [180/489], Loss: 0.2738, Time: 6.4023 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [185/489], Loss: 0.2743, Time: 6.4063 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [190/489], Loss: 0.2734, Time: 6.4101 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [195/489], Loss: 0.2705, Time: 6.4140 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [200/489], Loss: 0.2709, Time: 6.4178 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [205/489], Loss: 0.2773, Time: 6.4216 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [210/489], Loss: 0.2706, Time: 6.4256 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [215/489], Loss: 0.2746, Time: 6.4294 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [220/489], Loss: 0.2740, Time: 6.4333 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [225/489], Loss: 0.2694, Time: 6.4371 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [230/489], Loss: 0.2741, Time: 6.4411 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [235/489], Loss: 0.2670, Time: 6.4450 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [240/489], Loss: 0.2719, Time: 6.4488 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [245/489], Loss: 0.2708, Time: 6.4526 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [250/489], Loss: 0.2657, Time: 6.4565 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [255/489], Loss: 0.2855, Time: 6.4603 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [260/489], Loss: 0.2822, Time: 6.4644 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [265/489], Loss: 0.2784, Time: 6.4683 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [270/489], Loss: 0.2696, Time: 6.4721 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [275/489], Loss: 0.2769, Time: 6.4761 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [280/489], Loss: 0.2700, Time: 6.4800 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [285/489], Loss: 0.2692, Time: 6.4839 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [290/489], Loss: 0.2694, Time: 6.4878 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [295/489], Loss: 0.2712, Time: 6.4918 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [300/489], Loss: 0.2674, Time: 6.4956 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [305/489], Loss: 0.2689, Time: 6.4995 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [310/489], Loss: 0.2681, Time: 6.5033 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [315/489], Loss: 0.2735, Time: 6.5074 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [320/489], Loss: 0.2917, Time: 6.5112 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [325/489], Loss: 0.2720, Time: 6.5151 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [330/489], Loss: 0.2731, Time: 6.5189 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [335/489], Loss: 0.2739, Time: 6.5228 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [340/489], Loss: 0.2646, Time: 6.5266 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [345/489], Loss: 0.2767, Time: 6.5305 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [350/489], Loss: 0.2709, Time: 6.5343 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [355/489], Loss: 0.2745, Time: 6.5382 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [360/489], Loss: 0.2736, Time: 6.5422 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [365/489], Loss: 0.2708, Time: 6.5460 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [370/489], Loss: 0.2757, Time: 6.5499 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [375/489], Loss: 0.2746, Time: 6.5537 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [380/489], Loss: 0.2744, Time: 6.5577 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [385/489], Loss: 0.2692, Time: 6.5616 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [390/489], Loss: 0.2724, Time: 6.5655 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [395/489], Loss: 0.2723, Time: 6.5693 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [400/489], Loss: 0.2810, Time: 6.5733 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [405/489], Loss: 0.2770, Time: 6.5771 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [410/489], Loss: 0.2837, Time: 6.5810 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [415/489], Loss: 0.2834, Time: 6.5849 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [420/489], Loss: 0.2706, Time: 6.5888 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [425/489], Loss: 0.2815, Time: 6.5927 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [430/489], Loss: 0.2737, Time: 6.5966 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [435/489], Loss: 0.2765, Time: 6.6005 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [440/489], Loss: 0.2683, Time: 6.6043 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [445/489], Loss: 0.2743, Time: 6.6082 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [450/489], Loss: 0.2714, Time: 6.6121 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [455/489], Loss: 0.2746, Time: 6.6159 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [460/489], Loss: 0.2735, Time: 6.6198 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [465/489], Loss: 0.2724, Time: 6.6238 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [470/489], Loss: 0.2730, Time: 6.6276 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [475/489], Loss: 0.2736, Time: 6.6315 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [480/489], Loss: 0.2822, Time: 6.6361 secs, learning rate: 0.0001\n",
      "Epoch [13/30], Step [485/489], Loss: 0.2782, Time: 6.6408 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [5/489], Loss: 0.2670, Time: 6.7697 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [10/489], Loss: 0.2705, Time: 6.7747 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [15/489], Loss: 0.2723, Time: 6.7795 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [20/489], Loss: 0.2742, Time: 6.7840 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [25/489], Loss: 0.2683, Time: 6.7884 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [30/489], Loss: 0.2714, Time: 6.7923 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [35/489], Loss: 0.2711, Time: 6.7961 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [40/489], Loss: 0.2749, Time: 6.8000 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [45/489], Loss: 0.2721, Time: 6.8038 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [50/489], Loss: 0.2758, Time: 6.8078 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [55/489], Loss: 0.2718, Time: 6.8117 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [60/489], Loss: 0.2753, Time: 6.8155 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [65/489], Loss: 0.2702, Time: 6.8194 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [70/489], Loss: 0.2709, Time: 6.8233 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [75/489], Loss: 0.2671, Time: 6.8272 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [80/489], Loss: 0.2740, Time: 6.8310 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [85/489], Loss: 0.2725, Time: 6.8348 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [90/489], Loss: 0.2791, Time: 6.8388 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [95/489], Loss: 0.2694, Time: 6.8427 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [100/489], Loss: 0.2669, Time: 6.8465 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [105/489], Loss: 0.2734, Time: 6.8504 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [110/489], Loss: 0.2701, Time: 6.8542 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [115/489], Loss: 0.2745, Time: 6.8582 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [120/489], Loss: 0.2684, Time: 6.8623 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [125/489], Loss: 0.2794, Time: 6.8667 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [130/489], Loss: 0.2722, Time: 6.8710 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [135/489], Loss: 0.2686, Time: 6.8760 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [140/489], Loss: 0.2712, Time: 6.8813 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [145/489], Loss: 0.2745, Time: 6.8853 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [150/489], Loss: 0.2730, Time: 6.8893 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [155/489], Loss: 0.2697, Time: 6.8931 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [160/489], Loss: 0.2694, Time: 6.8970 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [165/489], Loss: 0.2718, Time: 6.9008 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [170/489], Loss: 0.2751, Time: 6.9047 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [175/489], Loss: 0.2712, Time: 6.9086 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [180/489], Loss: 0.2676, Time: 6.9125 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [185/489], Loss: 0.2744, Time: 6.9164 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [190/489], Loss: 0.2720, Time: 6.9202 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [195/489], Loss: 0.2732, Time: 6.9242 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [200/489], Loss: 0.2729, Time: 6.9280 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [205/489], Loss: 0.2672, Time: 6.9318 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [210/489], Loss: 0.2696, Time: 6.9357 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [215/489], Loss: 0.2703, Time: 6.9396 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [220/489], Loss: 0.2611, Time: 6.9435 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [225/489], Loss: 0.2713, Time: 6.9473 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [230/489], Loss: 0.2670, Time: 6.9512 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [235/489], Loss: 0.2707, Time: 6.9550 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [240/489], Loss: 0.2713, Time: 6.9589 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [245/489], Loss: 0.2702, Time: 6.9628 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [250/489], Loss: 0.2752, Time: 6.9666 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [255/489], Loss: 0.2841, Time: 6.9706 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [260/489], Loss: 0.2733, Time: 6.9747 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [265/489], Loss: 0.2758, Time: 6.9785 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [270/489], Loss: 0.2721, Time: 6.9824 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [275/489], Loss: 0.2709, Time: 6.9863 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [280/489], Loss: 0.2757, Time: 6.9901 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [285/489], Loss: 0.2701, Time: 6.9939 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [290/489], Loss: 0.2708, Time: 6.9978 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [295/489], Loss: 0.2718, Time: 7.0016 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [300/489], Loss: 0.2691, Time: 7.0054 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [305/489], Loss: 0.2700, Time: 7.0093 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [310/489], Loss: 0.2714, Time: 7.0131 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [315/489], Loss: 0.2762, Time: 7.0169 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [320/489], Loss: 0.2730, Time: 7.0207 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [325/489], Loss: 0.2774, Time: 7.0246 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [330/489], Loss: 0.2716, Time: 7.0284 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [335/489], Loss: 0.2748, Time: 7.0323 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [340/489], Loss: 0.2706, Time: 7.0361 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [345/489], Loss: 0.2662, Time: 7.0401 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [350/489], Loss: 0.2735, Time: 7.0439 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [355/489], Loss: 0.2794, Time: 7.0478 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [360/489], Loss: 0.2734, Time: 7.0516 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [365/489], Loss: 0.2646, Time: 7.0555 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [370/489], Loss: 0.2689, Time: 7.0594 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [375/489], Loss: 0.2715, Time: 7.0633 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [380/489], Loss: 0.2717, Time: 7.0671 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [385/489], Loss: 0.2719, Time: 7.0709 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [390/489], Loss: 0.2688, Time: 7.0749 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [395/489], Loss: 0.2762, Time: 7.0787 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [400/489], Loss: 0.2720, Time: 7.0826 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [405/489], Loss: 0.2684, Time: 7.0864 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [410/489], Loss: 0.2731, Time: 7.0904 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [415/489], Loss: 0.2931, Time: 7.0943 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [420/489], Loss: 0.2826, Time: 7.0981 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [425/489], Loss: 0.2710, Time: 7.1020 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [430/489], Loss: 0.2728, Time: 7.1059 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [435/489], Loss: 0.2838, Time: 7.1097 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [440/489], Loss: 0.2734, Time: 7.1136 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [445/489], Loss: 0.2838, Time: 7.1174 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [450/489], Loss: 0.2713, Time: 7.1213 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [455/489], Loss: 0.3123, Time: 7.1253 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [460/489], Loss: 0.2765, Time: 7.1291 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [465/489], Loss: 0.2762, Time: 7.1330 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [470/489], Loss: 0.2902, Time: 7.1368 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [475/489], Loss: 0.2732, Time: 7.1407 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [480/489], Loss: 0.2771, Time: 7.1446 secs, learning rate: 0.0001\n",
      "Epoch [14/30], Step [485/489], Loss: 0.2999, Time: 7.1485 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [5/489], Loss: 0.2726, Time: 7.2795 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [10/489], Loss: 0.2853, Time: 7.2845 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [15/489], Loss: 0.2691, Time: 7.2892 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [20/489], Loss: 0.2701, Time: 7.2932 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [25/489], Loss: 0.2698, Time: 7.2971 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [30/489], Loss: 0.2731, Time: 7.3009 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [35/489], Loss: 0.2730, Time: 7.3048 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [40/489], Loss: 0.2798, Time: 7.3088 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [45/489], Loss: 0.2714, Time: 7.3126 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [50/489], Loss: 0.2711, Time: 7.3165 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [55/489], Loss: 0.2682, Time: 7.3203 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [60/489], Loss: 0.2799, Time: 7.3244 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [65/489], Loss: 0.2772, Time: 7.3282 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [70/489], Loss: 0.2798, Time: 7.3320 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [75/489], Loss: 0.2698, Time: 7.3359 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [80/489], Loss: 0.2707, Time: 7.3399 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [85/489], Loss: 0.2719, Time: 7.3437 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [90/489], Loss: 0.2713, Time: 7.3476 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [95/489], Loss: 0.2738, Time: 7.3514 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [100/489], Loss: 0.2778, Time: 7.3553 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [105/489], Loss: 0.2708, Time: 7.3592 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [110/489], Loss: 0.2718, Time: 7.3631 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [115/489], Loss: 0.2782, Time: 7.3669 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [120/489], Loss: 0.2665, Time: 7.3707 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [125/489], Loss: 0.2759, Time: 7.3748 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [130/489], Loss: 0.2707, Time: 7.3786 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [135/489], Loss: 0.2779, Time: 7.3824 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [140/489], Loss: 0.2765, Time: 7.3862 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [145/489], Loss: 0.2706, Time: 7.3903 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [150/489], Loss: 0.2686, Time: 7.3941 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [155/489], Loss: 0.2659, Time: 7.3979 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [160/489], Loss: 0.2750, Time: 7.4018 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [165/489], Loss: 0.2729, Time: 7.4058 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [170/489], Loss: 0.2711, Time: 7.4097 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [175/489], Loss: 0.2701, Time: 7.4135 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [180/489], Loss: 0.2753, Time: 7.4173 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [185/489], Loss: 0.2751, Time: 7.4212 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [190/489], Loss: 0.2992, Time: 7.4251 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [195/489], Loss: 0.2714, Time: 7.4290 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [200/489], Loss: 0.2684, Time: 7.4328 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [205/489], Loss: 0.2751, Time: 7.4367 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [210/489], Loss: 0.2738, Time: 7.4407 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [215/489], Loss: 0.2750, Time: 7.4445 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [220/489], Loss: 0.2674, Time: 7.4484 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [225/489], Loss: 0.2835, Time: 7.4522 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [230/489], Loss: 0.2734, Time: 7.4562 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [235/489], Loss: 0.2695, Time: 7.4600 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [240/489], Loss: 0.2701, Time: 7.4639 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [245/489], Loss: 0.2704, Time: 7.4677 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [250/489], Loss: 0.2697, Time: 7.4715 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [255/489], Loss: 0.2673, Time: 7.4755 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [260/489], Loss: 0.2709, Time: 7.4794 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [265/489], Loss: 0.2704, Time: 7.4834 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [270/489], Loss: 0.2703, Time: 7.4874 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [275/489], Loss: 0.2711, Time: 7.4914 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [280/489], Loss: 0.2740, Time: 7.4952 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [285/489], Loss: 0.2678, Time: 7.4991 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [290/489], Loss: 0.2730, Time: 7.5029 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [295/489], Loss: 0.2699, Time: 7.5067 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [300/489], Loss: 0.2695, Time: 7.5106 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [305/489], Loss: 0.2704, Time: 7.5144 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [310/489], Loss: 0.2716, Time: 7.5183 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [315/489], Loss: 0.2760, Time: 7.5221 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [320/489], Loss: 0.2943, Time: 7.5261 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [325/489], Loss: 0.2661, Time: 7.5299 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [330/489], Loss: 0.2764, Time: 7.5338 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [335/489], Loss: 0.2855, Time: 7.5376 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [340/489], Loss: 0.2793, Time: 7.5415 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [345/489], Loss: 0.2835, Time: 7.5453 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [350/489], Loss: 0.2714, Time: 7.5492 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [355/489], Loss: 0.2718, Time: 7.5530 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [360/489], Loss: 0.2745, Time: 7.5570 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [365/489], Loss: 0.2726, Time: 7.5608 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [370/489], Loss: 0.2802, Time: 7.5646 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [375/489], Loss: 0.2698, Time: 7.5685 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [380/489], Loss: 0.2883, Time: 7.5725 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [385/489], Loss: 0.2661, Time: 7.5763 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [390/489], Loss: 0.2796, Time: 7.5802 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [395/489], Loss: 0.2731, Time: 7.5840 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [400/489], Loss: 0.2745, Time: 7.5879 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [405/489], Loss: 0.2781, Time: 7.5919 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [410/489], Loss: 0.2682, Time: 7.5957 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [415/489], Loss: 0.2681, Time: 7.5996 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [420/489], Loss: 0.2692, Time: 7.6034 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [425/489], Loss: 0.2827, Time: 7.6074 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [430/489], Loss: 0.2642, Time: 7.6112 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [435/489], Loss: 0.2740, Time: 7.6150 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [440/489], Loss: 0.2752, Time: 7.6189 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [445/489], Loss: 0.2709, Time: 7.6227 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [450/489], Loss: 0.2694, Time: 7.6266 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [455/489], Loss: 0.2725, Time: 7.6305 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [460/489], Loss: 0.2631, Time: 7.6343 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [465/489], Loss: 0.2658, Time: 7.6382 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [470/489], Loss: 0.2803, Time: 7.6422 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [475/489], Loss: 0.2686, Time: 7.6461 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [480/489], Loss: 0.2757, Time: 7.6499 secs, learning rate: 0.0001\n",
      "Epoch [15/30], Step [485/489], Loss: 0.2695, Time: 7.6538 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [5/489], Loss: 0.2778, Time: 7.7836 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [10/489], Loss: 0.2688, Time: 7.7884 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [15/489], Loss: 0.2732, Time: 7.7934 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [20/489], Loss: 0.2738, Time: 7.7978 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [25/489], Loss: 0.2680, Time: 7.8017 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [30/489], Loss: 0.2708, Time: 7.8056 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [35/489], Loss: 0.2861, Time: 7.8095 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [40/489], Loss: 0.2679, Time: 7.8134 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [45/489], Loss: 0.2648, Time: 7.8172 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [50/489], Loss: 0.2760, Time: 7.8211 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [55/489], Loss: 0.2701, Time: 7.8249 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [60/489], Loss: 0.2949, Time: 7.8287 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [65/489], Loss: 0.2856, Time: 7.8326 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [70/489], Loss: 0.2676, Time: 7.8364 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [75/489], Loss: 0.2853, Time: 7.8404 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [80/489], Loss: 0.2661, Time: 7.8443 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [85/489], Loss: 0.2730, Time: 7.8481 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [90/489], Loss: 0.2741, Time: 7.8519 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [95/489], Loss: 0.2713, Time: 7.8559 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [100/489], Loss: 0.2689, Time: 7.8597 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [105/489], Loss: 0.2771, Time: 7.8635 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [110/489], Loss: 0.2661, Time: 7.8674 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [115/489], Loss: 0.2752, Time: 7.8712 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [120/489], Loss: 0.2652, Time: 7.8752 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [125/489], Loss: 0.2671, Time: 7.8791 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [130/489], Loss: 0.2708, Time: 7.8829 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [135/489], Loss: 0.2746, Time: 7.8868 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [140/489], Loss: 0.2711, Time: 7.8907 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [145/489], Loss: 0.2780, Time: 7.8945 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [150/489], Loss: 0.2721, Time: 7.8984 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [155/489], Loss: 0.2670, Time: 7.9022 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [160/489], Loss: 0.2708, Time: 7.9062 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [165/489], Loss: 0.2698, Time: 7.9101 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [170/489], Loss: 0.2669, Time: 7.9140 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [175/489], Loss: 0.2954, Time: 7.9178 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [180/489], Loss: 0.2752, Time: 7.9216 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [185/489], Loss: 0.2802, Time: 7.9256 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [190/489], Loss: 0.2670, Time: 7.9295 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [195/489], Loss: 0.2668, Time: 7.9333 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [200/489], Loss: 0.2717, Time: 7.9371 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [205/489], Loss: 0.2726, Time: 7.9411 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [210/489], Loss: 0.2693, Time: 7.9449 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [215/489], Loss: 0.2703, Time: 7.9488 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [220/489], Loss: 0.2698, Time: 7.9526 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [225/489], Loss: 0.2945, Time: 7.9566 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [230/489], Loss: 0.2648, Time: 7.9605 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [235/489], Loss: 0.2687, Time: 7.9643 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [240/489], Loss: 0.2679, Time: 7.9682 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [245/489], Loss: 0.2716, Time: 7.9720 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [250/489], Loss: 0.2747, Time: 7.9760 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [255/489], Loss: 0.2737, Time: 7.9798 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [260/489], Loss: 0.2747, Time: 7.9837 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [265/489], Loss: 0.2757, Time: 7.9876 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [270/489], Loss: 0.2771, Time: 7.9916 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [275/489], Loss: 0.2824, Time: 7.9954 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [280/489], Loss: 0.2757, Time: 7.9993 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [285/489], Loss: 0.2687, Time: 8.0031 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [290/489], Loss: 0.2763, Time: 8.0071 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [295/489], Loss: 0.2676, Time: 8.0110 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [300/489], Loss: 0.2938, Time: 8.0148 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [305/489], Loss: 0.2902, Time: 8.0187 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [310/489], Loss: 0.2711, Time: 8.0226 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [315/489], Loss: 0.2704, Time: 8.0265 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [320/489], Loss: 0.2666, Time: 8.0303 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [325/489], Loss: 0.2691, Time: 8.0341 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [330/489], Loss: 0.2717, Time: 8.0380 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [335/489], Loss: 0.2660, Time: 8.0420 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [340/489], Loss: 0.2785, Time: 8.0458 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [345/489], Loss: 0.2677, Time: 8.0497 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [350/489], Loss: 0.2714, Time: 8.0535 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [355/489], Loss: 0.2673, Time: 8.0574 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [360/489], Loss: 0.2661, Time: 8.0612 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [365/489], Loss: 0.2661, Time: 8.0651 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [370/489], Loss: 0.2666, Time: 8.0689 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [375/489], Loss: 0.2726, Time: 8.0728 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [380/489], Loss: 0.2673, Time: 8.0766 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [385/489], Loss: 0.2776, Time: 8.0804 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [390/489], Loss: 0.2726, Time: 8.0843 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [395/489], Loss: 0.2860, Time: 8.0881 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [400/489], Loss: 0.2835, Time: 8.0921 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [405/489], Loss: 0.2701, Time: 8.0960 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [410/489], Loss: 0.2724, Time: 8.0998 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [415/489], Loss: 0.2796, Time: 8.1036 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [420/489], Loss: 0.2691, Time: 8.1075 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [425/489], Loss: 0.2709, Time: 8.1113 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [430/489], Loss: 0.2667, Time: 8.1152 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [435/489], Loss: 0.2699, Time: 8.1190 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [440/489], Loss: 0.2876, Time: 8.1230 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [445/489], Loss: 0.2877, Time: 8.1268 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [450/489], Loss: 0.2817, Time: 8.1306 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [455/489], Loss: 0.3035, Time: 8.1345 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [460/489], Loss: 0.2704, Time: 8.1383 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [465/489], Loss: 0.2778, Time: 8.1422 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [470/489], Loss: 0.2721, Time: 8.1461 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [475/489], Loss: 0.2739, Time: 8.1499 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [480/489], Loss: 0.2627, Time: 8.1537 secs, learning rate: 0.0001\n",
      "Epoch [16/30], Step [485/489], Loss: 0.2786, Time: 8.1577 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [5/489], Loss: 0.2760, Time: 8.2875 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [10/489], Loss: 0.2680, Time: 8.2929 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [15/489], Loss: 0.2669, Time: 8.2979 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [20/489], Loss: 0.2659, Time: 8.3023 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [25/489], Loss: 0.2686, Time: 8.3063 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [30/489], Loss: 0.2653, Time: 8.3102 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [35/489], Loss: 0.2682, Time: 8.3140 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [40/489], Loss: 0.2830, Time: 8.3179 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [45/489], Loss: 0.2701, Time: 8.3217 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [50/489], Loss: 0.2862, Time: 8.3256 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [55/489], Loss: 0.2734, Time: 8.3294 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [60/489], Loss: 0.2937, Time: 8.3333 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [65/489], Loss: 0.2728, Time: 8.3371 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [70/489], Loss: 0.3125, Time: 8.3411 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [75/489], Loss: 0.2758, Time: 8.3450 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [80/489], Loss: 0.2741, Time: 8.3488 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [85/489], Loss: 0.2784, Time: 8.3527 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [90/489], Loss: 0.2735, Time: 8.3566 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [95/489], Loss: 0.2818, Time: 8.3608 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [100/489], Loss: 0.2676, Time: 8.3653 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [105/489], Loss: 0.2706, Time: 8.3696 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [110/489], Loss: 0.2729, Time: 8.3743 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [115/489], Loss: 0.2734, Time: 8.3786 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [120/489], Loss: 0.2748, Time: 8.3825 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [125/489], Loss: 0.2764, Time: 8.3863 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [130/489], Loss: 0.3305, Time: 8.3903 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [135/489], Loss: 0.2709, Time: 8.3941 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [140/489], Loss: 0.3176, Time: 8.3980 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [145/489], Loss: 0.2769, Time: 8.4018 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [150/489], Loss: 0.2659, Time: 8.4059 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [155/489], Loss: 0.2775, Time: 8.4097 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [160/489], Loss: 0.2673, Time: 8.4136 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [165/489], Loss: 0.2637, Time: 8.4174 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [170/489], Loss: 0.2724, Time: 8.4213 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [175/489], Loss: 0.2689, Time: 8.4252 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [180/489], Loss: 0.2769, Time: 8.4291 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [185/489], Loss: 0.2716, Time: 8.4331 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [190/489], Loss: 0.2662, Time: 8.4374 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [195/489], Loss: 0.2699, Time: 8.4418 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [200/489], Loss: 0.2724, Time: 8.4470 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [205/489], Loss: 0.2685, Time: 8.4522 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [210/489], Loss: 0.2669, Time: 8.4566 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [215/489], Loss: 0.2659, Time: 8.4605 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [220/489], Loss: 0.2631, Time: 8.4643 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [225/489], Loss: 0.2696, Time: 8.4682 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [230/489], Loss: 0.2643, Time: 8.4721 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [235/489], Loss: 0.2729, Time: 8.4760 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [240/489], Loss: 0.2735, Time: 8.4799 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [245/489], Loss: 0.2682, Time: 8.4837 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [250/489], Loss: 0.2678, Time: 8.4875 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [255/489], Loss: 0.2832, Time: 8.4919 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [260/489], Loss: 0.2705, Time: 8.4958 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [265/489], Loss: 0.2841, Time: 8.4999 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [270/489], Loss: 0.2742, Time: 8.5052 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [275/489], Loss: 0.2728, Time: 8.5099 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [280/489], Loss: 0.2656, Time: 8.5138 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [285/489], Loss: 0.2726, Time: 8.5176 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [290/489], Loss: 0.2702, Time: 8.5215 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [295/489], Loss: 0.2822, Time: 8.5254 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [300/489], Loss: 0.2731, Time: 8.5293 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [305/489], Loss: 0.2889, Time: 8.5332 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [310/489], Loss: 0.2692, Time: 8.5370 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [315/489], Loss: 0.2701, Time: 8.5408 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [320/489], Loss: 0.2702, Time: 8.5447 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [325/489], Loss: 0.2847, Time: 8.5485 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [330/489], Loss: 0.2623, Time: 8.5524 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [335/489], Loss: 0.2697, Time: 8.5563 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [340/489], Loss: 0.2648, Time: 8.5602 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [345/489], Loss: 0.2682, Time: 8.5641 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [350/489], Loss: 0.2638, Time: 8.5679 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [355/489], Loss: 0.2729, Time: 8.5718 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [360/489], Loss: 0.2844, Time: 8.5757 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [365/489], Loss: 0.2684, Time: 8.5796 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [370/489], Loss: 0.2819, Time: 8.5834 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [375/489], Loss: 0.2756, Time: 8.5873 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [380/489], Loss: 0.2718, Time: 8.5913 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [385/489], Loss: 0.2666, Time: 8.5952 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [390/489], Loss: 0.2714, Time: 8.5990 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [395/489], Loss: 0.2710, Time: 8.6029 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [400/489], Loss: 0.2704, Time: 8.6067 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [405/489], Loss: 0.2815, Time: 8.6106 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [410/489], Loss: 0.2819, Time: 8.6145 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [415/489], Loss: 0.2755, Time: 8.6184 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [420/489], Loss: 0.2713, Time: 8.6222 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [425/489], Loss: 0.2776, Time: 8.6263 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [430/489], Loss: 0.2668, Time: 8.6309 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [435/489], Loss: 0.2817, Time: 8.6354 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [440/489], Loss: 0.2631, Time: 8.6399 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [445/489], Loss: 0.2683, Time: 8.6438 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [450/489], Loss: 0.2648, Time: 8.6476 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [455/489], Loss: 0.2700, Time: 8.6515 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [460/489], Loss: 0.2653, Time: 8.6553 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [465/489], Loss: 0.2677, Time: 8.6592 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [470/489], Loss: 0.2718, Time: 8.6630 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [475/489], Loss: 0.2653, Time: 8.6669 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [480/489], Loss: 0.2719, Time: 8.6707 secs, learning rate: 0.0001\n",
      "Epoch [17/30], Step [485/489], Loss: 0.2699, Time: 8.6747 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [5/489], Loss: 0.2918, Time: 8.8057 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [10/489], Loss: 0.2660, Time: 8.8106 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [15/489], Loss: 0.2681, Time: 8.8154 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [20/489], Loss: 0.2847, Time: 8.8197 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [25/489], Loss: 0.2813, Time: 8.8237 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [30/489], Loss: 0.3009, Time: 8.8276 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [35/489], Loss: 0.2673, Time: 8.8314 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [40/489], Loss: 0.2772, Time: 8.8353 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [45/489], Loss: 0.2774, Time: 8.8392 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [50/489], Loss: 0.2680, Time: 8.8431 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [55/489], Loss: 0.2733, Time: 8.8469 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [60/489], Loss: 0.2678, Time: 8.8508 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [65/489], Loss: 0.2642, Time: 8.8546 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [70/489], Loss: 0.2643, Time: 8.8586 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [75/489], Loss: 0.2767, Time: 8.8625 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [80/489], Loss: 0.2629, Time: 8.8663 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [85/489], Loss: 0.2689, Time: 8.8702 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [90/489], Loss: 0.2685, Time: 8.8741 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [95/489], Loss: 0.2657, Time: 8.8779 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [100/489], Loss: 0.2679, Time: 8.8818 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [105/489], Loss: 0.2813, Time: 8.8857 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [110/489], Loss: 0.2641, Time: 8.8897 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [115/489], Loss: 0.2668, Time: 8.8935 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [120/489], Loss: 0.2658, Time: 8.8974 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [125/489], Loss: 0.2673, Time: 8.9012 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [130/489], Loss: 0.2666, Time: 8.9051 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [135/489], Loss: 0.2702, Time: 8.9090 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [140/489], Loss: 0.2919, Time: 8.9129 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [145/489], Loss: 0.2972, Time: 8.9167 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [150/489], Loss: 0.2653, Time: 8.9205 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [155/489], Loss: 0.2674, Time: 8.9245 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [160/489], Loss: 0.2680, Time: 8.9284 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [165/489], Loss: 0.2649, Time: 8.9322 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [170/489], Loss: 0.2703, Time: 8.9360 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [175/489], Loss: 0.2647, Time: 8.9400 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [180/489], Loss: 0.2718, Time: 8.9438 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [185/489], Loss: 0.2665, Time: 8.9477 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [190/489], Loss: 0.2690, Time: 8.9515 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [195/489], Loss: 0.2718, Time: 8.9553 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [200/489], Loss: 0.2683, Time: 8.9593 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [205/489], Loss: 0.2667, Time: 8.9631 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [210/489], Loss: 0.2660, Time: 8.9670 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [215/489], Loss: 0.2622, Time: 8.9708 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [220/489], Loss: 0.2701, Time: 8.9748 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [225/489], Loss: 0.2771, Time: 8.9787 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [230/489], Loss: 0.2650, Time: 8.9825 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [235/489], Loss: 0.2692, Time: 8.9864 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [240/489], Loss: 0.2728, Time: 8.9903 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [245/489], Loss: 0.2873, Time: 8.9942 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [250/489], Loss: 0.2833, Time: 8.9980 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [255/489], Loss: 0.2661, Time: 9.0018 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [260/489], Loss: 0.2840, Time: 9.0058 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [265/489], Loss: 0.2722, Time: 9.0099 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [270/489], Loss: 0.2665, Time: 9.0139 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [275/489], Loss: 0.2639, Time: 9.0177 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [280/489], Loss: 0.2701, Time: 9.0216 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [285/489], Loss: 0.2699, Time: 9.0256 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [290/489], Loss: 0.2709, Time: 9.0294 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [295/489], Loss: 0.2811, Time: 9.0333 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [300/489], Loss: 0.2862, Time: 9.0371 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [305/489], Loss: 0.2655, Time: 9.0411 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [310/489], Loss: 0.2707, Time: 9.0449 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [315/489], Loss: 0.2673, Time: 9.0487 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [320/489], Loss: 0.2637, Time: 9.0526 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [325/489], Loss: 0.2795, Time: 9.0566 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [330/489], Loss: 0.2677, Time: 9.0605 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [335/489], Loss: 0.2654, Time: 9.0643 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [340/489], Loss: 0.2757, Time: 9.0681 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [345/489], Loss: 0.2732, Time: 9.0720 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [350/489], Loss: 0.2698, Time: 9.0759 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [355/489], Loss: 0.2692, Time: 9.0798 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [360/489], Loss: 0.2782, Time: 9.0836 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [365/489], Loss: 0.2635, Time: 9.0875 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [370/489], Loss: 0.2927, Time: 9.0915 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [375/489], Loss: 0.3454, Time: 9.0953 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [380/489], Loss: 0.3028, Time: 9.0992 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [385/489], Loss: 0.2690, Time: 9.1030 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [390/489], Loss: 0.2880, Time: 9.1068 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [395/489], Loss: 0.3292, Time: 9.1107 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [400/489], Loss: 0.2814, Time: 9.1145 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [405/489], Loss: 0.2796, Time: 9.1183 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [410/489], Loss: 0.2698, Time: 9.1222 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [415/489], Loss: 0.2746, Time: 9.1260 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [420/489], Loss: 0.3015, Time: 9.1298 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [425/489], Loss: 0.2895, Time: 9.1337 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [430/489], Loss: 0.2688, Time: 9.1375 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [435/489], Loss: 0.2651, Time: 9.1413 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [440/489], Loss: 0.2834, Time: 9.1452 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [445/489], Loss: 0.2661, Time: 9.1490 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [450/489], Loss: 0.2654, Time: 9.1529 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [455/489], Loss: 0.2713, Time: 9.1567 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [460/489], Loss: 0.2804, Time: 9.1606 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [465/489], Loss: 0.2676, Time: 9.1644 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [470/489], Loss: 0.2662, Time: 9.1682 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [475/489], Loss: 0.2661, Time: 9.1721 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [480/489], Loss: 0.3003, Time: 9.1761 secs, learning rate: 0.0001\n",
      "Epoch [18/30], Step [485/489], Loss: 0.2885, Time: 9.1799 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [5/489], Loss: 0.2860, Time: 9.3099 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [10/489], Loss: 0.2705, Time: 9.3147 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [15/489], Loss: 0.2640, Time: 9.3197 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [20/489], Loss: 0.2714, Time: 9.3242 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [25/489], Loss: 0.2651, Time: 9.3286 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [30/489], Loss: 0.2704, Time: 9.3325 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [35/489], Loss: 0.2674, Time: 9.3364 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [40/489], Loss: 0.2865, Time: 9.3404 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [45/489], Loss: 0.2756, Time: 9.3442 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [50/489], Loss: 0.2663, Time: 9.3480 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [55/489], Loss: 0.2567, Time: 9.3519 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [60/489], Loss: 0.2631, Time: 9.3559 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [65/489], Loss: 0.2662, Time: 9.3597 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [70/489], Loss: 0.2636, Time: 9.3635 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [75/489], Loss: 0.2563, Time: 9.3674 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [80/489], Loss: 0.2659, Time: 9.3713 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [85/489], Loss: 0.2635, Time: 9.3752 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [90/489], Loss: 0.2707, Time: 9.3791 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [95/489], Loss: 0.2748, Time: 9.3829 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [100/489], Loss: 0.2670, Time: 9.3868 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [105/489], Loss: 0.2744, Time: 9.3907 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [110/489], Loss: 0.2631, Time: 9.3946 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [115/489], Loss: 0.2692, Time: 9.3984 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [120/489], Loss: 0.2673, Time: 9.4023 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [125/489], Loss: 0.2720, Time: 9.4068 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [130/489], Loss: 0.2718, Time: 9.4112 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [135/489], Loss: 0.2729, Time: 9.4156 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [140/489], Loss: 0.2662, Time: 9.4201 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [145/489], Loss: 0.2660, Time: 9.4241 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [150/489], Loss: 0.2774, Time: 9.4280 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [155/489], Loss: 0.2856, Time: 9.4319 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [160/489], Loss: 0.2799, Time: 9.4357 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [165/489], Loss: 0.2691, Time: 9.4398 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [170/489], Loss: 0.2610, Time: 9.4436 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [175/489], Loss: 0.2766, Time: 9.4474 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [180/489], Loss: 0.2663, Time: 9.4513 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [185/489], Loss: 0.2880, Time: 9.4551 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [190/489], Loss: 0.2838, Time: 9.4590 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [195/489], Loss: 0.2682, Time: 9.4628 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [200/489], Loss: 0.3212, Time: 9.4667 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [205/489], Loss: 0.2921, Time: 9.4705 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [210/489], Loss: 0.2640, Time: 9.4745 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [215/489], Loss: 0.2671, Time: 9.4784 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [220/489], Loss: 0.2686, Time: 9.4822 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [225/489], Loss: 0.2679, Time: 9.4861 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [230/489], Loss: 0.2641, Time: 9.4900 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [235/489], Loss: 0.2679, Time: 9.4938 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [240/489], Loss: 0.2697, Time: 9.4977 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [245/489], Loss: 0.2663, Time: 9.5015 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [250/489], Loss: 0.2795, Time: 9.5054 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [255/489], Loss: 0.2780, Time: 9.5093 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [260/489], Loss: 0.2791, Time: 9.5134 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [265/489], Loss: 0.2696, Time: 9.5173 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [270/489], Loss: 0.2800, Time: 9.5212 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [275/489], Loss: 0.2683, Time: 9.5253 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [280/489], Loss: 0.2668, Time: 9.5292 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [285/489], Loss: 0.2683, Time: 9.5333 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [290/489], Loss: 0.2683, Time: 9.5373 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [295/489], Loss: 0.2697, Time: 9.5413 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [300/489], Loss: 0.2669, Time: 9.5451 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [305/489], Loss: 0.2661, Time: 9.5490 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [310/489], Loss: 0.2630, Time: 9.5529 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [315/489], Loss: 0.2654, Time: 9.5568 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [320/489], Loss: 0.2740, Time: 9.5606 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [325/489], Loss: 0.2696, Time: 9.5645 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [330/489], Loss: 0.2605, Time: 9.5683 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [335/489], Loss: 0.2632, Time: 9.5722 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [340/489], Loss: 0.2756, Time: 9.5762 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [345/489], Loss: 0.2811, Time: 9.5806 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [350/489], Loss: 0.2648, Time: 9.5852 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [355/489], Loss: 0.2641, Time: 9.5892 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [360/489], Loss: 0.2627, Time: 9.5931 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [365/489], Loss: 0.2884, Time: 9.5970 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [370/489], Loss: 0.2641, Time: 9.6008 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [375/489], Loss: 0.2665, Time: 9.6047 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [380/489], Loss: 0.2670, Time: 9.6085 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [385/489], Loss: 0.2679, Time: 9.6124 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [390/489], Loss: 0.2777, Time: 9.6162 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [395/489], Loss: 0.2669, Time: 9.6200 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [400/489], Loss: 0.2634, Time: 9.6240 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [405/489], Loss: 0.2664, Time: 9.6279 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [410/489], Loss: 0.2674, Time: 9.6317 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [415/489], Loss: 0.2624, Time: 9.6356 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [420/489], Loss: 0.2653, Time: 9.6396 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [425/489], Loss: 0.2643, Time: 9.6434 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [430/489], Loss: 0.2707, Time: 9.6473 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [435/489], Loss: 0.2646, Time: 9.6511 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [440/489], Loss: 0.2658, Time: 9.6549 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [445/489], Loss: 0.2612, Time: 9.6589 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [450/489], Loss: 0.2679, Time: 9.6628 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [455/489], Loss: 0.2655, Time: 9.6666 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [460/489], Loss: 0.2710, Time: 9.6705 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [465/489], Loss: 0.2613, Time: 9.6744 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [470/489], Loss: 0.2620, Time: 9.6783 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [475/489], Loss: 0.2624, Time: 9.6821 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [480/489], Loss: 0.2716, Time: 9.6859 secs, learning rate: 0.0001\n",
      "Epoch [19/30], Step [485/489], Loss: 0.2669, Time: 9.6900 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [5/489], Loss: 0.2736, Time: 9.8215 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [10/489], Loss: 0.2743, Time: 9.8263 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [15/489], Loss: 0.2627, Time: 9.8306 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [20/489], Loss: 0.2687, Time: 9.8348 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [25/489], Loss: 0.2789, Time: 9.8386 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [30/489], Loss: 0.2669, Time: 9.8426 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [35/489], Loss: 0.2896, Time: 9.8465 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [40/489], Loss: 0.2649, Time: 9.8503 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [45/489], Loss: 0.2746, Time: 9.8542 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [50/489], Loss: 0.2680, Time: 9.8582 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [55/489], Loss: 0.2673, Time: 9.8620 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [60/489], Loss: 0.2668, Time: 9.8658 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [65/489], Loss: 0.2654, Time: 9.8697 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [70/489], Loss: 0.2648, Time: 9.8738 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [75/489], Loss: 0.2656, Time: 9.8788 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [80/489], Loss: 0.2739, Time: 9.8834 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [85/489], Loss: 0.2859, Time: 9.8875 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [90/489], Loss: 0.2820, Time: 9.8916 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [95/489], Loss: 0.2621, Time: 9.8956 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [100/489], Loss: 0.2764, Time: 9.8995 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [105/489], Loss: 0.2683, Time: 9.9034 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [110/489], Loss: 0.2629, Time: 9.9074 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [115/489], Loss: 0.2721, Time: 9.9112 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [120/489], Loss: 0.2637, Time: 9.9151 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [125/489], Loss: 0.2661, Time: 9.9189 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [130/489], Loss: 0.2615, Time: 9.9229 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [135/489], Loss: 0.2724, Time: 9.9267 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [140/489], Loss: 0.2646, Time: 9.9306 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [145/489], Loss: 0.2643, Time: 9.9344 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [150/489], Loss: 0.2613, Time: 9.9383 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [155/489], Loss: 0.2735, Time: 9.9423 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [160/489], Loss: 0.2829, Time: 9.9461 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [165/489], Loss: 0.2731, Time: 9.9500 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [170/489], Loss: 0.2637, Time: 9.9538 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [175/489], Loss: 0.2654, Time: 9.9578 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [180/489], Loss: 0.3097, Time: 9.9617 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [185/489], Loss: 0.2953, Time: 9.9655 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [190/489], Loss: 0.2658, Time: 9.9701 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [195/489], Loss: 0.2692, Time: 9.9747 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [200/489], Loss: 0.2650, Time: 9.9792 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [205/489], Loss: 0.2644, Time: 9.9838 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [210/489], Loss: 0.2652, Time: 9.9876 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [215/489], Loss: 0.2648, Time: 9.9916 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [220/489], Loss: 0.2648, Time: 9.9954 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [225/489], Loss: 0.2707, Time: 9.9992 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [230/489], Loss: 0.3095, Time: 10.0031 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [235/489], Loss: 0.2642, Time: 10.0071 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [240/489], Loss: 0.2654, Time: 10.0109 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [245/489], Loss: 0.2649, Time: 10.0148 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [250/489], Loss: 0.2645, Time: 10.0186 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [255/489], Loss: 0.2670, Time: 10.0227 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [260/489], Loss: 0.2636, Time: 10.0267 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [265/489], Loss: 0.2711, Time: 10.0305 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [270/489], Loss: 0.2707, Time: 10.0344 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [275/489], Loss: 0.2723, Time: 10.0383 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [280/489], Loss: 0.2605, Time: 10.0423 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [285/489], Loss: 0.2516, Time: 10.0462 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [290/489], Loss: 0.2625, Time: 10.0501 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [295/489], Loss: 0.2987, Time: 10.0539 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [300/489], Loss: 0.2680, Time: 10.0579 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [305/489], Loss: 0.2833, Time: 10.0617 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [310/489], Loss: 0.2743, Time: 10.0656 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [315/489], Loss: 0.2649, Time: 10.0694 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [320/489], Loss: 0.2645, Time: 10.0734 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [325/489], Loss: 0.2639, Time: 10.0772 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [330/489], Loss: 0.2607, Time: 10.0811 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [335/489], Loss: 0.2630, Time: 10.0850 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [340/489], Loss: 0.2615, Time: 10.0888 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [345/489], Loss: 0.2713, Time: 10.0927 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [350/489], Loss: 0.2648, Time: 10.0966 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [355/489], Loss: 0.2657, Time: 10.1005 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [360/489], Loss: 0.2645, Time: 10.1043 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [365/489], Loss: 0.2683, Time: 10.1082 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [370/489], Loss: 0.2641, Time: 10.1121 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [375/489], Loss: 0.2697, Time: 10.1159 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [380/489], Loss: 0.2638, Time: 10.1197 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [385/489], Loss: 0.2641, Time: 10.1237 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [390/489], Loss: 0.2636, Time: 10.1275 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [395/489], Loss: 0.2693, Time: 10.1314 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [400/489], Loss: 0.2766, Time: 10.1353 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [405/489], Loss: 0.2712, Time: 10.1392 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [410/489], Loss: 0.2661, Time: 10.1448 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [415/489], Loss: 0.2639, Time: 10.1491 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [420/489], Loss: 0.2740, Time: 10.1531 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [425/489], Loss: 0.2964, Time: 10.1570 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [430/489], Loss: 0.2687, Time: 10.1608 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [435/489], Loss: 0.2676, Time: 10.1646 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [440/489], Loss: 0.2712, Time: 10.1684 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [445/489], Loss: 0.2600, Time: 10.1723 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [450/489], Loss: 0.2626, Time: 10.1761 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [455/489], Loss: 0.2759, Time: 10.1799 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [460/489], Loss: 0.2585, Time: 10.1837 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [465/489], Loss: 0.2619, Time: 10.1876 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [470/489], Loss: 0.2759, Time: 10.1915 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [475/489], Loss: 0.2955, Time: 10.1954 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [480/489], Loss: 0.2910, Time: 10.1993 secs, learning rate: 0.0001\n",
      "Epoch [20/30], Step [485/489], Loss: 0.2699, Time: 10.2031 secs, learning rate: 0.0001\n",
      "Epoch [21/30], Step [5/489], Loss: 0.2618, Time: 10.3352 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [10/489], Loss: 0.2667, Time: 10.3402 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [15/489], Loss: 0.2652, Time: 10.3454 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [20/489], Loss: 0.2677, Time: 10.3499 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [25/489], Loss: 0.2618, Time: 10.3549 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [30/489], Loss: 0.2599, Time: 10.3590 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [35/489], Loss: 0.2612, Time: 10.3628 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [40/489], Loss: 0.2624, Time: 10.3667 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [45/489], Loss: 0.2627, Time: 10.3705 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [50/489], Loss: 0.2625, Time: 10.3745 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [55/489], Loss: 0.2619, Time: 10.3784 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [60/489], Loss: 0.2616, Time: 10.3822 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [65/489], Loss: 0.2613, Time: 10.3861 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [70/489], Loss: 0.2587, Time: 10.3900 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [75/489], Loss: 0.2624, Time: 10.3938 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [80/489], Loss: 0.2617, Time: 10.3977 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [85/489], Loss: 0.2614, Time: 10.4015 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [90/489], Loss: 0.2543, Time: 10.4054 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [95/489], Loss: 0.2647, Time: 10.4094 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [100/489], Loss: 0.2623, Time: 10.4132 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [105/489], Loss: 0.2634, Time: 10.4170 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [110/489], Loss: 0.2627, Time: 10.4223 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [115/489], Loss: 0.2632, Time: 10.4317 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [120/489], Loss: 0.2608, Time: 10.4356 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [125/489], Loss: 0.2628, Time: 10.4397 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [130/489], Loss: 0.2639, Time: 10.4436 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [135/489], Loss: 0.2620, Time: 10.4474 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [140/489], Loss: 0.2619, Time: 10.4512 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [145/489], Loss: 0.2628, Time: 10.4551 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [150/489], Loss: 0.2633, Time: 10.4590 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [155/489], Loss: 0.2608, Time: 10.4628 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [160/489], Loss: 0.2608, Time: 10.4667 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [165/489], Loss: 0.2642, Time: 10.4706 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [170/489], Loss: 0.2628, Time: 10.4746 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [175/489], Loss: 0.2660, Time: 10.4785 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [180/489], Loss: 0.2618, Time: 10.4823 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [185/489], Loss: 0.2597, Time: 10.4861 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [190/489], Loss: 0.2592, Time: 10.4921 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [195/489], Loss: 0.2635, Time: 10.5026 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [200/489], Loss: 0.2625, Time: 10.5111 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [205/489], Loss: 0.2625, Time: 10.5177 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [210/489], Loss: 0.2614, Time: 10.5216 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [215/489], Loss: 0.2610, Time: 10.5259 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [220/489], Loss: 0.2616, Time: 10.5298 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [225/489], Loss: 0.2629, Time: 10.5336 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [230/489], Loss: 0.2645, Time: 10.5376 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [235/489], Loss: 0.2597, Time: 10.5416 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [240/489], Loss: 0.2626, Time: 10.5455 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [245/489], Loss: 0.2624, Time: 10.5493 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [250/489], Loss: 0.2611, Time: 10.5532 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [255/489], Loss: 0.2633, Time: 10.5572 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [260/489], Loss: 0.2600, Time: 10.5611 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [265/489], Loss: 0.2594, Time: 10.5649 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [270/489], Loss: 0.2566, Time: 10.5688 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [275/489], Loss: 0.2599, Time: 10.5727 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [280/489], Loss: 0.2642, Time: 10.5765 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [285/489], Loss: 0.2646, Time: 10.5804 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [290/489], Loss: 0.2594, Time: 10.5843 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [295/489], Loss: 0.2621, Time: 10.5882 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [300/489], Loss: 0.2630, Time: 10.5923 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [305/489], Loss: 0.2559, Time: 10.5961 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [310/489], Loss: 0.2648, Time: 10.6000 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [315/489], Loss: 0.2628, Time: 10.6038 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [320/489], Loss: 0.2590, Time: 10.6078 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [325/489], Loss: 0.2614, Time: 10.6117 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [330/489], Loss: 0.2602, Time: 10.6155 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [335/489], Loss: 0.2596, Time: 10.6194 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [340/489], Loss: 0.2560, Time: 10.6233 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [345/489], Loss: 0.2623, Time: 10.6272 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [350/489], Loss: 0.2605, Time: 10.6310 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [355/489], Loss: 0.2604, Time: 10.6348 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [360/489], Loss: 0.2645, Time: 10.6387 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [365/489], Loss: 0.2594, Time: 10.6427 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [370/489], Loss: 0.2556, Time: 10.6466 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [375/489], Loss: 0.2635, Time: 10.6504 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [380/489], Loss: 0.2626, Time: 10.6542 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [385/489], Loss: 0.2638, Time: 10.6582 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [390/489], Loss: 0.2619, Time: 10.6620 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [395/489], Loss: 0.2640, Time: 10.6659 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [400/489], Loss: 0.2632, Time: 10.6697 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [405/489], Loss: 0.2614, Time: 10.6737 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [410/489], Loss: 0.2614, Time: 10.6775 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [415/489], Loss: 0.2604, Time: 10.6814 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [420/489], Loss: 0.2597, Time: 10.6852 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [425/489], Loss: 0.2593, Time: 10.6892 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [430/489], Loss: 0.2650, Time: 10.6930 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [435/489], Loss: 0.2589, Time: 10.6969 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [440/489], Loss: 0.2599, Time: 10.7007 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [445/489], Loss: 0.2686, Time: 10.7045 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [450/489], Loss: 0.2614, Time: 10.7084 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [455/489], Loss: 0.2624, Time: 10.7122 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [460/489], Loss: 0.2610, Time: 10.7161 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [465/489], Loss: 0.2603, Time: 10.7199 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [470/489], Loss: 0.2621, Time: 10.7240 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [475/489], Loss: 0.2628, Time: 10.7278 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [480/489], Loss: 0.2623, Time: 10.7317 secs, learning rate: 0.0000\n",
      "Epoch [21/30], Step [485/489], Loss: 0.2595, Time: 10.7355 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [5/489], Loss: 0.2626, Time: 10.8653 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [10/489], Loss: 0.2627, Time: 10.8705 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [15/489], Loss: 0.2656, Time: 10.8754 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [20/489], Loss: 0.2619, Time: 10.8800 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [25/489], Loss: 0.2618, Time: 10.8842 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [30/489], Loss: 0.2699, Time: 10.8881 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [35/489], Loss: 0.2615, Time: 10.8921 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [40/489], Loss: 0.2624, Time: 10.8959 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [45/489], Loss: 0.2599, Time: 10.8998 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [50/489], Loss: 0.2633, Time: 10.9036 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [55/489], Loss: 0.2644, Time: 10.9075 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [60/489], Loss: 0.2637, Time: 10.9113 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [65/489], Loss: 0.2615, Time: 10.9152 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [70/489], Loss: 0.2622, Time: 10.9190 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [75/489], Loss: 0.2656, Time: 10.9229 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [80/489], Loss: 0.2610, Time: 10.9267 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [85/489], Loss: 0.2601, Time: 10.9306 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [90/489], Loss: 0.2607, Time: 10.9344 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [95/489], Loss: 0.2623, Time: 10.9382 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [100/489], Loss: 0.2627, Time: 10.9423 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [105/489], Loss: 0.2600, Time: 10.9461 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [110/489], Loss: 0.2635, Time: 10.9499 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [115/489], Loss: 0.2608, Time: 10.9538 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [120/489], Loss: 0.2631, Time: 10.9576 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [125/489], Loss: 0.2639, Time: 10.9615 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [130/489], Loss: 0.2588, Time: 10.9653 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [135/489], Loss: 0.2619, Time: 10.9692 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [140/489], Loss: 0.2636, Time: 10.9732 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [145/489], Loss: 0.2626, Time: 10.9770 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [150/489], Loss: 0.2594, Time: 10.9808 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [155/489], Loss: 0.2600, Time: 10.9847 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [160/489], Loss: 0.2621, Time: 10.9885 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [165/489], Loss: 0.2593, Time: 10.9924 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [170/489], Loss: 0.2595, Time: 10.9962 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [175/489], Loss: 0.2619, Time: 11.0001 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [180/489], Loss: 0.2659, Time: 11.0039 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [185/489], Loss: 0.2673, Time: 11.0079 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [190/489], Loss: 0.2636, Time: 11.0118 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [195/489], Loss: 0.2602, Time: 11.0156 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [200/489], Loss: 0.2601, Time: 11.0194 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [205/489], Loss: 0.2611, Time: 11.0233 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [210/489], Loss: 0.2586, Time: 11.0272 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [215/489], Loss: 0.2642, Time: 11.0311 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [220/489], Loss: 0.2664, Time: 11.0349 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [225/489], Loss: 0.2640, Time: 11.0387 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [230/489], Loss: 0.2614, Time: 11.0427 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [235/489], Loss: 0.2614, Time: 11.0465 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [240/489], Loss: 0.2638, Time: 11.0504 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [245/489], Loss: 0.2631, Time: 11.0542 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [250/489], Loss: 0.2599, Time: 11.0582 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [255/489], Loss: 0.2635, Time: 11.0620 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [260/489], Loss: 0.2579, Time: 11.0660 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [265/489], Loss: 0.2631, Time: 11.0699 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [270/489], Loss: 0.2601, Time: 11.0739 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [275/489], Loss: 0.2608, Time: 11.0778 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [280/489], Loss: 0.2632, Time: 11.0816 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [285/489], Loss: 0.2672, Time: 11.0855 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [290/489], Loss: 0.2607, Time: 11.0893 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [295/489], Loss: 0.2610, Time: 11.0932 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [300/489], Loss: 0.2613, Time: 11.0970 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [305/489], Loss: 0.2587, Time: 11.1014 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [310/489], Loss: 0.2573, Time: 11.1086 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [315/489], Loss: 0.2626, Time: 11.1134 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [320/489], Loss: 0.2629, Time: 11.1179 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [325/489], Loss: 0.2616, Time: 11.1225 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [330/489], Loss: 0.2639, Time: 11.1267 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [335/489], Loss: 0.2656, Time: 11.1306 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [340/489], Loss: 0.2641, Time: 11.1344 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [345/489], Loss: 0.2647, Time: 11.1383 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [350/489], Loss: 0.2626, Time: 11.1421 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [355/489], Loss: 0.2634, Time: 11.1459 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [360/489], Loss: 0.2615, Time: 11.1498 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [365/489], Loss: 0.2614, Time: 11.1536 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [370/489], Loss: 0.2615, Time: 11.1575 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [375/489], Loss: 0.2610, Time: 11.1613 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [380/489], Loss: 0.2631, Time: 11.1652 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [385/489], Loss: 0.2599, Time: 11.1690 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [390/489], Loss: 0.2601, Time: 11.1728 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [395/489], Loss: 0.2626, Time: 11.1767 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [400/489], Loss: 0.2623, Time: 11.1805 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [405/489], Loss: 0.2641, Time: 11.1844 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [410/489], Loss: 0.2642, Time: 11.1882 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [415/489], Loss: 0.2596, Time: 11.1922 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [420/489], Loss: 0.2609, Time: 11.1961 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [425/489], Loss: 0.2649, Time: 11.1999 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [430/489], Loss: 0.2623, Time: 11.2037 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [435/489], Loss: 0.2620, Time: 11.2075 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [440/489], Loss: 0.2628, Time: 11.2114 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [445/489], Loss: 0.2588, Time: 11.2152 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [450/489], Loss: 0.2601, Time: 11.2190 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [455/489], Loss: 0.2635, Time: 11.2228 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [460/489], Loss: 0.2609, Time: 11.2267 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [465/489], Loss: 0.2623, Time: 11.2305 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [470/489], Loss: 0.2610, Time: 11.2343 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [475/489], Loss: 0.2664, Time: 11.2381 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [480/489], Loss: 0.2623, Time: 11.2420 secs, learning rate: 0.0000\n",
      "Epoch [22/30], Step [485/489], Loss: 0.2630, Time: 11.2459 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [5/489], Loss: 0.2612, Time: 11.3766 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [10/489], Loss: 0.2617, Time: 11.3818 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [15/489], Loss: 0.2635, Time: 11.3868 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [20/489], Loss: 0.2642, Time: 11.3912 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [25/489], Loss: 0.2583, Time: 11.3951 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [30/489], Loss: 0.2592, Time: 11.3989 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [35/489], Loss: 0.2610, Time: 11.4028 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [40/489], Loss: 0.2613, Time: 11.4067 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [45/489], Loss: 0.2603, Time: 11.4106 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [50/489], Loss: 0.2604, Time: 11.4144 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [55/489], Loss: 0.2653, Time: 11.4183 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [60/489], Loss: 0.2624, Time: 11.4221 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [65/489], Loss: 0.2625, Time: 11.4260 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [70/489], Loss: 0.2635, Time: 11.4299 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [75/489], Loss: 0.2638, Time: 11.4337 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [80/489], Loss: 0.2612, Time: 11.4375 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [85/489], Loss: 0.2605, Time: 11.4415 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [90/489], Loss: 0.2618, Time: 11.4454 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [95/489], Loss: 0.2656, Time: 11.4496 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [100/489], Loss: 0.2634, Time: 11.4542 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [105/489], Loss: 0.2625, Time: 11.4587 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [110/489], Loss: 0.2608, Time: 11.4629 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [115/489], Loss: 0.2635, Time: 11.4667 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [120/489], Loss: 0.2623, Time: 11.4706 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [125/489], Loss: 0.2634, Time: 11.4746 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [130/489], Loss: 0.2624, Time: 11.4785 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [135/489], Loss: 0.2607, Time: 11.4823 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [140/489], Loss: 0.2621, Time: 11.4862 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [145/489], Loss: 0.2623, Time: 11.4901 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [150/489], Loss: 0.2577, Time: 11.4939 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [155/489], Loss: 0.2605, Time: 11.4977 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [160/489], Loss: 0.2591, Time: 11.5016 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [165/489], Loss: 0.2605, Time: 11.5054 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [170/489], Loss: 0.2649, Time: 11.5094 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [175/489], Loss: 0.2586, Time: 11.5132 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [180/489], Loss: 0.2573, Time: 11.5171 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [185/489], Loss: 0.2610, Time: 11.5209 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [190/489], Loss: 0.2618, Time: 11.5249 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [195/489], Loss: 0.2613, Time: 11.5287 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [200/489], Loss: 0.2611, Time: 11.5325 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [205/489], Loss: 0.2610, Time: 11.5364 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [210/489], Loss: 0.2600, Time: 11.5403 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [215/489], Loss: 0.2639, Time: 11.5441 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [220/489], Loss: 0.2627, Time: 11.5479 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [225/489], Loss: 0.2662, Time: 11.5518 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [230/489], Loss: 0.2609, Time: 11.5556 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [235/489], Loss: 0.2593, Time: 11.5600 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [240/489], Loss: 0.2583, Time: 11.5646 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [245/489], Loss: 0.2614, Time: 11.5691 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [250/489], Loss: 0.2603, Time: 11.5734 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [255/489], Loss: 0.2620, Time: 11.5774 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [260/489], Loss: 0.2634, Time: 11.5813 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [265/489], Loss: 0.2619, Time: 11.5851 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [270/489], Loss: 0.2629, Time: 11.5892 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [275/489], Loss: 0.2622, Time: 11.5930 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [280/489], Loss: 0.2621, Time: 11.5969 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [285/489], Loss: 0.2617, Time: 11.6007 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [290/489], Loss: 0.2632, Time: 11.6045 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [295/489], Loss: 0.2626, Time: 11.6084 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [300/489], Loss: 0.2579, Time: 11.6122 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [305/489], Loss: 0.2648, Time: 11.6161 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [310/489], Loss: 0.2603, Time: 11.6199 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [315/489], Loss: 0.2620, Time: 11.6239 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [320/489], Loss: 0.2621, Time: 11.6277 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [325/489], Loss: 0.2627, Time: 11.6315 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [330/489], Loss: 0.2596, Time: 11.6354 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [335/489], Loss: 0.2608, Time: 11.6392 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [340/489], Loss: 0.2618, Time: 11.6431 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [345/489], Loss: 0.2638, Time: 11.6469 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [350/489], Loss: 0.2600, Time: 11.6507 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [355/489], Loss: 0.2647, Time: 11.6546 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [360/489], Loss: 0.2597, Time: 11.6586 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [365/489], Loss: 0.2598, Time: 11.6624 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [370/489], Loss: 0.2622, Time: 11.6662 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [375/489], Loss: 0.2622, Time: 11.6701 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [380/489], Loss: 0.2584, Time: 11.6739 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [385/489], Loss: 0.2603, Time: 11.6778 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [390/489], Loss: 0.2627, Time: 11.6816 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [395/489], Loss: 0.2705, Time: 11.6855 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [400/489], Loss: 0.2608, Time: 11.6893 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [405/489], Loss: 0.2587, Time: 11.6932 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [410/489], Loss: 0.2610, Time: 11.6970 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [415/489], Loss: 0.2611, Time: 11.7009 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [420/489], Loss: 0.2621, Time: 11.7047 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [425/489], Loss: 0.2629, Time: 11.7087 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [430/489], Loss: 0.2596, Time: 11.7126 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [435/489], Loss: 0.2634, Time: 11.7164 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [440/489], Loss: 0.2626, Time: 11.7202 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [445/489], Loss: 0.2598, Time: 11.7242 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [450/489], Loss: 0.2662, Time: 11.7280 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [455/489], Loss: 0.2597, Time: 11.7318 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [460/489], Loss: 0.2595, Time: 11.7356 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [465/489], Loss: 0.2608, Time: 11.7396 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [470/489], Loss: 0.2647, Time: 11.7435 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [475/489], Loss: 0.2623, Time: 11.7473 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [480/489], Loss: 0.2608, Time: 11.7512 secs, learning rate: 0.0000\n",
      "Epoch [23/30], Step [485/489], Loss: 0.2563, Time: 11.7550 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [5/489], Loss: 0.2601, Time: 11.8853 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [10/489], Loss: 0.2583, Time: 11.8906 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [15/489], Loss: 0.2621, Time: 11.8952 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [20/489], Loss: 0.2627, Time: 11.8996 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [25/489], Loss: 0.2603, Time: 11.9039 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [30/489], Loss: 0.2641, Time: 11.9079 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [35/489], Loss: 0.2649, Time: 11.9117 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [40/489], Loss: 0.2645, Time: 11.9156 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [45/489], Loss: 0.2620, Time: 11.9194 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [50/489], Loss: 0.2599, Time: 11.9234 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [55/489], Loss: 0.2592, Time: 11.9272 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [60/489], Loss: 0.2623, Time: 11.9310 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [65/489], Loss: 0.2602, Time: 11.9349 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [70/489], Loss: 0.2570, Time: 11.9387 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [75/489], Loss: 0.2634, Time: 11.9426 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [80/489], Loss: 0.2605, Time: 11.9465 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [85/489], Loss: 0.2607, Time: 11.9503 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [90/489], Loss: 0.2642, Time: 11.9541 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [95/489], Loss: 0.2590, Time: 11.9582 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [100/489], Loss: 0.2640, Time: 11.9620 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [105/489], Loss: 0.2619, Time: 11.9658 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [110/489], Loss: 0.2586, Time: 11.9696 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [115/489], Loss: 0.2625, Time: 11.9736 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [120/489], Loss: 0.2628, Time: 11.9775 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [125/489], Loss: 0.2664, Time: 11.9813 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [130/489], Loss: 0.2601, Time: 11.9851 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [135/489], Loss: 0.2628, Time: 11.9890 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [140/489], Loss: 0.2614, Time: 11.9930 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [145/489], Loss: 0.2616, Time: 11.9968 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [150/489], Loss: 0.2608, Time: 12.0006 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [155/489], Loss: 0.2600, Time: 12.0045 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [160/489], Loss: 0.2585, Time: 12.0083 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [165/489], Loss: 0.2624, Time: 12.0121 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [170/489], Loss: 0.2616, Time: 12.0160 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [175/489], Loss: 0.2726, Time: 12.0198 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [180/489], Loss: 0.2611, Time: 12.0238 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [185/489], Loss: 0.2605, Time: 12.0276 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [190/489], Loss: 0.2607, Time: 12.0315 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [195/489], Loss: 0.2631, Time: 12.0353 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [200/489], Loss: 0.2620, Time: 12.0393 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [205/489], Loss: 0.2595, Time: 12.0431 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [210/489], Loss: 0.2593, Time: 12.0469 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [215/489], Loss: 0.2592, Time: 12.0507 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [220/489], Loss: 0.2625, Time: 12.0546 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [225/489], Loss: 0.2605, Time: 12.0585 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [230/489], Loss: 0.2674, Time: 12.0624 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [235/489], Loss: 0.2633, Time: 12.0662 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [240/489], Loss: 0.2616, Time: 12.0700 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [245/489], Loss: 0.2652, Time: 12.0741 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [250/489], Loss: 0.2571, Time: 12.0779 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [255/489], Loss: 0.2615, Time: 12.0817 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [260/489], Loss: 0.2612, Time: 12.0856 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [265/489], Loss: 0.2669, Time: 12.0897 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [270/489], Loss: 0.2595, Time: 12.0937 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [275/489], Loss: 0.2651, Time: 12.0975 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [280/489], Loss: 0.2646, Time: 12.1013 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [285/489], Loss: 0.2606, Time: 12.1052 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [290/489], Loss: 0.2597, Time: 12.1091 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [295/489], Loss: 0.2585, Time: 12.1129 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [300/489], Loss: 0.2627, Time: 12.1167 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [305/489], Loss: 0.2624, Time: 12.1206 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [310/489], Loss: 0.2625, Time: 12.1245 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [315/489], Loss: 0.2591, Time: 12.1284 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [320/489], Loss: 0.2594, Time: 12.1322 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [325/489], Loss: 0.2618, Time: 12.1360 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [330/489], Loss: 0.2635, Time: 12.1400 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [335/489], Loss: 0.2588, Time: 12.1438 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [340/489], Loss: 0.2624, Time: 12.1477 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [345/489], Loss: 0.2608, Time: 12.1515 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [350/489], Loss: 0.2559, Time: 12.1553 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [355/489], Loss: 0.2596, Time: 12.1593 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [360/489], Loss: 0.2599, Time: 12.1631 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [365/489], Loss: 0.2601, Time: 12.1669 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [370/489], Loss: 0.2596, Time: 12.1708 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [375/489], Loss: 0.2658, Time: 12.1747 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [380/489], Loss: 0.2681, Time: 12.1786 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [385/489], Loss: 0.2638, Time: 12.1824 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [390/489], Loss: 0.2615, Time: 12.1863 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [395/489], Loss: 0.2596, Time: 12.1901 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [400/489], Loss: 0.2586, Time: 12.1939 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [405/489], Loss: 0.2622, Time: 12.1978 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [410/489], Loss: 0.2612, Time: 12.2016 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [415/489], Loss: 0.2594, Time: 12.2055 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [420/489], Loss: 0.2602, Time: 12.2094 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [425/489], Loss: 0.2621, Time: 12.2132 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [430/489], Loss: 0.2599, Time: 12.2171 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [435/489], Loss: 0.2607, Time: 12.2209 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [440/489], Loss: 0.2594, Time: 12.2249 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [445/489], Loss: 0.2618, Time: 12.2287 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [450/489], Loss: 0.2620, Time: 12.2326 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [455/489], Loss: 0.2601, Time: 12.2364 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [460/489], Loss: 0.2611, Time: 12.2403 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [465/489], Loss: 0.2612, Time: 12.2442 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [470/489], Loss: 0.2615, Time: 12.2480 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [475/489], Loss: 0.2624, Time: 12.2518 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [480/489], Loss: 0.2619, Time: 12.2557 secs, learning rate: 0.0000\n",
      "Epoch [24/30], Step [485/489], Loss: 0.2733, Time: 12.2595 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [5/489], Loss: 0.2650, Time: 12.3886 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [10/489], Loss: 0.2601, Time: 12.3933 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [15/489], Loss: 0.2627, Time: 12.3978 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [20/489], Loss: 0.2595, Time: 12.4019 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [25/489], Loss: 0.2568, Time: 12.4059 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [30/489], Loss: 0.2595, Time: 12.4098 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [35/489], Loss: 0.2630, Time: 12.4136 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [40/489], Loss: 0.2601, Time: 12.4175 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [45/489], Loss: 0.2579, Time: 12.4213 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [50/489], Loss: 0.2607, Time: 12.4253 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [55/489], Loss: 0.2620, Time: 12.4291 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [60/489], Loss: 0.2592, Time: 12.4329 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [65/489], Loss: 0.2568, Time: 12.4368 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [70/489], Loss: 0.2617, Time: 12.4408 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [75/489], Loss: 0.2609, Time: 12.4446 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [80/489], Loss: 0.2592, Time: 12.4484 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [85/489], Loss: 0.2613, Time: 12.4523 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [90/489], Loss: 0.2651, Time: 12.4562 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [95/489], Loss: 0.2639, Time: 12.4601 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [100/489], Loss: 0.2625, Time: 12.4639 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [105/489], Loss: 0.2605, Time: 12.4677 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [110/489], Loss: 0.2617, Time: 12.4716 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [115/489], Loss: 0.2609, Time: 12.4756 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [120/489], Loss: 0.2616, Time: 12.4795 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [125/489], Loss: 0.2628, Time: 12.4833 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [130/489], Loss: 0.2630, Time: 12.4871 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [135/489], Loss: 0.2603, Time: 12.4911 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [140/489], Loss: 0.2583, Time: 12.4949 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [145/489], Loss: 0.2602, Time: 12.4987 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [150/489], Loss: 0.2607, Time: 12.5026 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [155/489], Loss: 0.2602, Time: 12.5065 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [160/489], Loss: 0.2626, Time: 12.5104 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [165/489], Loss: 0.2623, Time: 12.5142 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [170/489], Loss: 0.2636, Time: 12.5180 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [175/489], Loss: 0.2610, Time: 12.5219 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [180/489], Loss: 0.2581, Time: 12.5257 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [185/489], Loss: 0.2620, Time: 12.5295 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [190/489], Loss: 0.2575, Time: 12.5334 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [195/489], Loss: 0.2621, Time: 12.5372 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [200/489], Loss: 0.2658, Time: 12.5412 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [205/489], Loss: 0.2613, Time: 12.5450 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [210/489], Loss: 0.2620, Time: 12.5489 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [215/489], Loss: 0.2611, Time: 12.5527 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [220/489], Loss: 0.2588, Time: 12.5567 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [225/489], Loss: 0.2625, Time: 12.5605 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [230/489], Loss: 0.2667, Time: 12.5644 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [235/489], Loss: 0.2601, Time: 12.5683 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [240/489], Loss: 0.2599, Time: 12.5721 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [245/489], Loss: 0.2627, Time: 12.5761 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [250/489], Loss: 0.2599, Time: 12.5800 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [255/489], Loss: 0.2617, Time: 12.5838 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [260/489], Loss: 0.2600, Time: 12.5877 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [265/489], Loss: 0.2623, Time: 12.5918 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [270/489], Loss: 0.2646, Time: 12.5958 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [275/489], Loss: 0.2614, Time: 12.5996 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [280/489], Loss: 0.2622, Time: 12.6035 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [285/489], Loss: 0.2569, Time: 12.6075 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [290/489], Loss: 0.2603, Time: 12.6113 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [295/489], Loss: 0.2608, Time: 12.6152 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [300/489], Loss: 0.2715, Time: 12.6190 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [305/489], Loss: 0.2613, Time: 12.6230 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [310/489], Loss: 0.2623, Time: 12.6269 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [315/489], Loss: 0.2610, Time: 12.6307 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [320/489], Loss: 0.2619, Time: 12.6345 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [325/489], Loss: 0.2595, Time: 12.6383 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [330/489], Loss: 0.2638, Time: 12.6422 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [335/489], Loss: 0.2593, Time: 12.6460 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [340/489], Loss: 0.2592, Time: 12.6498 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [345/489], Loss: 0.2617, Time: 12.6537 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [350/489], Loss: 0.2595, Time: 12.6575 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [355/489], Loss: 0.2639, Time: 12.6614 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [360/489], Loss: 0.2641, Time: 12.6652 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [365/489], Loss: 0.2594, Time: 12.6690 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [370/489], Loss: 0.2602, Time: 12.6730 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [375/489], Loss: 0.2616, Time: 12.6768 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [380/489], Loss: 0.2587, Time: 12.6807 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [385/489], Loss: 0.2586, Time: 12.6845 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [390/489], Loss: 0.2592, Time: 12.6883 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [395/489], Loss: 0.2603, Time: 12.6923 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [400/489], Loss: 0.2589, Time: 12.6961 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [405/489], Loss: 0.2614, Time: 12.7000 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [410/489], Loss: 0.2603, Time: 12.7038 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [415/489], Loss: 0.2642, Time: 12.7078 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [420/489], Loss: 0.2626, Time: 12.7116 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [425/489], Loss: 0.2572, Time: 12.7154 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [430/489], Loss: 0.2617, Time: 12.7193 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [435/489], Loss: 0.2608, Time: 12.7232 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [440/489], Loss: 0.2598, Time: 12.7270 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [445/489], Loss: 0.2604, Time: 12.7309 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [450/489], Loss: 0.2631, Time: 12.7347 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [455/489], Loss: 0.2574, Time: 12.7385 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [460/489], Loss: 0.2604, Time: 12.7424 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [465/489], Loss: 0.2604, Time: 12.7462 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [470/489], Loss: 0.2627, Time: 12.7501 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [475/489], Loss: 0.2586, Time: 12.7539 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [480/489], Loss: 0.2665, Time: 12.7579 secs, learning rate: 0.0000\n",
      "Epoch [25/30], Step [485/489], Loss: 0.2624, Time: 12.7618 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [5/489], Loss: 0.2600, Time: 12.8931 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [10/489], Loss: 0.2619, Time: 12.8983 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [15/489], Loss: 0.2622, Time: 12.9035 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [20/489], Loss: 0.2561, Time: 12.9083 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [25/489], Loss: 0.2617, Time: 12.9127 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [30/489], Loss: 0.2652, Time: 12.9166 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [35/489], Loss: 0.2745, Time: 12.9204 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [40/489], Loss: 0.2621, Time: 12.9244 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [45/489], Loss: 0.2599, Time: 12.9283 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [50/489], Loss: 0.2615, Time: 12.9321 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [55/489], Loss: 0.2597, Time: 12.9360 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [60/489], Loss: 0.2653, Time: 12.9400 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [65/489], Loss: 0.2593, Time: 12.9438 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [70/489], Loss: 0.2605, Time: 12.9477 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [75/489], Loss: 0.2619, Time: 12.9515 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [80/489], Loss: 0.2597, Time: 12.9554 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [85/489], Loss: 0.2649, Time: 12.9593 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [90/489], Loss: 0.2602, Time: 12.9632 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [95/489], Loss: 0.2619, Time: 12.9670 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [100/489], Loss: 0.2585, Time: 12.9708 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [105/489], Loss: 0.2588, Time: 12.9749 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [110/489], Loss: 0.2645, Time: 12.9787 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [115/489], Loss: 0.2650, Time: 12.9825 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [120/489], Loss: 0.2680, Time: 12.9863 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [125/489], Loss: 0.2596, Time: 12.9903 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [130/489], Loss: 0.2554, Time: 12.9942 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [135/489], Loss: 0.2634, Time: 12.9980 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [140/489], Loss: 0.2614, Time: 13.0019 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [145/489], Loss: 0.2625, Time: 13.0059 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [150/489], Loss: 0.2637, Time: 13.0097 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [155/489], Loss: 0.2605, Time: 13.0136 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [160/489], Loss: 0.2567, Time: 13.0173 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [165/489], Loss: 0.2595, Time: 13.0212 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [170/489], Loss: 0.2586, Time: 13.0252 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [175/489], Loss: 0.2642, Time: 13.0290 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [180/489], Loss: 0.2590, Time: 13.0329 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [185/489], Loss: 0.2615, Time: 13.0367 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [190/489], Loss: 0.2583, Time: 13.0407 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [195/489], Loss: 0.2589, Time: 13.0445 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [200/489], Loss: 0.2589, Time: 13.0483 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [205/489], Loss: 0.2646, Time: 13.0522 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [210/489], Loss: 0.2614, Time: 13.0560 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [215/489], Loss: 0.2571, Time: 13.0599 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [220/489], Loss: 0.2603, Time: 13.0637 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [225/489], Loss: 0.2604, Time: 13.0675 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [230/489], Loss: 0.2600, Time: 13.0714 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [235/489], Loss: 0.2597, Time: 13.0753 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [240/489], Loss: 0.2637, Time: 13.0792 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [245/489], Loss: 0.2574, Time: 13.0830 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [250/489], Loss: 0.2613, Time: 13.0868 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [255/489], Loss: 0.2582, Time: 13.0908 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [260/489], Loss: 0.2614, Time: 13.0947 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [265/489], Loss: 0.2608, Time: 13.0986 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [270/489], Loss: 0.2597, Time: 13.1025 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [275/489], Loss: 0.2662, Time: 13.1064 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [280/489], Loss: 0.2669, Time: 13.1103 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [285/489], Loss: 0.2639, Time: 13.1141 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [290/489], Loss: 0.2591, Time: 13.1180 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [295/489], Loss: 0.2580, Time: 13.1218 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [300/489], Loss: 0.2603, Time: 13.1257 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [305/489], Loss: 0.2641, Time: 13.1296 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [310/489], Loss: 0.2621, Time: 13.1334 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [315/489], Loss: 0.2609, Time: 13.1372 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [320/489], Loss: 0.2613, Time: 13.1412 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [325/489], Loss: 0.2590, Time: 13.1451 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [330/489], Loss: 0.2602, Time: 13.1489 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [335/489], Loss: 0.2573, Time: 13.1527 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [340/489], Loss: 0.2659, Time: 13.1567 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [345/489], Loss: 0.2622, Time: 13.1605 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [350/489], Loss: 0.2582, Time: 13.1644 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [355/489], Loss: 0.2581, Time: 13.1682 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [360/489], Loss: 0.2621, Time: 13.1721 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [365/489], Loss: 0.2557, Time: 13.1760 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [370/489], Loss: 0.2617, Time: 13.1798 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [375/489], Loss: 0.2600, Time: 13.1837 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [380/489], Loss: 0.2626, Time: 13.1875 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [385/489], Loss: 0.2628, Time: 13.1915 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [390/489], Loss: 0.2608, Time: 13.1953 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [395/489], Loss: 0.2616, Time: 13.1991 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [400/489], Loss: 0.2604, Time: 13.2030 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [405/489], Loss: 0.2602, Time: 13.2068 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [410/489], Loss: 0.2587, Time: 13.2106 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [415/489], Loss: 0.2586, Time: 13.2145 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [420/489], Loss: 0.2619, Time: 13.2183 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [425/489], Loss: 0.2618, Time: 13.2222 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [430/489], Loss: 0.2596, Time: 13.2261 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [435/489], Loss: 0.2612, Time: 13.2299 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [440/489], Loss: 0.2712, Time: 13.2338 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [445/489], Loss: 0.2625, Time: 13.2376 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [450/489], Loss: 0.2620, Time: 13.2415 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [455/489], Loss: 0.2618, Time: 13.2454 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [460/489], Loss: 0.2595, Time: 13.2492 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [465/489], Loss: 0.2601, Time: 13.2530 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [470/489], Loss: 0.2594, Time: 13.2570 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [475/489], Loss: 0.2615, Time: 13.2608 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [480/489], Loss: 0.2633, Time: 13.2646 secs, learning rate: 0.0000\n",
      "Epoch [26/30], Step [485/489], Loss: 0.2621, Time: 13.2685 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [5/489], Loss: 0.2616, Time: 13.3978 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [10/489], Loss: 0.2605, Time: 13.4027 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [15/489], Loss: 0.2585, Time: 13.4075 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [20/489], Loss: 0.2628, Time: 13.4128 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [25/489], Loss: 0.2620, Time: 13.4170 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [30/489], Loss: 0.2598, Time: 13.4209 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [35/489], Loss: 0.2588, Time: 13.4249 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [40/489], Loss: 0.2599, Time: 13.4287 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [45/489], Loss: 0.2680, Time: 13.4326 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [50/489], Loss: 0.2577, Time: 13.4364 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [55/489], Loss: 0.2614, Time: 13.4404 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [60/489], Loss: 0.2633, Time: 13.4442 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [65/489], Loss: 0.2615, Time: 13.4481 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [70/489], Loss: 0.2608, Time: 13.4519 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [75/489], Loss: 0.2587, Time: 13.4559 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [80/489], Loss: 0.2633, Time: 13.4597 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [85/489], Loss: 0.2592, Time: 13.4636 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [90/489], Loss: 0.2600, Time: 13.4674 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [95/489], Loss: 0.2613, Time: 13.4713 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [100/489], Loss: 0.2548, Time: 13.4752 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [105/489], Loss: 0.2613, Time: 13.4791 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [110/489], Loss: 0.2605, Time: 13.4829 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [115/489], Loss: 0.2646, Time: 13.4868 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [120/489], Loss: 0.2671, Time: 13.4908 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [125/489], Loss: 0.2635, Time: 13.4946 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [130/489], Loss: 0.2607, Time: 13.4984 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [135/489], Loss: 0.2603, Time: 13.5023 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [140/489], Loss: 0.2628, Time: 13.5062 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [145/489], Loss: 0.2592, Time: 13.5101 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [150/489], Loss: 0.2598, Time: 13.5139 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [155/489], Loss: 0.2655, Time: 13.5178 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [160/489], Loss: 0.2571, Time: 13.5216 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [165/489], Loss: 0.2618, Time: 13.5256 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [170/489], Loss: 0.2617, Time: 13.5295 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [175/489], Loss: 0.2608, Time: 13.5333 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [180/489], Loss: 0.2606, Time: 13.5371 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [185/489], Loss: 0.2595, Time: 13.5411 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [190/489], Loss: 0.2609, Time: 13.5449 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [195/489], Loss: 0.2590, Time: 13.5488 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [200/489], Loss: 0.2596, Time: 13.5526 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [205/489], Loss: 0.2589, Time: 13.5566 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [210/489], Loss: 0.2624, Time: 13.5604 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [215/489], Loss: 0.2604, Time: 13.5643 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [220/489], Loss: 0.2600, Time: 13.5681 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [225/489], Loss: 0.2627, Time: 13.5720 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [230/489], Loss: 0.2637, Time: 13.5758 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [235/489], Loss: 0.2588, Time: 13.5797 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [240/489], Loss: 0.2590, Time: 13.5836 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [245/489], Loss: 0.2592, Time: 13.5881 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [250/489], Loss: 0.2625, Time: 13.5928 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [255/489], Loss: 0.2614, Time: 13.5971 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [260/489], Loss: 0.2565, Time: 13.6012 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [265/489], Loss: 0.2626, Time: 13.6051 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [270/489], Loss: 0.2553, Time: 13.6091 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [275/489], Loss: 0.2608, Time: 13.6129 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [280/489], Loss: 0.2616, Time: 13.6168 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [285/489], Loss: 0.2586, Time: 13.6206 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [290/489], Loss: 0.2568, Time: 13.6246 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [295/489], Loss: 0.2601, Time: 13.6284 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [300/489], Loss: 0.2610, Time: 13.6323 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [305/489], Loss: 0.2599, Time: 13.6362 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [310/489], Loss: 0.2601, Time: 13.6401 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [315/489], Loss: 0.2631, Time: 13.6439 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [320/489], Loss: 0.2629, Time: 13.6478 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [325/489], Loss: 0.2610, Time: 13.6517 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [330/489], Loss: 0.2614, Time: 13.6555 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [335/489], Loss: 0.2592, Time: 13.6595 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [340/489], Loss: 0.2625, Time: 13.6634 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [345/489], Loss: 0.2635, Time: 13.6672 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [350/489], Loss: 0.2601, Time: 13.6711 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [355/489], Loss: 0.2606, Time: 13.6750 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [360/489], Loss: 0.2640, Time: 13.6788 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [365/489], Loss: 0.2597, Time: 13.6827 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [370/489], Loss: 0.2616, Time: 13.6865 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [375/489], Loss: 0.2611, Time: 13.6906 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [380/489], Loss: 0.2589, Time: 13.6944 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [385/489], Loss: 0.2604, Time: 13.6982 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [390/489], Loss: 0.2587, Time: 13.7021 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [395/489], Loss: 0.2613, Time: 13.7060 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [400/489], Loss: 0.2649, Time: 13.7099 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [405/489], Loss: 0.2727, Time: 13.7137 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [410/489], Loss: 0.2628, Time: 13.7176 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [415/489], Loss: 0.2609, Time: 13.7214 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [420/489], Loss: 0.2633, Time: 13.7254 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [425/489], Loss: 0.2576, Time: 13.7293 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [430/489], Loss: 0.2564, Time: 13.7331 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [435/489], Loss: 0.2620, Time: 13.7370 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [440/489], Loss: 0.2692, Time: 13.7409 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [445/489], Loss: 0.2577, Time: 13.7447 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [450/489], Loss: 0.2573, Time: 13.7486 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [455/489], Loss: 0.2625, Time: 13.7524 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [460/489], Loss: 0.2686, Time: 13.7564 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [465/489], Loss: 0.2780, Time: 13.7603 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [470/489], Loss: 0.2638, Time: 13.7641 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [475/489], Loss: 0.2657, Time: 13.7679 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [480/489], Loss: 0.2599, Time: 13.7718 secs, learning rate: 0.0000\n",
      "Epoch [27/30], Step [485/489], Loss: 0.2635, Time: 13.7757 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [5/489], Loss: 0.2586, Time: 13.9055 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [10/489], Loss: 0.2615, Time: 13.9108 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [15/489], Loss: 0.2621, Time: 13.9156 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [20/489], Loss: 0.2629, Time: 13.9199 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [25/489], Loss: 0.2739, Time: 13.9243 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [30/489], Loss: 0.2711, Time: 13.9284 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [35/489], Loss: 0.2708, Time: 13.9322 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [40/489], Loss: 0.2607, Time: 13.9361 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [45/489], Loss: 0.2596, Time: 13.9400 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [50/489], Loss: 0.2640, Time: 13.9439 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [55/489], Loss: 0.2648, Time: 13.9477 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [60/489], Loss: 0.2574, Time: 13.9516 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [65/489], Loss: 0.2601, Time: 13.9555 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [70/489], Loss: 0.2613, Time: 13.9594 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [75/489], Loss: 0.2651, Time: 13.9633 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [80/489], Loss: 0.2563, Time: 13.9671 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [85/489], Loss: 0.2681, Time: 13.9709 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [90/489], Loss: 0.2591, Time: 13.9749 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [95/489], Loss: 0.2598, Time: 13.9788 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [100/489], Loss: 0.2595, Time: 13.9826 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [105/489], Loss: 0.2588, Time: 13.9865 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [110/489], Loss: 0.2578, Time: 13.9904 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [115/489], Loss: 0.2590, Time: 13.9943 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [120/489], Loss: 0.2574, Time: 13.9981 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [125/489], Loss: 0.2571, Time: 14.0020 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [130/489], Loss: 0.2568, Time: 14.0059 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [135/489], Loss: 0.2631, Time: 14.0098 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [140/489], Loss: 0.2582, Time: 14.0136 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [145/489], Loss: 0.2608, Time: 14.0175 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [150/489], Loss: 0.2590, Time: 14.0213 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [155/489], Loss: 0.2586, Time: 14.0253 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [160/489], Loss: 0.2611, Time: 14.0292 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [165/489], Loss: 0.2612, Time: 14.0330 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [170/489], Loss: 0.2639, Time: 14.0368 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [175/489], Loss: 0.2613, Time: 14.0408 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [180/489], Loss: 0.2596, Time: 14.0446 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [185/489], Loss: 0.2659, Time: 14.0485 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [190/489], Loss: 0.2619, Time: 14.0523 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [195/489], Loss: 0.2632, Time: 14.0563 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [200/489], Loss: 0.2694, Time: 14.0601 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [205/489], Loss: 0.2651, Time: 14.0639 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [210/489], Loss: 0.2574, Time: 14.0678 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [215/489], Loss: 0.2607, Time: 14.0716 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [220/489], Loss: 0.2590, Time: 14.0756 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [225/489], Loss: 0.2619, Time: 14.0795 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [230/489], Loss: 0.2624, Time: 14.0834 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [235/489], Loss: 0.2609, Time: 14.0872 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [240/489], Loss: 0.2580, Time: 14.0912 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [245/489], Loss: 0.2605, Time: 14.0950 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [250/489], Loss: 0.2580, Time: 14.0988 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [255/489], Loss: 0.2599, Time: 14.1027 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [260/489], Loss: 0.2599, Time: 14.1068 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [265/489], Loss: 0.2613, Time: 14.1108 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [270/489], Loss: 0.2597, Time: 14.1146 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [275/489], Loss: 0.2641, Time: 14.1185 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [280/489], Loss: 0.2603, Time: 14.1223 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [285/489], Loss: 0.2606, Time: 14.1263 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [290/489], Loss: 0.2597, Time: 14.1301 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [295/489], Loss: 0.2575, Time: 14.1340 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [300/489], Loss: 0.2611, Time: 14.1378 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [305/489], Loss: 0.2601, Time: 14.1417 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [310/489], Loss: 0.2620, Time: 14.1455 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [315/489], Loss: 0.2592, Time: 14.1494 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [320/489], Loss: 0.2584, Time: 14.1532 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [325/489], Loss: 0.2604, Time: 14.1572 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [330/489], Loss: 0.2581, Time: 14.1610 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [335/489], Loss: 0.2617, Time: 14.1648 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [340/489], Loss: 0.2614, Time: 14.1687 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [345/489], Loss: 0.2612, Time: 14.1726 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [350/489], Loss: 0.2583, Time: 14.1765 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [355/489], Loss: 0.2633, Time: 14.1803 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [360/489], Loss: 0.2586, Time: 14.1841 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [365/489], Loss: 0.2606, Time: 14.1880 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [370/489], Loss: 0.2615, Time: 14.1919 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [375/489], Loss: 0.2634, Time: 14.1957 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [380/489], Loss: 0.2711, Time: 14.1996 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [385/489], Loss: 0.2628, Time: 14.2034 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [390/489], Loss: 0.2608, Time: 14.2074 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [395/489], Loss: 0.2592, Time: 14.2113 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [400/489], Loss: 0.2585, Time: 14.2151 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [405/489], Loss: 0.2564, Time: 14.2189 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [410/489], Loss: 0.2601, Time: 14.2229 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [415/489], Loss: 0.2579, Time: 14.2268 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [420/489], Loss: 0.2587, Time: 14.2306 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [425/489], Loss: 0.2599, Time: 14.2345 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [430/489], Loss: 0.2604, Time: 14.2383 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [435/489], Loss: 0.2610, Time: 14.2423 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [440/489], Loss: 0.2620, Time: 14.2461 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [445/489], Loss: 0.2602, Time: 14.2500 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [450/489], Loss: 0.2633, Time: 14.2538 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [455/489], Loss: 0.2609, Time: 14.2578 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [460/489], Loss: 0.2626, Time: 14.2616 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [465/489], Loss: 0.2598, Time: 14.2655 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [470/489], Loss: 0.2590, Time: 14.2693 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [475/489], Loss: 0.2591, Time: 14.2733 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [480/489], Loss: 0.2607, Time: 14.2771 secs, learning rate: 0.0000\n",
      "Epoch [28/30], Step [485/489], Loss: 0.2590, Time: 14.2810 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [5/489], Loss: 0.2588, Time: 14.4099 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [10/489], Loss: 0.2658, Time: 14.4145 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [15/489], Loss: 0.2630, Time: 14.4193 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [20/489], Loss: 0.2609, Time: 14.4236 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [25/489], Loss: 0.2609, Time: 14.4278 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [30/489], Loss: 0.2599, Time: 14.4316 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [35/489], Loss: 0.2626, Time: 14.4355 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [40/489], Loss: 0.2605, Time: 14.4394 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [45/489], Loss: 0.2736, Time: 14.4432 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [50/489], Loss: 0.2718, Time: 14.4470 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [55/489], Loss: 0.2598, Time: 14.4509 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [60/489], Loss: 0.2586, Time: 14.4547 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [65/489], Loss: 0.2520, Time: 14.4587 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [70/489], Loss: 0.2592, Time: 14.4625 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [75/489], Loss: 0.2611, Time: 14.4664 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [80/489], Loss: 0.2598, Time: 14.4702 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [85/489], Loss: 0.2601, Time: 14.4742 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [90/489], Loss: 0.2627, Time: 14.4780 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [95/489], Loss: 0.2645, Time: 14.4819 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [100/489], Loss: 0.2612, Time: 14.4857 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [105/489], Loss: 0.2587, Time: 14.4897 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [110/489], Loss: 0.2585, Time: 14.4936 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [115/489], Loss: 0.2600, Time: 14.4974 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [120/489], Loss: 0.2598, Time: 14.5012 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [125/489], Loss: 0.2608, Time: 14.5051 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [130/489], Loss: 0.2595, Time: 14.5090 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [135/489], Loss: 0.2598, Time: 14.5129 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [140/489], Loss: 0.2574, Time: 14.5167 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [145/489], Loss: 0.2624, Time: 14.5205 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [150/489], Loss: 0.2603, Time: 14.5245 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [155/489], Loss: 0.2598, Time: 14.5284 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [160/489], Loss: 0.2635, Time: 14.5323 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [165/489], Loss: 0.2580, Time: 14.5362 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [170/489], Loss: 0.2597, Time: 14.5401 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [175/489], Loss: 0.2582, Time: 14.5440 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [180/489], Loss: 0.2556, Time: 14.5478 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [185/489], Loss: 0.2661, Time: 14.5517 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [190/489], Loss: 0.2584, Time: 14.5555 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [195/489], Loss: 0.2622, Time: 14.5595 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [200/489], Loss: 0.2587, Time: 14.5634 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [205/489], Loss: 0.2582, Time: 14.5672 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [210/489], Loss: 0.2598, Time: 14.5711 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [215/489], Loss: 0.2578, Time: 14.5750 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [220/489], Loss: 0.2612, Time: 14.5789 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [225/489], Loss: 0.2588, Time: 14.5828 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [230/489], Loss: 0.2584, Time: 14.5866 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [235/489], Loss: 0.2573, Time: 14.5907 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [240/489], Loss: 0.2577, Time: 14.5945 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [245/489], Loss: 0.2599, Time: 14.5984 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [250/489], Loss: 0.2620, Time: 14.6022 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [255/489], Loss: 0.2697, Time: 14.6061 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [260/489], Loss: 0.2611, Time: 14.6099 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [265/489], Loss: 0.2644, Time: 14.6139 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [270/489], Loss: 0.2579, Time: 14.6178 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [275/489], Loss: 0.2579, Time: 14.6217 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [280/489], Loss: 0.2583, Time: 14.6257 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [285/489], Loss: 0.2581, Time: 14.6295 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [290/489], Loss: 0.2636, Time: 14.6334 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [295/489], Loss: 0.2610, Time: 14.6372 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [300/489], Loss: 0.2573, Time: 14.6412 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [305/489], Loss: 0.2573, Time: 14.6451 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [310/489], Loss: 0.2623, Time: 14.6489 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [315/489], Loss: 0.2584, Time: 14.6528 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [320/489], Loss: 0.2579, Time: 14.6567 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [325/489], Loss: 0.2608, Time: 14.6606 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [330/489], Loss: 0.2616, Time: 14.6645 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [335/489], Loss: 0.2601, Time: 14.6683 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [340/489], Loss: 0.2604, Time: 14.6722 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [345/489], Loss: 0.2617, Time: 14.6761 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [350/489], Loss: 0.2611, Time: 14.6800 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [355/489], Loss: 0.2628, Time: 14.6838 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [360/489], Loss: 0.2582, Time: 14.6877 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [365/489], Loss: 0.2589, Time: 14.6916 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [370/489], Loss: 0.2586, Time: 14.6954 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [375/489], Loss: 0.2592, Time: 14.6993 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [380/489], Loss: 0.2696, Time: 14.7031 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [385/489], Loss: 0.2663, Time: 14.7071 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [390/489], Loss: 0.2602, Time: 14.7109 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [395/489], Loss: 0.2636, Time: 14.7147 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [400/489], Loss: 0.2621, Time: 14.7186 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [405/489], Loss: 0.2657, Time: 14.7226 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [410/489], Loss: 0.2624, Time: 14.7264 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [415/489], Loss: 0.2599, Time: 14.7303 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [420/489], Loss: 0.2499, Time: 14.7341 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [425/489], Loss: 0.2633, Time: 14.7380 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [430/489], Loss: 0.2604, Time: 14.7419 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [435/489], Loss: 0.2640, Time: 14.7458 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [440/489], Loss: 0.2620, Time: 14.7496 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [445/489], Loss: 0.2580, Time: 14.7535 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [450/489], Loss: 0.2587, Time: 14.7574 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [455/489], Loss: 0.2566, Time: 14.7613 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [460/489], Loss: 0.2643, Time: 14.7651 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [465/489], Loss: 0.2620, Time: 14.7689 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [470/489], Loss: 0.2596, Time: 14.7729 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [475/489], Loss: 0.2599, Time: 14.7767 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [480/489], Loss: 0.2607, Time: 14.7806 secs, learning rate: 0.0000\n",
      "Epoch [29/30], Step [485/489], Loss: 0.2606, Time: 14.7844 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [5/489], Loss: 0.2578, Time: 14.9148 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [10/489], Loss: 0.2609, Time: 14.9198 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [15/489], Loss: 0.2573, Time: 14.9250 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [20/489], Loss: 0.2595, Time: 14.9296 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [25/489], Loss: 0.2603, Time: 14.9340 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [30/489], Loss: 0.2618, Time: 14.9379 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [35/489], Loss: 0.2560, Time: 14.9419 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [40/489], Loss: 0.2611, Time: 14.9457 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [45/489], Loss: 0.2574, Time: 14.9496 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [50/489], Loss: 0.2622, Time: 14.9534 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [55/489], Loss: 0.2590, Time: 14.9575 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [60/489], Loss: 0.2585, Time: 14.9613 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [65/489], Loss: 0.2641, Time: 14.9651 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [70/489], Loss: 0.2608, Time: 14.9690 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [75/489], Loss: 0.2626, Time: 14.9729 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [80/489], Loss: 0.2560, Time: 14.9767 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [85/489], Loss: 0.2637, Time: 14.9806 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [90/489], Loss: 0.2582, Time: 14.9844 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [95/489], Loss: 0.2597, Time: 14.9882 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [100/489], Loss: 0.2575, Time: 14.9922 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [105/489], Loss: 0.2611, Time: 14.9961 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [110/489], Loss: 0.2576, Time: 14.9999 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [115/489], Loss: 0.2609, Time: 15.0037 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [120/489], Loss: 0.2566, Time: 15.0076 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [125/489], Loss: 0.2600, Time: 15.0114 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [130/489], Loss: 0.2597, Time: 15.0153 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [135/489], Loss: 0.2584, Time: 15.0191 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [140/489], Loss: 0.2580, Time: 15.0231 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [145/489], Loss: 0.2591, Time: 15.0269 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [150/489], Loss: 0.2586, Time: 15.0307 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [155/489], Loss: 0.2626, Time: 15.0346 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [160/489], Loss: 0.2597, Time: 15.0384 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [165/489], Loss: 0.2631, Time: 15.0424 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [170/489], Loss: 0.2596, Time: 15.0462 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [175/489], Loss: 0.2593, Time: 15.0500 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [180/489], Loss: 0.2575, Time: 15.0538 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [185/489], Loss: 0.2638, Time: 15.0578 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [190/489], Loss: 0.2581, Time: 15.0617 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [195/489], Loss: 0.2587, Time: 15.0655 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [200/489], Loss: 0.2606, Time: 15.0693 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [205/489], Loss: 0.2541, Time: 15.0734 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [210/489], Loss: 0.2588, Time: 15.0772 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [215/489], Loss: 0.2609, Time: 15.0811 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [220/489], Loss: 0.2587, Time: 15.0849 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [225/489], Loss: 0.2617, Time: 15.0888 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [230/489], Loss: 0.2608, Time: 15.0927 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [235/489], Loss: 0.2588, Time: 15.0965 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [240/489], Loss: 0.2649, Time: 15.1004 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [245/489], Loss: 0.2647, Time: 15.1042 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [250/489], Loss: 0.2589, Time: 15.1082 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [255/489], Loss: 0.2614, Time: 15.1121 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [260/489], Loss: 0.2587, Time: 15.1161 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [265/489], Loss: 0.2623, Time: 15.1200 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [270/489], Loss: 0.2583, Time: 15.1240 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [275/489], Loss: 0.2614, Time: 15.1278 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [280/489], Loss: 0.2581, Time: 15.1317 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [285/489], Loss: 0.2609, Time: 15.1355 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [290/489], Loss: 0.2571, Time: 15.1394 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [295/489], Loss: 0.2541, Time: 15.1432 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [300/489], Loss: 0.2614, Time: 15.1471 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [305/489], Loss: 0.2633, Time: 15.1509 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [310/489], Loss: 0.2609, Time: 15.1548 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [315/489], Loss: 0.2598, Time: 15.1587 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [320/489], Loss: 0.2583, Time: 15.1626 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [325/489], Loss: 0.2598, Time: 15.1665 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [330/489], Loss: 0.2594, Time: 15.1703 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [335/489], Loss: 0.2634, Time: 15.1742 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [340/489], Loss: 0.2625, Time: 15.1780 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [345/489], Loss: 0.2601, Time: 15.1819 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [350/489], Loss: 0.2569, Time: 15.1857 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [355/489], Loss: 0.2579, Time: 15.1897 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [360/489], Loss: 0.2589, Time: 15.1935 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [365/489], Loss: 0.2581, Time: 15.1974 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [370/489], Loss: 0.2614, Time: 15.2012 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [375/489], Loss: 0.2553, Time: 15.2051 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [380/489], Loss: 0.2603, Time: 15.2090 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [385/489], Loss: 0.2594, Time: 15.2129 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [390/489], Loss: 0.2579, Time: 15.2167 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [395/489], Loss: 0.2595, Time: 15.2206 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [400/489], Loss: 0.2619, Time: 15.2246 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [405/489], Loss: 0.2635, Time: 15.2284 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [410/489], Loss: 0.2581, Time: 15.2323 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [415/489], Loss: 0.2614, Time: 15.2361 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [420/489], Loss: 0.2599, Time: 15.2401 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [425/489], Loss: 0.2602, Time: 15.2439 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [430/489], Loss: 0.2579, Time: 15.2477 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [435/489], Loss: 0.2593, Time: 15.2516 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [440/489], Loss: 0.2601, Time: 15.2554 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [445/489], Loss: 0.2581, Time: 15.2594 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [450/489], Loss: 0.2618, Time: 15.2632 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [455/489], Loss: 0.2569, Time: 15.2671 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [460/489], Loss: 0.2581, Time: 15.2709 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [465/489], Loss: 0.2646, Time: 15.2749 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [470/489], Loss: 0.2608, Time: 15.2788 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [475/489], Loss: 0.2585, Time: 15.2826 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [480/489], Loss: 0.2590, Time: 15.2865 secs, learning rate: 0.0000\n",
      "Epoch [30/30], Step [485/489], Loss: 0.2595, Time: 15.2904 secs, learning rate: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(YTrain)\n",
    "start_time = time.time()\n",
    "model = model.float()\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "loss_tmp = 0\n",
    "norm = 1\n",
    "for i in range(len(YVal)):\n",
    "        # each i is a batch of 128 samples\n",
    "        x, y = XVal[i,:,:,:], YVal[i,:]\n",
    "\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        y = y.unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass val\n",
    "        outputs = model(x.float())\n",
    "        # loss = criterion(nn.Sigmoid()(norm*outputs), nn.Sigmoid()(norm*y.float().T))\n",
    "        # print(\"#1\")\n",
    "        # print(norm*y.float().T)\n",
    "        # loss = criterion(nn.Sigmoid()(norm*outputs), norm*y.float().T)\n",
    "        loss = criterion(norm*outputs, norm*y.float().T)\n",
    "        # print(f\"{loss=}\")\n",
    "        loss_tmp += loss.item()\n",
    "loss_val.append(loss_tmp/len(YVal))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ind = np.arange(int(total_step/batch_size))\n",
    "    random.shuffle(ind)\n",
    "    for i,k in enumerate(ind):\n",
    "        # each i is a batch of 128 samples\n",
    "        x, y = XTrain[k*batch_size:(k+1)*batch_size,:,:,:], YTrain[k*batch_size:(k+1)*batch_size,:]\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x.float())\n",
    "        # loss = criterion(nn.Sigmoid()(norm*y.float()), nn.Sigmoid()(norm*outputs))\n",
    "        # print(\"#2\")\n",
    "        # print(norm*y.float().T)\n",
    "        # loss = criterion(norm*y.float(), nn.Sigmoid()(norm*outputs))\n",
    "        loss = criterion(norm*outputs, norm*y.float().T)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "        if (i + 1) % 5== 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Time: {:.4f} secs, learning rate: {:.4f}'\n",
    "                    .format(epoch + 1, num_epochs, i + 1, int(total_step/batch_size), loss.item(), time.time() - start_time, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    loss_tmp = 0\n",
    "    result = []\n",
    "    for i in range(len(YVal)):\n",
    "        # each i is a batch of 128 samples\n",
    "        x, y = XVal[i,:,:,:], YVal[i,:]\n",
    "\n",
    "        x = x.unsqueeze(0).to(device)\n",
    "        y = y.unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass val\n",
    "        # print(f\"{x.float()=},\")\n",
    "        outputs = model(x.float())\n",
    "        # print(f\"{outputs=},\")\n",
    "        # print(f\"{nn.Sigmoid()(outputs)=},\")\n",
    "        # print(f\"{y.float().T=},\")\n",
    "        # loss = criterion(nn.Sigmoid()(norm*outputs), nn.Sigmoid()(norm*y.float().T))\n",
    "        # print(\"#3\")\n",
    "        # print(f\"{norm*y.float().T=}\")\n",
    "        # print(f\"{nn.Sigmoid()(norm*outputs)=}\")\n",
    "        # print(f\"{outputs=},\")\n",
    "        # loss = criterion(nn.Sigmoid()(norm*outputs), norm*y.float().T)\n",
    "        loss = criterion(norm*outputs, norm*y.float().T)\n",
    "        # print(f\"{loss=},\")\n",
    "        loss_tmp += loss.item()\n",
    "        # result.append(nn.Sigmoid()(outputs[0,0]).item())\n",
    "        result.append(outputs[0,0].item())\n",
    "    loss_val.append(loss_tmp/len(YVal))\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 20.0)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.style.use('seaborn')\n",
    "plt.plot(np.arange(len(loss_train)), loss_train, label = 'Training error')\n",
    "plt.plot(np.arange(0, len(loss_train)+1, int(total_step/batch_size)), loss_val, label = 'Validation error')\n",
    "plt.ylabel('loss', fontsize = 14)\n",
    "plt.xlabel('steps', fontsize = 14)\n",
    "plt.title('Learning curves for a linear regression model', fontsize = 18, y = 1.03)\n",
    "plt.legend()\n",
    "plt.ylim([0,20]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f871165b580>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAG1CAYAAAAC+gv1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABfnUlEQVR4nO3dd3gU9drG8e+m0xJKJIQO0juGIihSlC6iqIAFRAFFBAWsiAU9KnpURI8H8CiI+CJwFPVYEAlKL9KVJiItlIRIS6gJSeb9Y8jGFGCz2c3sZu7Pde21s7OzM89OBubZX3UYhmEgIiIiIk4BVgcgIiIi4muUIImIiIjkoARJREREJAclSCIiIiI5KEESERERyUEJkoiIiEgOSpBEREREcgiyOgB/lJGRweHDhylVqhQOh8PqcERERMQFhmFw6tQpKlasSEDA5cuIlCC54fDhw1SpUsXqMERERMQNBw4coHLlypfdRgmSG0qVKgWYJzg8PNziaERERMQVycnJVKlSxXkfvxwlSG7IrFYLDw9XgiQiIuJnXGkeo0baIiIiIjkoQRIRERHJQQmSiIiISA5qgyQiIraWnp7OhQsXrA5DPCA4OJjAwECP7EsJkoiI2JJhGCQkJHDy5EmrQxEPKl26NBUqVCjwOIVKkERExJYyk6Py5ctTvHhxDfzr5wzD4OzZsyQmJgIQHR1doP0pQRIREdtJT093JkflypWzOhzxkGLFigGQmJhI+fLlC1TdpkbaIiJiO5ltjooXL25xJOJpmX/TgrYrU4IkIiK2pWq1osdTf1MlSCIiIiI5+H2CtGzZMnr16kXFihVxOBx8/fXXV/zM0qVLiYmJISwsjJo1azJ16lTvByoiIiJ+w+8TpDNnztC0aVPef/99l7bfu3cvPXr0oF27dmzatIlnn32WRx99lHnz5nk5UhEREet16NCBUaNGOV9Xr16dSZMmXfYzrhZAXImn9lMY/L4XW/fu3enevbvL20+dOpWqVas6L4b69euzfv163nrrLW6//XYvRSliPcMwiE86T4ZhWB2K+JFiwYGUKxlqdRhyUa9evTh37hyLFi3K9d7q1atp27YtGzZs4JprrnF5n+vWraNEiRKeDJPx48fz9ddfs3nz5mzr4+PjKVOmjEeP5S1+nyDl1+rVq+nSpUu2dV27dmXatGlcuHCB4ODgXJ9JSUkhJSXF+To5OdnrcYp42pj//spXmw5ZHYb4GYcDJvVrRu9mlawORYDBgwfTp08f9u/fT7Vq1bK9N336dJo1a5av5Ajgqquu8mSIl1WhQoVCO1ZB+X0VW34lJCQQFRWVbV1UVBRpaWkcPXo0z89MmDCBiIgI56NKlSqFEaqIR/164CQAwYEOQoMC9NDjio/AAAeGAVsPJVl78RYSwzA4m5pmycNwsWT35ptvpnz58syYMSPb+rNnzzJ37lxuvfVW7rrrLipXrkzx4sVp3Lgxs2fPvuw+c1ax7dq1ixtuuIGwsDAaNGhAbGxsrs88/fTT1KlTh+LFi1OzZk2ef/55Z7f6GTNm8NJLL/Hrr7/icDhwOBzOeHNWsW3ZsoVOnTpRrFgxypUrx4MPPsjp06ed7w8aNIhbb72Vt956i+joaMqVK8cjjzxSKFPD2K4ECXJ3Acy8MC/VNXDs2LGMGTPG+To5OVlJkvidzP9+Pxt6LS2rl7U0FvEPr//wO1OX7sYutbLnLqTT4IUfLTn29pe7UjzkyrfkoKAgBg4cyIwZM3jhhRec963PP/+c1NRUhgwZwuzZs3n66acJDw/n+++/Z8CAAdSsWZPWrVtfcf8ZGRn06dOHyMhI1qxZQ3Jycrb2SplKlSrFjBkzqFixIlu2bGHo0KGUKlWKp556in79+rF161YWLFjgrAqMiIjItY+zZ8/SrVs3rr32WtatW0diYiJDhgxhxIgR2RLAxYsXEx0dzeLFi/nzzz/p168fzZo1Y+jQoVf8PgVhuwSpQoUKJCQkZFuXmJhIUFDQJUdTDQ0NJTRUdfDi31z9hSqSk64c3/LAAw/w5ptvsmTJEjp27AiY1Wt9+vShUqVKPPHEE85tR44cyYIFC/j8889dSpAWLVrEjh072LdvH5UrVwbgtddey9XW97nnnnMuV69enccff5y5c+fy1FNPUaxYMUqWLElQUNBlq9RmzZrFuXPnmDlzprMN1Pvvv0+vXr144403nLU9ZcqU4f333ycwMJB69erRs2dPfvrpJyVIntamTRu+/fbbbOsWLlxIixYt8mx/JFJUZN7kNCyeuCqzUN0uuXWx4EC2v9zVsmO7ql69erRt25bp06fTsWNHdu/ezfLly1m4cCHp6em8/vrrzJ07l0OHDjnb0LraCHvHjh1UrVrVmRyBed/M6YsvvmDSpEn8+eefnD59mrS0NMLDw13+DpnHatq0abbYrrvuOjIyMti5c6czQWrYsGG2KUOio6PZsmVLvo7lDr9vg3T69Gk2b97sbCm/d+9eNm/eTFxcHGBWjw0cONC5/bBhw9i/fz9jxoxhx44dTJ8+nWnTpmXLuEWKMg0cLK6y26XicDgoHhJkySO/oz8PHjyYefPmkZyczMcff0y1atW48cYbefvtt3nnnXd46qmn+Pnnn9m8eTNdu3YlNTXVpf3mVdKcM7Y1a9bQv39/unfvznfffcemTZsYN26cy8f4+7Eu9b3/vj5n4YXD4SAjIyNfx3KH3ydI69evp3nz5jRv3hyAMWPG0Lx5c1544QXA7FKYmSwB1KhRg/nz57NkyRKaNWvGP/7xD9577z118ZciL+v/Pbvd9sRdzhIkVbL5nL59+xIYGMhnn33GJ598wv3334/D4WD58uX07t2be++9l6ZNm1KzZk127drl8n4bNGhAXFwchw8fdq5bvXp1tm1WrlxJtWrVGDduHC1atKB27drs378/2zYhISGkp6df8VibN2/mzJkz2fYdEBBAnTp1XI7ZW/y+iq1Dhw6XbVuRs6U/QPv27dm4caMXoxLxPbrJibvsUsXmT0qWLEm/fv149tlnSUpKYtCgQQDUqlWLefPmsWrVKsqUKcPEiRNJSEigfv36Lu33pptuom7dugwcOJC3336b5ORkxo0bl22bWrVqERcXx5w5c2jZsiXff/89X331VbZtqlev7qzRqVy5MqVKlcrVlveee+7hxRdf5L777mP8+PH89ddfjBw5kgEDBuTqbW4Fvy9BEhHXZN7kVMUmrnKotNGnDR48mBMnTnDTTTdRtWpVAJ5//nmuueYaunbtSocOHahQoQK33nqry/sMCAjgq6++IiUlhVatWjFkyBBeffXVbNv07t2b0aNHM2LECJo1a8aqVat4/vnns21z++23061bNzp27MhVV12V51ADxYsX58cff+T48eO0bNmSO+64gxtvvNHlmTG8zWGoa0u+JScnExERQVJSUr4bpYlY5brXf+bQyXN8Nbwtzav6x0i2Yq23F+7kXz//yX1tqvFS70ZWh+NR58+fZ+/evdSoUYOwsDCrwxEPutzfNj/3b5UgidhMfhuDin3pShE7U4IkYjO66YnLLibTqmYQO1KCJGITqk0Xd+nSETtSgiRiE86BIlWEJC7KvFTUA1LsSAmSiM2oZ5K4Ssm02JkSJBGbUDd/ya/MZFpVbGJHSpBEbELVJOIuXTliR0qQRGxCpQCSX3abrFbk75QgidiEGmlLfmVdKsqQiroOHTowatQol7fft28fDofDOVF8UeT3c7GJSP6okba4Ssm077nSQK/33XdfnnOQXsmXX35JcHCwy9tXqVKF+Ph4IiMj830sf6EEScQm1Ehb8ivzZqwqNt8RHx/vXJ47dy4vvPACO3fudK4rVqxYtu0vXLjgUuJTtmzZfMURGBhIhQoV8vUZf6MqNhHb0F1O3KMEyXdUqFDB+YiIiMDhcDhfnz9/ntKlS/Pf//6XDh06EBYWxv/93/9x7Ngx7rrrLipXrkzx4sVp3Lhxrsljc1axVa9enddee40HHniAUqVKUbVqVf7zn/84389ZxbZkyRIcDgc//fQTLVq0oHjx4rRt2zZb8gbwyiuvUL58eUqVKsWQIUN45plnaNasmbdOV4EoQRKxCZUgibts0wPSMCD1jDUPD2ahTz/9NI8++ig7duyga9eunD9/npiYGL777ju2bt3Kgw8+yIABA/jll18uu5+3336bFi1asGnTJoYPH87DDz/M77//ftnPjBs3jrfffpv169cTFBTEAw884Hxv1qxZvPrqq7zxxhts2LCBqlWrMmXKFI98Z29QFZuITTgbaasNkrjIdr3YLpyF1ypac+xnD0NICY/satSoUfTp0yfbuieeeMK5PHLkSBYsWMDnn39O69atL7mfHj16MHz4cMBMut555x2WLFlCvXr1LvmZV199lfbt2wPwzDPP0LNnT86fP09YWBj/+te/GDx4MPfffz8AL7zwAgsXLuT06dNuf1dvUgmSiM2oBElcpWTaP7Vo0SLb6/T0dF599VWaNGlCuXLlKFmyJAsXLiQuLu6y+2nSpIlzObMqLzEx0eXPREdHAzg/s3PnTlq1apVt+5yvfYlKkERsInOyWt3yJL/sUoBEcHGzJMeqY3tIiRLZS6Lefvtt3nnnHSZNmkTjxo0pUaIEo0aNIjU19fIh5Wjc7XA4yMjIcPkzmY38//6ZnL3wfHkSbSVIIjbhu/8Nia+yXRWbw+Gxai5fsnz5cnr37s29994LmAnLrl27qF+/fqHGUbduXdauXcuAAQOc69avX1+oMeSHqthEbEKNtCW/Mi8V2zTSLqJq1apFbGwsq1atYseOHTz00EMkJCQUehwjR45k2rRpfPLJJ+zatYtXXnmF33777YpjO1lFJUgituOb/xmJ7/HR+5bk0/PPP8/evXvp2rUrxYsX58EHH+TWW28lKSmpUOO455572LNnD0888QTnz5+nb9++DBo0iLVr1xZqHK5yGL5cAeijkpOTiYiIICkpifDwcKvDEXFJk/E/knw+jZ8eb8/VV5W0OhzxAx8u28Or83fQp3klJvZrZnU4HnX+/Hn27t1LjRo1CAsLszoc2+rcuTMVKlTg008/9dg+L/e3zc/9WyVIIjahX0LiLl074glnz55l6tSpdO3alcDAQGbPns2iRYuIjY21OrQ8KUESsYvMNkjWRiF+JKuRtlIkKTiHw8H8+fN55ZVXSElJoW7dusybN4+bbrrJ6tDypARJxCacA0WqYYnkk9Ij8YRixYqxaNEiq8NwmXqxidiM0iNxlZJpsTMlSCI24RwoUvc8cZGzm38RLkJS9WHR46m/qRIkEZvQbUDcVRSvncwRn8+ePWtxJOJpmX/TnCOB55faIInYhHOgSFWyiYuKciPtwMBASpcu7ZwnrHjx4qpS9HOGYXD27FkSExMpXbo0gYGBBdqfEiQRm9E9QFxV1C+VChUqAFxxAlbxL6VLl3b+bQtCCZKITWi6CMmvzBKVonrlOBwOoqOjKV++PBcuXLA6HPGA4ODgApccZVKCJGITmotN3FZUM6SLAgMDPXZTlaJDjbRFbKKI3+PEC5xtkHT1iA0pQRKxC2cJkoqQxDV26OYvcilKkERsRumRuEzJtNiYEiQRm8isJtE9T1ylEiSxMyVIIjahm5y4S22QxI6UIInYhHOyWlWyiYuyBoq0Ng4RKyhBErEZVbGJq5RMi50pQRKxCedktRbHIf4jq5u/iP0oQRKxCedNThmS5JOq2MSOikSCNHnyZGrUqEFYWBgxMTEsX778stvPmjWLpk2bUrx4caKjo7n//vs5duxYIUUrYg3d5CS/snJpXTxiP36fIM2dO5dRo0Yxbtw4Nm3aRLt27ejevTtxcXF5br9ixQoGDhzI4MGD2bZtG59//jnr1q1jyJAhhRy5iDXUrkRcpUbaYmd+nyBNnDiRwYMHM2TIEOrXr8+kSZOoUqUKU6ZMyXP7NWvWUL16dR599FFq1KjB9ddfz0MPPcT69esLOXIRa6iRtrhKybTYmV8nSKmpqWzYsIEuXbpkW9+lSxdWrVqV52fatm3LwYMHmT9/PoZhcOTIEb744gt69ux5yeOkpKSQnJyc7SHiT4y/FQHolicuUyNtsTG/TpCOHj1Keno6UVFR2dZHRUWRkJCQ52fatm3LrFmz6NevHyEhIVSoUIHSpUvzr3/965LHmTBhAhEREc5HlSpVPPo9RLzt71UkmotN8stQHZvYkF8nSJly/odvGMYlbwLbt2/n0Ucf5YUXXmDDhg0sWLCAvXv3MmzYsEvuf+zYsSQlJTkfBw4c8Gj8It6m25u4wznViKVRiFgjyOoACiIyMpLAwMBcpUWJiYm5SpUyTZgwgeuuu44nn3wSgCZNmlCiRAnatWvHK6+8QnR0dK7PhIaGEhoa6vkvIGIBlR+Jq1TaKHbm1yVIISEhxMTEEBsbm219bGwsbdu2zfMzZ8+eJSAg+9cODAwEVIwsRVe2Nki654mLNFmt2JlfJ0gAY8aM4aOPPmL69Ons2LGD0aNHExcX56wyGzt2LAMHDnRu36tXL7788kumTJnCnj17WLlyJY8++iitWrWiYsWKVn0NEa/6+/1NPZMkv5QfiR35dRUbQL9+/Th27Bgvv/wy8fHxNGrUiPnz51OtWjUA4uPjs42JNGjQIE6dOsX777/P448/TunSpenUqRNvvPGGVV9BxOtUAiDuyBoHSReQ2I/fJ0gAw4cPZ/jw4Xm+N2PGjFzrRo4cyciRI70clYjvMP5eBqACJHGRqmPFzvy+ik1E8kc3PXGVqmPFzpQgidiAoQIkcYOmGhE7U4IkYjPqui35ZaiZttiQEiQRG1AJgBSErh+xIyVIIjbw9xIAlR+JqzJLG5UgiR0pQRKxGdWwiat0qYidKUESsYHsjbR125P8URsksSMlSCI2kG0kbeVH4iL1YhM7U4IkYgMaCVnckVnaqKtH7EgJkoiI5EmljWJnSpBEbEBVbOIO56WiIiSxISVIIjagRtpSEGqkLXakBEnEDnR/EzeokbbYmRIkERvINlCkCpDEZWqkLfalBEnEZpQfiauUTIudKUESsYFsbZB015N80jARYkdKkERsIFsvNsuiEH+Tea0oPRI7UoIkYgMqARB3aLJasTMlSCI2oxo2cZUuFbEzJUgiNpB9oEjd9sQ1zm7+1oYhYgklSCI2oCoSKRBdQGJDSpBEbEAjIYs7VIIkdqYEScQOLt7hVLsm+ZE5LY0KkMSOlCCJ2IjyI8kXXTBiY0qQRGwgswBADbQlP7LGQVIRktiPEiQRG8isIlF6JO5QFZvYkRIkERtQCYC4QwNFip0pQRKxEdWwSX7ochE7U4IkYgNZVWy65Ynr1M1f7EwJkogNOG9wyo/EDZrLT+xICZKIDWTe4JQfSX6oxFHsTAmSiA2oAEDc4axi0/UjNqQEScRG1Ehb8kOXi9iZEiQRG1GVieSLs5G2ipDEfpQgidiAobnYpABUxSZ2pARJxAZUAiDucE5Wa3EcIlZQgiRiIypAkvxQiaPYmRIkERvIqmLTHU9c55ysVnVsYkNKkERsIPP2pvRI3KH0SOxICZKIDRhZc42IuMyhuUbExpQgidiA7m/iDuVHYmdFIkGaPHkyNWrUICwsjJiYGJYvX37Z7VNSUhg3bhzVqlUjNDSUq6++munTpxdStCLWUQGS5IeuF7GzIKsDKKi5c+cyatQoJk+ezHXXXccHH3xA9+7d2b59O1WrVs3zM3379uXIkSNMmzaNWrVqkZiYSFpaWiFHLlJ41Ehb3JE11YjKkMR+/D5BmjhxIoMHD2bIkCEATJo0iR9//JEpU6YwYcKEXNsvWLCApUuXsmfPHsqWLQtA9erVCzNkEQtcnKxW+ZG4QemR2JFfV7GlpqayYcMGunTpkm19ly5dWLVqVZ6f+eabb2jRogX//Oc/qVSpEnXq1OGJJ57g3LlzlzxOSkoKycnJ2R4i/kRttMU9FweKVIYkNuTXJUhHjx4lPT2dqKiobOujoqJISEjI8zN79uxhxYoVhIWF8dVXX3H06FGGDx/O8ePHL9kOacKECbz00ksej1+ksOj+Ju5waC42sTG/LkHKlLNdhWEYl2xrkZGRgcPhYNasWbRq1YoePXowceJEZsyYcclSpLFjx5KUlOR8HDhwwOPfQaQwqA2S5IeuFrEzvy5BioyMJDAwMFdpUWJiYq5SpUzR0dFUqlSJiIgI57r69etjGAYHDx6kdu3auT4TGhpKaGioZ4MXKUSqYpOCUBWb2JFflyCFhIQQExNDbGxstvWxsbG0bds2z89cd911HD58mNOnTzvX/fHHHwQEBFC5cmWvxitiFUONtMUNmSWOSpDEjvw6QQIYM2YMH330EdOnT2fHjh2MHj2auLg4hg0bBpjVYwMHDnRuf/fdd1OuXDnuv/9+tm/fzrJly3jyySd54IEHKFasmFVfQ8SrdIMTdyifFjvz6yo2gH79+nHs2DFefvll4uPjadSoEfPnz6datWoAxMfHExcX59y+ZMmSxMbGMnLkSFq0aEG5cuXo27cvr7zyilVfQaQQ6ZYnrlOJo9iZ3ydIAMOHD2f48OF5vjdjxoxc6+rVq5erWk6kKMsaKNLaOMS/OJzd/FUEKfbj91VsInJlzjZIFsch/knpkdiREiQRG1AJkrgja6oRa+MQsYISJBERuSwNFCl2pARJxEYcqmSTfFCJo9iZEiQRG1AVmxSEqtjEjpQgidiAGmmLO5y92CyOQ8QKSpBEbCCrBEkpkrhOjbTFzpQgiYhInpRPi50pQRKxARUAiDuyGvXrChL7UYIkYgOZIyGrREDcoSo2sSMlSCI2kHl/U4Ik+eFsg2RtGCKWUIIkYgMqARB3OCvYdAGJDSlBErERDRQp+aESR7EzJUgitqA2SOIOjYMk9qUEScQGnOMgWRuG+CnVsIkdKUESsYGsRtpKkcR1WQNFKkMS+1GCJCIieVI6LXamBEnEBlTFJu7ILHFU+ZHYkRIkERswlCFJQShDEhtSgiRiA842SJZGIf5GE42InSlBErEBtbEVd6iRttiZEiQRG1EvNskPDSwqdqYEScQGjMyBIi2OQ/yL5mITO1OCJGIHmW20lSGJG1TDJnakBEnEBrIaaStDkvwzVIYkNqQESURE8qQSR7EzJUgiNmCoik3c4BwoUgVIYkNKkERsQFUkUhC6esSOlCCJ2EBWCZKKkMR1zqtFGZLYkBIkERvQSNrijqxu/sqQxH6UIImISJ7U61HsTAmSiA1kThWhGjbJj6ypRqyNQ8QKSpBEbMBZxaYESdyg/EjsSAmSiB1kNtJWlYnkQ+bVoslqxY6UIInYgBrZils0F5vYmBIkERtRFZvkh0ocxc6CPLGT06dP88cff3DmzBnatWvniV2KiAc5x0GyNgzxU6phEzsqUAnSvn376N27N2XKlKFly5Z07NjR+d7KlStp0KABS5YsKWiMIlJAhlppixt0uYiduZ0gxcXFce211zJ//nx69+5NmzZtsjXka926NUePHmX27NkeCVRE3KeBIsUdf79e1FBb7MbtBOnFF1/kxIkTLF26lC+++ILOnTtnez8oKIh27dqxcuXKAgcpIiKFT1PTiJ25nSD9+OOP3HbbbbRt2/aS21StWpVDhw65ewgR8RANFCnuyF6CZFkYIpZwO0E6fvw41atXv+J2KSkp7h7CZZMnT6ZGjRqEhYURExPD8uXLXfrcypUrCQoKolmzZt4NUMRiqmKTglJ+JHbjdoIUFRXFn3/+edlttm7dStWqVd09hEvmzp3LqFGjGDduHJs2baJdu3Z0796duLi4y34uKSmJgQMHcuONN3o1PhFf4OzFpiIkyYe/Xy5qgyR243aC1LlzZ7799lu2bt2a5/vLly/np59+okePHm4H54qJEycyePBghgwZQv369Zk0aRJVqlRhypQpl/3cQw89xN13302bNm28Gp+Ib7hYxWZxFOJf/j4OktIjsRu3E6TnnnuOYsWKcf311/Paa685S5N++OEHnn/+ebp160ZkZCRPPvmkx4LNKTU1lQ0bNtClS5ds67t06cKqVasu+bmPP/6Y3bt38+KLL7p0nJSUFJKTk7M9RESKPGXUYmNuDxRZvXp1fvzxR/r3789zzz2Hw+HAMAxuvvlmDMOgatWqfPHFF0RHR3sy3myOHj1Keno6UVFR2dZHRUWRkJCQ52d27drFM888w/LlywkKcu3rT5gwgZdeeqnA8YpYJauKzdo4xH+phk3spkAjabdu3Zpdu3bx7bff8ssvv3D8+HHCw8Np3bo1vXv3JiQkxFNxXlbOdhWGYeTZ1iI9PZ27776bl156iTp16ri8/7FjxzJmzBjn6+TkZKpUqeJ+wCKFLKuRtjIkcV22NkiqZBObKfBUI0FBQdx2223cdtttnognXyIjIwkMDMxVWpSYmJirVAng1KlTrF+/nk2bNjFixAgAMjIyMAyDoKAgFi5cSKdOnXJ9LjQ0lNDQUO98CZFCYKgbm7hB3fzFztxug9SpUydmzpx52W1mz56dZ8LhKSEhIcTExBAbG5ttfWxsbJ7jM4WHh7NlyxY2b97sfAwbNoy6deuyefNmWrdu7bVYRUT8jXo9ip25XYK0ZMkSOnTocNlt4uLiWLp0qbuHcMmYMWMYMGAALVq0oE2bNvznP/8hLi6OYcOGAWb12KFDh5g5cyYBAQE0atQo2+fLly9PWFhYrvUiRYmhXmziBl0vYmcFrmK7nDNnzhAcHOzNQ9CvXz+OHTvGyy+/THx8PI0aNWL+/PlUq1YNgPj4+CuOiSRS1KmRthSUqtjEbvKVIOVMNE6ePJln8pGens7Bgwf5/PPPXRptu6CGDx/O8OHD83xvxowZl/3s+PHjGT9+vOeDEvEhaqQt7lAjbbGzfCVI1atXd9ZJOxwO3n33Xd59991Lbm8YBm+++WbBIhSRAtNcbOKObANFKj8Sm8lXgjRw4EDneEczZ86kadOmec5jFhgYSNmyZenUqRPdunXzVKwiIlKIlFCLneUrQfp7ddXSpUu5//77efTRRz0dk4h4iW544i4VIInduN1Ie+/evZ6MQ0S8yNlIW22QxE2arFbsxu1xkETEfzi7+Ss/knzI3khbxF4K1M3/1KlTvP/++yxatIjDhw+TkpKSaxuHw8Hu3bsLchgREbGAShzFztxOkP766y/atm3L7t27CQ8PJzk5mYiICFJTUzl37hwAFStW9Po4SCJyZaodEXdkK0HSNSQ243YV2/jx49m9ezczZ87kxIkTAIwePZozZ87wyy+/0KpVK6pXr862bds8FqyIuCdroEg/KBE4sh3WfgjpaVZHIn+nBElsxu0Eaf78+dx4443ce++9uf7TbdmyJT/88AP79u3TIIwiPsBv5qo9/Rd8eivMfwJWv291NLaXbbJaZUhiM24nSPHx8TRv3tz5OjAw0Fm1BlCmTBm6d+/O559/XrAIRaTA/GKgyIQtMPU6OH3EfL3kdTh30tKQ7O7vP35VxSZ243aCFBERwYULF5yvy5Qpw8GDB7NtEx4ezpEjR9yPTkTsIW4N/N8dZnIUUdVcl3YOtn5hbVw258v5tIi3uZ0g1axZk3379jlfN2/enNjYWI4fPw7AuXPn+Pbbb6latWqBgxSRgvHpKrYD62BmbzidAOUbwLDl0HWC+d4PT8OepdbGZ1cXzuM4d8z5UgVIYjduJ0hdunThp59+4uzZswA89NBDJCYm0rRpU+68804aNWrE7t27GTRokKdiFRF3+XIj7R+egrTzUKszDF4IxUpDs7ugdDXISINvRqjBdmE7lQCTGuF482r+GfQBYGigSLEdtxOkYcOG8eGHHzoTpD59+vDmm29y+vRp5s2bR0JCAmPGjOHJJ5/0WLAi4h7nQJEWx5HL8b1weCM4AuDWKRBaylxfrIxZkhRWGk7GwdZ5SpIKy8aZ8HZdOPMXAH2DlnJv4CKVIIntuJ0gRUdH069fPyIjI53rHn/8cY4ePUp8fDynT5/mzTffJDAw0COBikjB+VwB0m9zzefq7aDkVdnfC4uAtiPM5a8ehPdjIOVU4cZnN4YBy97Met24LwBjgz4jICnOoqBErOF2grRs2TLi4nL/gwkMDCQqKgqHw8HBgwdZtmxZgQIUkYLzydqRs8dh9b/N5WsG5r1NmxFQ+mI7xhP7YMMnhRKabR39wyyxAxjyM9zyLxKN0pRwpFD2wxZmmzCV5IlNuJ0gdezYkRkzZlx2m1mzZtGxY0d3DyEiHpKVH/lQEdLPr0BKMkQ1hoZ98t4muBgMmm+2TwJY+4GPZntFxI5vzeerb4TKMRAcxsgLI/k1o6a5/pep8L9H9DcQW3A7QXKlwV5GRoZvNgoVsZmskbStjcPp0EZYP91c7v46BFzmv6LSVaDfpxBc3CzdWDkJMjIKJUxbuXAOfvnAXG50u3P1OqM+vVNfIenmaeAIhN/mwEaV5EnR53aC5Ipdu3YRERHhzUOIiAt8qpF2RgZ8/zhgQJN+UP36K38muBjUvliKtGg8rHzHmxHaT0YGfDUMziRCeGVo0tf5VuaP3PN1boYbXzBXrngHMtKtiFSk0ORrstoHHngg2+uvv/4621hImdLT053tj7p161agAEWkiFk/zey5FhoOnf/h+udaDoHt/zOXV/0LWj0EoSW9E6PdrPk3bP8aAoLh1n9DYNYk49mS6lYPmiV4J/bBr3Og+T2FG6dIIcpXgvT3NkcOh4PNmzezefPmPLd1OBy0bNmSd97RLz0Rq/lMFduOb82GvgAdn4VSUa5/tsYNMPYQfNAOju+BDTOyermJ+377Lyx83lzu/jrU7JDt7cxrxjCAkOJw3ShY9CJ8+ygY6ZduYC/i5/KVIO3duxcw2x/VrFmTUaNG8dhjj+XaLjAwkDJlylCiRAnPRCkiBZI1krZFGVJ6mjltyP9GmDfVpneZJUD5FVoSrh8N34yEVe+ZN+ewcM/Hawdnj8OCZ7KGWmg5FFoMvuTmzslq2zwCv38PB9fCd2OgTjcoWb4QAhYpXPlKkKpVq+Zc/vjjj2nevHm2dWAmT3/++SeAEiQRX2HlZLWGATNvgf0rzdcNboVb3r98w+zLadIflr9tVvMsGg83T/RQoDaye7FZknd0p/m6dhfo/s88LxAzqTayOq4FBsMDC2DKdfDXDlj9PnR6Plu1nEhR4HYj7dKlSzNp0iROnDjhXLdv3z4aN25MvXr1qFatGvfccw8Z6m0iYjlnCZIVCdLv32UlR9cMhD4fQmC+fptlFxQCvd4zlzd9apaEiGtSz8D6j2HWHWZyFBoOvd6F/rMvnbBmVrH9fV1AINzwhLm88l14vRoc3eXNyEUKndsJ0tSpU1m3bh1lypRxrhs1ahTbt2+nY8eONGnShDlz5vDxxx97JFAR8TPpF8wSnnlDzNftHodb/mUmOAVVsz1UaALpqWaDbbmygxvgnYbw3ShzjrsGvWHEOogZdNmE9ZI5daPbzfZIABfOwIpJHg1XxGpuJ0jbtm2jVatWztdJSUnMnz+ffv36sWjRItauXUv9+vWZNm2aRwIVEfc5G2kXRhukI9vhx3Gw+DWzO3jaebPhb7vHPXucVg+azysmwu/zPbvvouLAWkjcAbsWwWd94dwJCAyBTs/BHR9DqQou7yrX2HcOB3R+CQZ+Y77+bS4k/u7B4EWs5XY5919//UV0dLTz9YoVK0hLS+Ouu+4CIDg4mM6dOzNr1qyCRykiBWJkZUjelZYKM3ub4+lkum6UOX5OgIfnZWx+L+xfBb9+Zla11evh2f37u82fwdcPZ18X3RQGfZ81KbALsvViy0vN9mYbpl0LzdLCBxZo+AUpEtwuQQoPD+fYsWPO10uWLCEgIIB27do51wUHB3PmzJmCRSgiBZbVi82LUs/AnLuzJ0dRjbyTHIF5587s5v/nT3A+2fPH8FcrJplTgmQKKWWWuA2an6/kCFwsdez1LhSPhCNb4Ien8heriI9yO0GqV68e3377LcePHycpKYk5c+ZwzTXXZGuTtH//fqKi8jHOiYh4lVem/jEM+GIwvFYR/oyFwFAIK21OMnvnJ95JjjKVbwCRdSE9BX6d7b3j+Ivje+A/Hc1xiowMuOY+c+yoZ+Kgx5tuley4dMmEVzSng8EBm2fBXk1SLv7P7QTp0Ucf5fDhw1SqVIkqVapw+PBhhg0b5nw/PT2dFStW0LRpU48EKiLu89rcooZhNpLe+oX5OjAUBv4PntkPo7ZAZC0vHfgihwNaX2yLtGayvae/SD0DXzxgjlIO0PlluOU9Mylyd0gFskodr3gNVWsLMfeZy18MNodhEPFjbv+ruf322/n3v/9Nw4YNqVOnDhMmTMg2FclPP/3E2bNnNdWIiA/wShXbnz/BW7Uh9uIozDH3w/DVUK2NJ49yZU3vhmJlzRvy798V7rF9xelEmNETDm8yS+8eXgXX5R7EtyAMXMiyu74G5Rua1awze5tJm4ifKsBgJPDwww/z8MMP5/lely5dso2RJCLWMTw1UGR6GjgC4NRhmHMPpJ0z1ze+E25+x5qBlkKKQ8vBsOxNWPW+2X3dTgwD/nufmRwVKwt3/xeiGnps95nVsi6VQoaUgHvnwbTOZsK6/O2sCW5F/EyBEiQR8S/Z0pf0NLPtztI3oHIrCI82f/3/OhvO/AVHtkGx0mZD63Mn4cAas21JWGnzc2nnoOI10Oc/ULamtRO9tRxqDlh4cC3E/QJVW1sXS2H7/TuIWwVBxWDwQois7dHdO6vYXP1AeDR0ex3m3mM2Fq/THaq09GhMIoVBCZKI3RzdZY5PtHWeOWBgRlr+Pp/ZSy0w1Cw18vAN2S2loqBJX9j0f7DuI/skSH/thK8uluK3fsg7fwt38t56PaHRHWbbtO9Hw4NLvdtYX8QL3G+5JyJ+wzDAQQZtTn4H77cwexqlnb98clSmRvbXlVvC/Qug3yzo8CwMWw4Vm3k17ny5ZpD5/Pv3kHrW0lAKRXoafDkUUk9Bteug47NePVyugSIvx+Ewe82FRUDCFvN6E/EzKkES8ZaMdDifBMXLWhdDcjx8M4KbTqbTMWQntRIOZ3+/XC1zfJz0VKjRHvatgIhKUKGxmSDF/wqf3GL2Turyj6zP1b+5cL+HKyq3MIcWOBkHO76Bpv2tjsi7Ns8y/z5hpeH2aRAU6pXD5LuKLVPxstD+afjxWfjpH+YkxWHhng1OxIuUIIm463yy2f4js01OyQpQppo5EN+hjTBvsNmWp0Z78wZ2eJP5y/rqTt6rbtg6D04dgZZDzLZEi16EcyeoARAAaQQR1Pg2uOV9CA7L/fnoJtlfV2wGY+O8E6unORzmZLg/vwJL/2lW8RRkUlxflp5mfkeADs+Y7X68JF+NtHNqORTWTYPju80pYW4a79HYRLypiP7vIeIlJ/bBzh9gy+dwaINrn9m7FN7KMR5QZF24cNacET1mUNb6fSsg/jdzXXAxc+6s4mXNRtKh4XmPZ3PhHOCAv3aY489gwLJ/mp8FCArj96u6MjcunAuN+vPK7dfn91v7j9bDYM0U84b862dmwlQU7VoIyQeheDlzeAUvKlDb+6AQ6PIKzLkLVv8bmvSD8vU9FpuINylBEnHFqQSzK/WBNa5/pn4vs/rq0z6QcSH7e0d3ms/fPmY+QsPN6qGjf5jVXT+Ozdo2rDScPwlBYVDrJnPbCo2gxFVmKdHun3MfOzM5ajEYur7GklWH+Hjv79wemL9pJvxOaCm4fgwsHAdL3jBvyF6qerKMYZiDYgI0uzvvkkAPysqP3BxttG53qN0Vdv0I342G+3+wtsejiIuUIIlczum/YOUk89dv5g2i4jXmFBcVGpuz1GdcMBs7h4ZDsTJm2yMjw+xZBTDwazi4zkxuzvwFe5ZAjRtgwydmWxmAlGQ4sjXvGM6fNJ/TzmcNhPjrJeINCIK75sCZo1C6itl41+FwVo/Y4r7UcjCsft8sYVn/MVw77Mqf8Se7YmHfcggMMauwConbo7E7HGZvx3/FQNxq+ONHqKsBhMX3KUESycu5EzD/Kdjy36x1jkBzGo0a7S79ubxUv958ZLq6k/lco71ZTbdnKRjp5lhCZ/6C+reYCc6en6FCU7MdTUgp+G2O2e7pVLzZvTv1DAQGQ8Xm0P0Ns0QpIw1Kls8VQuYoyHbIjwguBjc8Cd+PgTX/NkvxCjDVhs9ZOcl8bv2Q2ebNy5xtkAqyk4hK5pQwK981S0wrr4IS5TwSn4i3FIkEafLkybz55pvEx8fTsGFDJk2aRLt2ed/EvvzyS6ZMmcLmzZtJSUmhYcOGjB8/nq5duxZy1OKzDMOsCtj2Vda6DmOhejuofp3njhMYDFWvNR85lakGlWOyryvAYHu2KkECaHoXLBpv9mjbv8IssSsKjmyD/SvNksJrhxfKIV2ei+1K2j9ttt87+gds/ATajSloaCJe5fc/q+bOncuoUaMYN24cmzZtol27dnTv3p24uLx73ixbtozOnTszf/58NmzYQMeOHenVqxebNm0q5MjFZ236NHty1PE5s6eQJ5MjizjsUYZkTj/SqI+5vOR1yMiwNh5P+X2++Vy7K4RXLJRDeiypDikB114c1HLnDx7aqYj3+H2CNHHiRAYPHsyQIUOoX78+kyZNokqVKkyZMiXP7SdNmsRTTz1Fy5YtqV27Nq+99hq1a9fm22+/LeTIxSetmQrfPGoud3oenkuE9k9aG5O45/rREFzCLHHZ+b3V0XjGnsXmc60bC/GgmVVsBS1CAupcbHt0cC1snFnw/Yl4kV8nSKmpqWzYsIEuXbpkW9+lSxdWrVrl0j4yMjI4deoUZcteejC/lJQUkpOTsz2kCNo8GxY8DRjQ4gHzBltEekB5bLJaf1KmetZgkXH56H3oq86dhANrzeWaHQr98AWuYgOz1CtzMuFvR0Hy4ctuLmIlv06Qjh49Snp6OlFRUdnWR0VFkZCQ4NI+3n77bc6cOUPfvn0vuc2ECROIiIhwPqpUqVKguMXHGIZZpfbNSPN125HQc2KRmjvKdm2QMlW62I7r8GZLw/CIdR+aPSavqm826C8kmdeMRxIkgDs+huimZseEDZ94aKcinufXCVImR47/9Q3DyLUuL7Nnz2b8+PHMnTuX8uVz9/zJNHbsWJKSkpyPAwcOFDhm8REZGTD3Xvh8kHnzadAbbnq5CGcSRfV7XULF5uZz/Gb/boeUnga/fGAutxtTqNenx48UEAhtL1Zjr3zXbHgu4oP8OkGKjIwkMDAwV2lRYmJirlKlnObOncvgwYP573//y0033XTZbUNDQwkPD8/2kCJi5aSssYUa9IZbpxatLuEXeerHv9+JrAPBxSH1NBz4xepo3LdvuTkERLGy0PC2Qj20swTJk1dRwz5w9Y2Qdg6+fthMAEV8jF/fCUJCQoiJiSE2Njbb+tjYWNq2bXvJz82ePZtBgwbx2Wef0bNnT2+HKb7qwDpz3i4w5ybrO9Ps/VQE2baKLTAIGt1uLq+YaG0s7srIgLUfmssNbjGHh7CAx6rYwPwRcttUCIswJ9zdpAbb4nv8OkECGDNmDB999BHTp09nx44djB49mri4OIYNM0fPHTt2LAMHZs3HNHv2bAYOHMjbb7/NtddeS0JCAgkJCSQlJVn1FcQKZ47CvAfMdhCNbofm91odkVfZaqDInK4fbT7/uQhSTlkbizs2fWr2wgsIgmvuK/TDe21oiJLlzfHFAJa9BRfOe+c4Im7y+wSpX79+TJo0iZdffplmzZqxbNky5s+fT7Vq5giz8fHx2cZE+uCDD0hLS+ORRx4hOjra+Xjssces+gpS2BJ3wCe9zEEEy1Q3p0Eo4kUrti1BAih3NURUNad/cXWCYV+y/X/m8w1PQaVrCv3wHm+k/Xcx90N4JUg+ZDZCF/EhRWIk7eHDhzN8eN6jys6YMSPb6yVLlng/IPFdKadg1p2QdACKR8I9X5jF/DZhm4Eic6rSEpLizGpVC7rIu+3COXMcJ8jqHl/IvHrFBIdBu8fNaWEWPgeOAGjziDePKOIyvy9BEnHZqQSY1tVMjiKqwsOrILK21VEVCts20s5UpbX57G8NtfcuMycpDq8EV9W1JISsudi8dBU1HwC1OpvLP78K59XcQXyDEiSxhwvnzd4yidvMEqM+H0Cpy/d0LFLsOFDk31W+OI/dwXX+1d1/6zzzud7Nlv/xvFLFBhAUAvd8DlfVgwtn4L8D/bOtmBQ5SpCk6DtzFD66EXb/DAHBcP8PUO3SvRyLosx7m13zIyo0hqBicP4kHPvT6mhck3oWdlwcgqLxHdbGgpdLIR0O6HSxim3PEvjkFkjY4s0jilyREiQp2jIyzF+kR7ZCiaug7ycQ1dDqqCzjygCqRVJgcFYDZ3+pZvtjgVmiUrpqVgmYBQrtkqnfCwZ8ZSZJhzfCzFshLaWQDi6SmxIkKbqSDsFHncxGrsElYNB8qGfPca+8Vj3iTzLbIe1dam0crsqsXmt0u6XVa1m92ArhIqrZAe6aay6fPQrrP/b+MUUuQQmSFE0XzptTiBzeZP4i7fk2XFXH6qgs47UGtv6k9sVJrXcthPQL1sZyJSfjYOcP5nLjO62N5aJCu4LqdMkau2rB0+Yk0iIWUIIkRY9hwHejzWL6YmXM3mrN7rI6KkvZehykTFVamVN1nE+CuDVWR3N5a/9jDmJa4wbLq4Qzh4Yo1FLI1sPMRtsAsc9DyulCPLiISQmSFC1njsK8IfDrZ2bJ0R0fQ/n6VkdluaxG2jbOkAICoU43czmzdMYXGQZsuzg4ZKsHrY2FvyfVhZghlaoAw1ZAmRrmHHSfD4LTfxXe8UVQgmQPaanw/RMwoQp80B5iXzAHoCtq4n+Dmb1h6xfm687/gKs7WhuTj7F1CRJA3e7m8875vtsw68g2c1DLoGLmhK4Ws+ySCQw2e7YB/BkL026Cw5vht88hI92qqMRGlCDZwfzHzWH8U5IhfjOsfBderZA1UWtRsGkW/KeD2VutWBm490toO8LqqHyGr+YChe7qThAYAif2wtFdVkeTt18vtrm5uqNPTJ7sHCjSimuoYR9zDCiAE/vgP+3hyyHmqNtKksTLlCAVdX/9ARtnZjVUvvFFcxlg2ZvmmCP+7sQ+s82RkQ61u5rjHNWy/pe3L7H1ZLV/F1oSql5rLu9ZbG0seTmdCOummcsx91sbSw6W5NgBAdB/FvT/LOv/LYA1k2F2f/8a9FP8jhKkom7TTPO5dhdoOQTajYFhKyGqsbl+7sCsyTD9UWaD7PQUs0Hr3XPV5igvaqSdpebFaldf/HGw8l1IOweVWkDtzlZHA2Ql1ZaWQtbraf6/1etd6PyyOeDrroXwdl1zOhYRL1CCVJTt+A5WTzaXm92dtT6qAQxeCFXbQkqSOZDiuo+sibEgTsbBu03MEbIDQ6D7m8oALsHZSFvnJ6td2t7lvtXd/3xSVulRh7G+cy0X5jhIlxPVAGIGwXWPQc+3zHVnEmFWXzi229LQpGhSglRUHd8L/xtuVjs1ugPq9cr+fkhxGPAltH7YfP3DM3Bif+HH6a6MDLPk6GSc+brnRChfz9qY/ICP3HKtVaGp2d0/9RQc2mB1NFl2LjBLjyLr+FQVsU9eMzGD4JG1Zklb2jlYMdHqiKQIUoJUFJ3+C6Z3NX+RVrwGbptq1uXnFFwMuk2AGu0h4wL89JJ/tOZNTzMHkPtzkVly9PBquGaA1VH5NMt//fuSgACo2d5c3u1D7ZAyq7ob3Oo7pUd/43NX0FV1odvr5vLmz8zebSIepASpKFr9Ppw+AmWvNhs4BgZfeluHI6vh9tZ5sOnTwovTHeeT4bO+5kB6AL3eM4ve5bKMrIGQBP7WDslHEqTzyWbCD9DwVktDycnSXmxXUqWlWZpkZJi92z69DbZ84aPBir9RglTUnDuZ1Y6h66sQXvHKn6kckzXeyNI3zRIaX3TuJHzcHXb/BMHFzUEgbT5Ctqs0UGQONTuYzwfXmyWtVtu10OxoUK4WlPethN/ZSNv3ypBMPd6G60YBDrM94rzB5lAJCVv8q9mA+BwlSEXN6vfNthXlG5pd3l3V+mEoXs4coO7TW31vaP+MDFjwjDnOUckouH8+NOpjdVR+xwdrbqxRphqUrWm20du3wupoYNtX5nOD3j73R/KxcHILDILOL8FdsyEswlz39cMw9XqzE8f0bpC4w9oYxS8pQSoq0tPgx3Hm2EZgdufPq93RpYQUh66vmW169i03G3j7kq8fzhpA747pULG5tfH4GdU45CGzms3qdkgpp7Oq1xrcamkoeXGWOvr6NVS3Ozy5O6t0MFPcapjWFY7+aUlY4r+UIBUVv0wxS48Arn0EGt2e/3007Q/3fWuOMbL9f7BvpWdjdIdhmEMQ/DYHcJjjoFS/3uqo/I4GiszD1T7SDun37yDtvDnvWIXG1sZyGb6eHwFme8t7voDbPoDhv8CYHVApxhzO5P0Y+GdNjZskLlOCVBRcOA+r/mUud/4HdHvN/XLxqtdm9Qj7+mHrf3Wtfh++f9xcbvGA2SBT8s3QQJG5VW9ndk449iecPGBNDGkpsPg1c7nZ3T75B8oMyW9KIQODzR975euZbTD7zzarUwHOHoPZd5uTWotcgRKkomDtf8xea+GVofWwgu+v/TNQuhqc3A/TOsPxPQXfZ36lnjEbmy96yXwdc79ZBSgFokbaf1OsNFRuaS7v+NaaGHbFmv/OSkZBm0esicFFPttI+0pKRcH9C6DjcxAYarbR3PyZ1VGJH1CC5O8unM8aJK3jsxAUUvB9loqCIYsguimcOw5LXi/4PvPDMOCzfvD9GHN8pga3ws3vQHBY4cZRBPlgAYW1Gt9pPv9q0Q3zjwXmc8PbIKSENTFcQZEYfb1UFLR/Enr803z9yweQdMjamMTnKUHydzvnw7kTZulR0/6e22/J8nDLxWq7LZ8X7sznf/xoNhQHs0qtz390Zy8gDRR5CY1uNzsmJGwp3GsczI4Vu2LN5dpdCvfYbigSl1Cj2yGiKiQfNCe79aWpZsTnKEHyd+unm89N+0NAoGf3Hd0U6vY0B2H7cRxcOOfZ/efl6C746iFzue1Is1F2UKj3j1vEaZzISyheFqq1NZczk5XCsvETOJ1gDq/hwx0PssZBKgJCS8Gg76BYGUj4DZZrihK5NCVI/uzPRWZJS0AwXDPQO8fo8DTggF0/wpu1zOH8vfVTMuUUzLoDzp80e550eNY7x7GhrJG0lSLlUquz+bxrYeEdMyMDVk4yl9s/49M/ArIaaReJFMkcA6vHxclul/0T4n+1Nh7xWUqQ/JVhwJI3zOVWQ81/9N4Q3RT6zjR/5aaeNofz/+gms1rP036ZCif2QUQVuGuuOTaTeJTSozzUuTig6r7lcOZY4RzzwBpzouXQcJ+fR7BI5tSNbof6t0BGGnw1zDdGUxefowTJX+1bDgfXmr0yrnvMu8dqcAs8sQua3WO+PrQevhtt/qfiqXYbO76DpRcbUN74ApS8yjP7FeBv4yAVxZtdQUXWNscfykiD7V8VzjF/m2s+N7jFnDTah2X2fCwi5UcmhwN6ToTikZC4HT7pVXjJsfgNJUj+KCMdfnjGXL5mAJSq4P1jBgTCrZNhwNfm2DHbvoLXq8L7LWDthwXbd+oZ+PZRSE+FOt3dG+RSLquo1I54TeO+5vOO77x/rPQL5kCsAI3u8P7xPKWoXUMlr4IBX5lJUvyvZum4/qHI3yhB8kdxayBxG4RGQMdxhXvsqzvCnZ9A0N9+9c5/EjbNcn9iyLUfmgO4lakB/T71fGNz0WS1V5I5qvbBdd6frHnPErOKukR5qHGDd4/lAc42SEUuQwKim5iNtgNDzIluC7Mdmvg8JUj+aPfP5nOdLmYvnMLW4BYYttwctbthH8Aw5257twl8MxLSUl3fV8ppWPWeudz+KXMUXPE4jaR9BeUbmBOdpp42ezd509YvzecGvf3ix4CzF1sRzI8AKF8/a4Ddz/pm/X3E9pQg+aPMiS2vvtG6GCJrw3WPwu3ToNWDWes3zoR/1oB/lIelb1768ymnzarCz+/LKj3KrOYQr1F+dAkBgVDlWnN53wrvHefCeXPuNfCfqmQ7ZNU3PGFWtQF8cT8cWGttPOITlCD5m4MbIH4zOALh6k5WRwMBAdDjTXh0M/T/zOyVk3oa0lNg8Sswq29We4tM6z+GCZXg5bJmshdcHG6bCoFBlnwFeyiqP/89qNbFHxy/f++9Y+z4BlKSIbwSVGntveN4UJEvQQKz9LD/Z+YzmJ1Qzh63NiaxnO5I/mblO+Zzk37m8Pm+omwN8zHq4ojEv82FdR+a4yft+tHcJqQUVLoG9i7N/tkbXzAnyRWvURWbC+r3gh+eggO/QHI8hEd7/hi/fGA+x9xv/rjwI0U5PwKgamsY/gtMvQ6ObIVPboGB/4MS5ayOTCziX/9C7S75MPw+31y+7lFrY7mUYqWhSkvo+Rb0+z8oVTHrvdRTWclRQBA0HwCtHoKWQywJ1U6yEiRlSJcUXhEqtwKMrGowTzq4wRwiIzDEnELHTxS5gSIvJzwa7vvObEB/ZAvMvQdO/2V1VGIRlSD5k42fgpEO1a4zGxb6uvq9zDmm1v4HSlczB7Pcu9z8H7fZ3eZw/yK+pMEt5vhi2/9nDsDqSesuDofRsI9fjfNlu5Q6qoHZ/f/DThC3GiZfC8NWeKdEUXyaSpD8RXqaOXcTQIsHrI0lP4JCzTnVGtxijsrddgS0eUTJUSHTQJEuqn+L+bx/JSRs9dx+M9Jh5w/msh+VHkFWqaMNyo+yVGhkloAXLwdnj8LH3TQliQ0pQfIXuxZC8iHzH2z9XlZHI37GDrUjHlGmmpkkGRnww9Oe22/8r+Ycg6HhULml5/ZbiGx3DdXpYrZBCilpToH034FmL0SxDSVI/mL9dPO5+b0+PbGl+CYNFJkP3SYADti/ApIOemafmWOXVW/nd701s64Yu2VImFPQDFsBJa4yk6TpXeEPDSZpF0qQ/EHSoayxj/yseF58g3qx5UNEZajaxlze8W3B95eRbo4PBlkT4/qRrEba1sZhmbI14NYp5tAq8Zthdn+zl6MUeUqQ/MEfPwCG2cOmbE2roxE/pvzIRQ0utkXKOYaXO7Z/DSf3m+3uGt9Z8P0VMpU6ArU7mw23wewokznZsBRpRSJBmjx5MjVq1CAsLIyYmBiWL19+2e2XLl1KTEwMYWFh1KxZk6lTpxZSpG7KbNxZr4e1cYjfUiPtfMps5xe3Bk4dcX8/aanw08vmcuuHIaR4wWMrbM652GyuZnu45V/m8oYZ3p+zTyzn9wnS3LlzGTVqFOPGjWPTpk20a9eO7t27ExcXl+f2e/fupUePHrRr145Nmzbx7LPP8uijjzJv3rxCjtxFyfHm5JYAdXtaGor4Mdvf3fIpojJUigEMc/Rrd+360Wy7UuIqs/emH7NtFdvfNbodipWFE3th6xdWRyNe5l+tBfMwceJEBg8ezJAh5mCDkyZN4scff2TKlClMmDAh1/ZTp06latWqTJo0CYD69euzfv163nrrLW6/3dq5kQ6dPEfcsbPZ1lX+dQpVMtJILt+CbcnlIPmYRdGJP/vrdAqg6pJ8adgHDm0wq1PcHRNp82zzudndEFrSc7EVoswrZueRU5TdHWJpLL6gUp1BVP11IunfjuaPpBBOVmpvdUhFVqXSxahazrpSV79OkFJTU9mwYQPPPPNMtvVdunRh1apVeX5m9erVdOnSJdu6rl27Mm3aNC5cuEBwcO7Z5FNSUkhJSXG+Tk5O9kD0uf1v8yH+uWCn83VJzrI8dBo4YNzBa/n2wzVeOa7YR0CAEiSXNb4DYp+Hg+vgr51wVd38ff7M0axpdpre7fn4CkngxWvmvZ928d5PuyyOxnrBNGNacGNuYAu1fxpM99TX2WVUtjqsIml4h6t5qls9y47v1wnS0aNHSU9PJyoq+5xkUVFRJCQk5PmZhISEPLdPS0vj6NGjREfnHi11woQJvPTSS54L/BLKFA+hVvmsX5n3pHxPmZTTxAVUYme5G6nlCPR6DFJ0lSkezE31y1sdhv8oVQHqdIOd82HFJLhtSv4+v+ULyEiDis2hvHX/yRfUPa2rcfxMKmkZqmPLNMF4kVJnX6J5+hZGl1zExGIjrA6pSCpbwtoSS79OkDLlnF/KMIzLzjmV1/Z5rc80duxYxowZ43ydnJxMlSpV3A33ku5qVZW7WlU1X5xPgkkDAKh628ssbNzJ48cTkSu44QkzQfptLnR9FYqXde1zaSmw9uLEtH5cegTQs0k0PZtomo1c9peEj7vRI20RPW7qC038r4eiXJ5fN9KOjIwkMDAwV2lRYmJirlKiTBUqVMhz+6CgIMqVy3vW5tDQUMLDw7M9vO73780kKbIONLzN+8cTkdwqxUBkXbNr995lrn9u/XQ4vgdKRkGzu7wXn1in6rXmuHRGBnw/xqxSlSLFrxOkkJAQYmJiiI2NzbY+NjaWtm3b5vmZNm3a5Np+4cKFtGjRIs/2R5bZc3HW+3o3Q4Cq1kQsc3VH83nPYtc/kzlOzg1PQmgpz8ck1nM4oOc75hyTKcmw4h2rIxIP8+sECWDMmDF89NFHTJ8+nR07djB69Gji4uIYNmwYYFaPDRw40Ln9sGHD2L9/P2PGjGHHjh1Mnz6dadOm8cQTT1j1FXIzjKyu/TXVQ0LEUldfrN7e8S2knLry9gfWweFN4AhQ6W9RFxAAHZ41lzfP0lxtRYzft0Hq168fx44d4+WXXyY+Pp5GjRoxf/58qlWrBkB8fHy2MZFq1KjB/PnzGT16NP/+97+pWLEi7733nuVd/LOJ3wynEyAoDKq0tjoaEXu7uhOUvRqO74ZfppqlQpdy+i+YdYe5XLMjlIgsnBjFOrU7Q0QVSDpgJkktB1sdkXiIwzA0/Fd+JScnExERQVJSknfaI/04Dla/b/76vHOG5/cvIvnz23/hy6FQojyM3nrpCaNXvAOLxkOZGnD/DxCuxs22sGYqLHjaHBB05EYIK4R2quKW/Ny//b6KrcgxDNj2tbnc6A5LQxGRixreBqUqwplE+HV23tvE/war/20u3/CEkiM7aTkYytWCM3/B8retjkY8RAmSr0ncDskHIagY1LrJ6mhEBCAwGNpeHOvmu9GwcWb29w+shendzBvkVfXNUbjFPgKDocsr5vKayeb0MuL3lCD5ml0LzecaN0BwmLWxiEiWlkPhqnpmt+5vRsL/HoGFz5uT2ca+ABfOQI328MAC/5yUVgqmTjfz75+eCnPugZMHrI5ICkgJki9JjodVF2eLrtPV2lhEJLugEHjgR2jxgPl60//Bqvfg7ToQtxoCguG2D6BYaUvDFIs4HND9DQiNgCNb4b8DIf2C1VFJAShB8iWxz8PZYxDVGJrdY3U0IpJTsdLQcyI0zWPwx+seU7sjuytfHx5aAmERcHgjLH3D6oikANSLzQ1e68V25pg5Imun5yCytuf2KyKelZEB8ZvM3mp7l0J4ZajS0uqoxFds/RK+uN8cC+uRdRBZy+qI5KL83L+VILnB6938RUTEv82602xT2nII9FTPNl+hbv4iIiJWanOx1+PGmZCwxdpYxC1KkERERDytxg1Qt6fZq+2HZ6yORtygBElERMTTHA7o8U9wBML+FZC4w+qIJJ+UIImIiHhDRGWo291cVo82v6MESURExFvaP2X2Ztv2Faz/2OpoJB+UIImIiHhLdFO4foy5/P0YVbX5ESVIIiIi3tTpOajbw5ymZuHzGmHbTyhBEhER8SaHAzo+a1a1/RkLnw9SkuQHlCCJiIh4W4XG0O//IDAUfv8OPuwIR7ZbHZVchhIkERGRwlCvJ9w5w5yrLWELTGkLPzwNmtDCJylBEhERKSz1esCI9VCnO2DAL1Nh4ydWRyV5UIIkIiJSmEqWh7vnmI23Ab4bA7/9FzLSrY1LslGCJCIiYoXrH4cm/cBIhy+HwnvN4Nhuq6OSi5QgiYiIWCEgAG553xwCAOBkHMy6E84nWxuXAEqQRERErBMUAnfNhsd3QnhlOL4bPuwEJ/ZZHZntKUESERGxWqkK0HcmFCsDx3bB/90BpxOtjsrWlCCJiIj4gsoxcP8CCAgyk6T3roEj26yOyraUIImIiPiK8vWgx5vmWEmpp2DeUI2TZBElSCIiIr6kxQPw6GYIKQmJ22D520qSLKAESURExNcULwsxg8zln/9hJklSqJQgiYiI+KIbX4CWQ83lxa/C4gmQnmZtTDaiBElERMQXBYWa7ZFaPQhGBix9Hf47EDIyrI7MFpQgiYiI+CqHw0ySbp8GQWGw83v4YhCcOWp1ZEWeEiQRERFf1/gOc9RtRyBs/x/8uzX8sdDqqIo0JUgiIiL+oMmdMPQnKN8Azh6FOXfB7sVWR1VkKUESERHxFxWbw4NLoGEfyEiDL+6H/autjqpIUoIkIiLiT4JC4bapENUYzp2Aj7vB2g+tjqrIUYIkIiLib4JCof8suPpG8/X8J2Hlu9bGVMQoQRIREfFHZarBvfOg9cOAAbEvwPqPrY6qyFCCJCIi4q8cDuj+OrR/xnz947NwYr+1MRURSpBERET8XfunoWpbuHAWZvaG3T9bHZHfU4IkIiLi7wICzIbbEVXgxF749DbY+KnVUfk1JUgiIiJFQZlqMPRnaHq3+fr7MRD/q7Ux+TElSCIiIkVFyfJw62So2wPSU+HjHrDsLUi/YHVkfsevE6QTJ04wYMAAIiIiiIiIYMCAAZw8efKS21+4cIGnn36axo0bU6JECSpWrMjAgQM5fPhw4QUtIiLiTQ4H3PIvqNoGUk/Dz/+AWXfCuZNWR+ZX/DpBuvvuu9m8eTMLFixgwYIFbN68mQEDBlxy+7Nnz7Jx40aef/55Nm7cyJdffskff/zBLbfcUohRi4iIeFmJSBj0Pdw6FYJLwJ7F8NGNkLjD6sj8hsMwDMPqINyxY8cOGjRowJo1a2jdujUAa9asoU2bNvz+++/UrVvXpf2sW7eOVq1asX//fqpWrerSZ5KTk4mIiCApKYnw8HC3v4OIiIjXxf8Gs++C5INmsjTgS6h6rdVRWSI/92+/LUFavXo1ERERzuQI4NprryUiIoJVq1a5vJ+kpCQcDgelS5e+5DYpKSkkJydne4iIiPiF6Cbw0FKocQNcOAOf9oFtX1kdlc/z2wQpISGB8uXL51pfvnx5EhISXNrH+fPneeaZZ7j77rsvm0lOmDDB2c4pIiKCKlWquB23iIhIoSsRCf1mmcMAXDgDnw+CPxZaHZVP87kEafz48Tgcjss+1q9fD4DD4cj1ecMw8lyf04ULF+jfvz8ZGRlMnjz5stuOHTuWpKQk5+PAgQPufTkRERGrhIXDkJ+g1k3m6y+Hwl9/WBuTDwuyOoCcRowYQf/+/S+7TfXq1fntt984cuRIrvf++usvoqKiLvv5Cxcu0LdvX/bu3cvPP/98xXrI0NBQQkNDrxy8iIiILysVZZYkfXIzHFwHn99njp0UXMzqyHyOzyVIkZGRREZGXnG7Nm3akJSUxNq1a2nVqhUAv/zyC0lJSbRt2/aSn8tMjnbt2sXixYspV66cx2IXERHxecFh0P8zmHIdJG43J7nt8abVUWU5/Rcsfws6PQehpSwLw+eq2FxVv359unXrxtChQ1mzZg1r1qxh6NCh3Hzzzdl6sNWrV4+vvjIbo6WlpXHHHXewfv16Zs2aRXp6OgkJCSQkJJCammrVVxERESlcJcvDrVPM5bX/gZ0LrI0nk2HANyPgl6kwb6ilofhtggQwa9YsGjduTJcuXejSpQtNmjTh00+zzz2zc+dOkpKSADh48CDffPMNBw8epFmzZkRHRzsf+en5JiIi4vdq3wTXDjeX/zccTrnWwclrDAMWPgd/LIDAELMEyUJ+Ow6SlTQOkoiIFAlpKfDhjXBkC1zdCe6ZZ058W5jOJ8PmWbD4NUi5OIxOr3chZpDHD2WLcZBERESkgIJC4faPIKgY7P4Z1k8r3OPvXgyvV4EFz2QlR7e875XkKL+UIImIiNhZ+XrQ+SVzeflESCuENrmGAUteh//rk7Wu2vXw6Ca45tJThhUmJUgiIiJ2FzMISkXDqcOw+l/eO45hwC8fwITKsGQCGBlQvqE51MCg76BsTe8dO5+UIImIiNhdUGhWo+ifXjYf3rDtS/jhKUg9DQHB0PU1cxqUSjHgwiDPhUkJkoiIiECze+DaR8zl5W97fiqS9AuwaLy53GIwPLkL2jwCgcGePY6HKEESERERswSn22tZXf+/fxxSz3hu/7sWwsk4KHEVdHkFipXx3L69QAmSiIiIZOk4zpzUNikuq8SnoM4chUUXG4I37Q8hxT2zXy9SgiQiIiJZQkvCzZPM5bX/gZ/+ARnpBdvnN4/C0Z0QGArX3FfgEAuDEiQRERHJrvZNZkkSmPOi/d/tcDrRvX2dOmKOjg3Q71OIrO2ZGL1MCZKIiIjk1v4p6PMRBBeHPYvh3aaw49v87+eXqWCkQ+WWUKer5+P0EiVIIiIikrcmd8KQn6BCE7hwFr4YDIsnQNJB1z7/x0JY+a65fN1j3ovTC5QgiYiIyKVFNYChi6F2F0hPgaWvw9TrzWlCLufINph7r1l61Lgv1O9VOPF6iBIkERERubzAIOg/G/p8CJF14NwJ+PRWWDMl7+0Nw2yYnZ4CtW6CWycXarieoARJREREriwwCJr0hQd+hHo3m+sWPAOLX4OMjOzbHlgLh9ZDUBj0nuyzg0FejhIkERERcV3xstDv/6BmB/P10jfgszvNKjWAM8fgu1HmcuM7oVSUFVEWWJDVAYiIiIifcTjg7s9h/TT48Vn4c5HZJqn5vXBoIyRuh5IV4IYnrI7UbSpBEhERkfwLCoFrH4aHlkHdHmZj7I2fwJEtZtXagC+hTHWro3SbEiQRERFxX4XG0G+WOUJ2eCVo2AfunQdRDa2OrEBUxSYiIiIFExAAt7xndRQepRIkERERkRyUIImIiIjkoARJREREJAclSCIiIiI5KEESERERyUEJkoiIiEgOSpBEREREclCCJCIiIpKDEiQRERGRHJQgiYiIiOSgBElEREQkByVIIiIiIjkoQRIRERHJQQmSiIiISA5BVgfgjwzDACA5OdniSERERMRVmfftzPv45ShBcsOpU6cAqFKlisWRiIiISH6dOnWKiIiIy27jMFxJoySbjIwMDh8+TKlSpXA4HB7dd3JyMlWqVOHAgQOEh4d7dN9Fnc6d+3TuCkbnz306dwWj85c/hmFw6tQpKlasSEDA5VsZqQTJDQEBAVSuXNmrxwgPD9fF7iadO/fp3BWMzp/7dO4KRufPdVcqOcqkRtoiIiIiOShBEhEREclBCZKPCQ0N5cUXXyQ0NNTqUPyOzp37dO4KRufPfTp3BaPz5z1qpC0iIiKSg0qQRERERHJQgiQiIiKSgxIkERERkRyUIImIiIjkoATJh0yePJkaNWoQFhZGTEwMy5cvtzokyy1btoxevXpRsWJFHA4HX3/9dbb3DcNg/PjxVKxYkWLFitGhQwe2bduWbZuUlBRGjhxJZGQkJUqU4JZbbuHgwYOF+C2sMWHCBFq2bEmpUqUoX748t956Kzt37sy2jc5f3qZMmUKTJk2cg++1adOGH374wfm+zpvrJkyYgMPhYNSoUc51On+XNn78eBwOR7ZHhQoVnO/r3BUiQ3zCnDlzjODgYOPDDz80tm/fbjz22GNGiRIljP3791sdmqXmz59vjBs3zpg3b54BGF999VW2919//XWjVKlSxrx584wtW7YY/fr1M6Kjo43k5GTnNsOGDTMqVapkxMbGGhs3bjQ6duxoNG3a1EhLSyvkb1O4unbtanz88cfG1q1bjc2bNxs9e/Y0qlatapw+fdq5jc5f3r755hvj+++/N3bu3Gns3LnTePbZZ43g4GBj69athmHovLlq7dq1RvXq1Y0mTZoYjz32mHO9zt+lvfjii0bDhg2N+Ph45yMxMdH5vs5d4VGC5CNatWplDBs2LNu6evXqGc8884xFEfmenAlSRkaGUaFCBeP11193rjt//rwRERFhTJ061TAMwzh58qQRHBxszJkzx7nNoUOHjICAAGPBggWFFrsvSExMNABj6dKlhmHo/OVXmTJljI8++kjnzUWnTp0yateubcTGxhrt27d3Jkg6f5f34osvGk2bNs3zPZ27wqUqNh+QmprKhg0b6NKlS7b1Xbp0YdWqVRZF5fv27t1LQkJCtvMWGhpK+/btnedtw4YNXLhwIds2FStWpFGjRrY7t0lJSQCULVsW0PlzVXp6OnPmzOHMmTO0adNG581FjzzyCD179uSmm27Ktl7n78p27dpFxYoVqVGjBv3792fPnj2Azl1h02S1PuDo0aOkp6cTFRWVbX1UVBQJCQkWReX7Ms9NXudt//79zm1CQkIoU6ZMrm3sdG4Nw2DMmDFcf/31NGrUCND5u5ItW7bQpk0bzp8/T8mSJfnqq69o0KCB8yaj83Zpc+bMYePGjaxbty7Xe7ruLq9169bMnDmTOnXqcOTIEV555RXatm3Ltm3bdO4KmRIkH+JwOLK9Ngwj1zrJzZ3zZrdzO2LECH777TdWrFiR6z2dv7zVrVuXzZs3c/LkSebNm8d9993H0qVLne/rvOXtwIEDPPbYYyxcuJCwsLBLbqfzl7fu3bs7lxs3bkybNm24+uqr+eSTT7j22msBnbvCoio2HxAZGUlgYGCu7D4xMTHXLwXJktmz43LnrUKFCqSmpnLixIlLblPUjRw5km+++YbFixdTuXJl53qdv8sLCQmhVq1atGjRggkTJtC0aVPeffddnbcr2LBhA4mJicTExBAUFERQUBBLly7lvffeIygoyPn9df5cU6JECRo3bsyuXbt07RUyJUg+ICQkhJiYGGJjY7Otj42NpW3bthZF5ftq1KhBhQoVsp231NRUli5d6jxvMTExBAcHZ9smPj6erVu3FvlzaxgGI0aM4Msvv+Tnn3+mRo0a2d7X+csfwzBISUnRebuCG2+8kS1btrB582bno0WLFtxzzz1s3ryZmjVr6vzlQ0pKCjt27CA6OlrXXmGzomW45JbZzX/atGnG9u3bjVGjRhklSpQw9u3bZ3Voljp16pSxadMmY9OmTQZgTJw40di0aZNz+IPXX3/diIiIML788ktjy5Ytxl133ZVnl9fKlSsbixYtMjZu3Gh06tTJFl1eH374YSMiIsJYsmRJti7DZ8+edW6j85e3sWPHGsuWLTP27t1r/Pbbb8azzz5rBAQEGAsXLjQMQ+ctv/7ei80wdP4u5/HHHzeWLFli7Nmzx1izZo1x8803G6VKlXLeC3TuCo8SJB/y73//26hWrZoREhJiXHPNNc7u2Ha2ePFiA8j1uO+++wzDMLu9vvjii0aFChWM0NBQ44YbbjC2bNmSbR/nzp0zRowYYZQtW9YoVqyYcfPNNxtxcXEWfJvCldd5A4yPP/7YuY3OX94eeOAB57/Fq666yrjxxhudyZFh6LzlV84ESefv0jLHNQoODjYqVqxo9OnTx9i2bZvzfZ27wuMwDMOwpuxKRERExDepDZKIiIhIDkqQRERERHJQgiQiIiKSgxIkERERkRyUIImIiIjkoARJREREJAclSCIiIiI5KEESERERyUEJkogUaePHj8fhcLBkyRKrQxERP6IESUT83pIlS3A4HIwfP97qUESkiFCCJCJF2ogRI9ixYwetWrWyOhQR8SNBVgcgIuJNkZGRREZGWh2GiPgZlSCJiF8bP348HTt2BOCll17C4XA4H/v27cuzDdK+fftwOBwMGjSIHTt2cPPNN1O6dGnKlCnDXXfdxdGjRwH45Zdf6Ny5M+Hh4ZQpU4ahQ4dy5syZPONYtmwZvXr1IjIyktDQUGrXrs1zzz3H2bNnvX4ORMTzVIIkIn6tQ4cO7Nu3j08++YT27dvToUMH53ulS5e+7Gf37t1L27ZtadGiBUOGDGH9+vXMmTOHAwcO8MYbb9C5c2c6d+7Mgw8+yJIlS/joo48A+PDDD7PtZ+rUqQwfPpwyZcrQq1cvrrrqKtatW8err77K4sWLWbx4MSEhIZ7+6iLiTYaIiJ9bvHixARgvvvhirvdefPFFAzAWL17sXLd3714DMABj0qRJzvUZGRlGjx49DMAoXbq08fXXXzvfS01NNZo0aWIEBwcbCQkJzvXbtm0zgoKCjObNmxvHjh3LduwJEyYYgPHWW2957suKSKFQFZuI2FbNmjUZOXKk87XD4aB///4ANG/enN69ezvfCw4O5o477uDChQvs2LHDuf6DDz4gLS2N9957j7Jly2bb/1NPPcVVV13F7NmzvfxNRMTTVMUmIrbVtGlTAgKy/06Mjo4GoFmzZrm2z3zv0KFDznVr1qwBYMGCBSxatCjXZ4KDg/n99989FbKIFBIlSCJiW+Hh4bnWBQUFXfG9CxcuONcdP34cgFdffdUbIYqIRVTFJiJSAJmJVHJyMoZhXPIhIv5FCZKI+L3AwEAA0tPTC/3YrVu3BrKq2kSkaFCCJCJ+L7Nx9MGDBwv92MOHDycoKIiRI0dy4MCBXO+fPHmSTZs2FXpcIlIwaoMkIn6vXr16VKxYkTlz5lC8eHEqV66Mw+Hg4Ycf9vqxGzVqxOTJk3n44YepW7cuPXr04OqrryY5OZk9e/awdOlSBg0axNSpU70ei4h4jhIkEfF7gYGBfPnllzz99NN8+umnnDp1CsDZZd/bhg4dSrNmzZg4cSLLli3jm2++ISIigqpVqzJ69Gjuu+++QolDRDzHYaj1oIiIiEg2aoMkIiIikoMSJBEREZEclCCJiIiI5KAESURERCQHJUgiIiIiOShBEhEREclBCZKIiIhIDkqQRERERHJQgiQiIiKSgxIkERERkRyUIImIiIjkoARJREREJIf/ByrpIwelCuggAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.style.use('seaborn')\n",
    "plt.plot(np.arange(len(result)), YVal, label = 'Validation')\n",
    "plt.plot(np.arange(len(result)), result, label = 'Training')\n",
    "plt.ylabel('state', fontsize = 14)\n",
    "plt.xlabel('time', fontsize = 14)\n",
    "plt.legend()\n",
    "# plt.ylim([-0.5,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
